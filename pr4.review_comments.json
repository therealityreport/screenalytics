[{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565054","pull_request_review_id":3449784939,"id":2515565054,"node_id":"PRRC_kwDOQRcQs86V8Hn-","diff_hunk":"@@ -0,0 +1,1173 @@\n+from __future__ import annotations\n+\n+import base64\n+import html\n+import json\n+import math\n+import numbers\n+import os\n+import re\n+import time\n+from functools import lru_cache\n+from pathlib import Path\n+from typing import Any, Dict, Generator, List, Optional, Tuple\n+from urllib.parse import urlparse\n+\n+import requests\n+import streamlit as st\n+import streamlit.components.v1 as components\n+\n+DEFAULT_TITLE = \"SCREENALYTICS\"\n+DATA_ROOT = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+DEFAULT_STRIDE = 3\n+DEFAULT_DETECTOR = \"retinaface\"\n+DEFAULT_TRACKER = \"bytetrack\"\n+DEFAULT_DEVICE = \"auto\"\n+DEFAULT_DEVICE_LABEL = \"Auto\"\n+DEFAULT_DET_THRESH = 0.5\n+_LOCAL_MEDIA_CACHE_SIZE = 256\n+\n+# Thumbnail constants\n+THUMB_W, THUMB_H = 200, 250\n+_PLACEHOLDER = \"apps/workspace-ui/assets/placeholder_face.svg\"\n+\n+LABEL = {\n+    DEFAULT_DETECTOR: \"RetinaFace (recommended)\",\n+    \"yolov8face\": \"YOLOv8-face (alt)\",\n+    DEFAULT_TRACKER: \"ByteTrack (default)\",\n+    \"strongsort\": \"StrongSORT (ReID)\",\n+}\n+DEVICE_LABELS = [\"Auto\", \"CPU\", \"MPS\", \"CUDA\"]\n+DEVICE_VALUE_MAP = {\"Auto\": \"auto\", \"CPU\": \"cpu\", \"MPS\": \"mps\", \"CUDA\": \"cuda\"}\n+DETECTOR_OPTIONS = [\n+    (\"RetinaFace (recommended)\", DEFAULT_DETECTOR),\n+    (\"YOLOv8-face (alt)\", \"yolov8face\"),\n+]\n+DETECTOR_LABELS = [label for label, _ in DETECTOR_OPTIONS]\n+DETECTOR_VALUE_MAP = {label: value for label, value in DETECTOR_OPTIONS}\n+DETECTOR_LABEL_MAP = {value: label for label, value in DETECTOR_OPTIONS}\n+FACE_ONLY_DETECTORS = {\"retinaface\", \"yolov8face\"}\n+TRACKER_OPTIONS = [\n+    (\"ByteTrack (default)\", DEFAULT_TRACKER),\n+    (\"StrongSORT (ReID)\", \"strongsort\"),\n+]\n+TRACKER_LABELS = [label for label, _ in TRACKER_OPTIONS]\n+TRACKER_VALUE_MAP = {label: value for label, value in TRACKER_OPTIONS}\n+TRACKER_LABEL_MAP = {value: label for label, value in TRACKER_OPTIONS}\n+_EP_ID_REGEX = re.compile(r\"^(?P<show>.+)-s(?P<season>\\d{2})e(?P<episode>\\d{2})$\", re.IGNORECASE)\n+\n+\n+def _env(key: str, default: str = \"\") -> str:\n+    return os.environ.get(key, default)\n+\n+\n+def _env_flag(name: str, default: bool) -> bool:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    value = raw.strip().lower()\n+    if value in {\"1\", \"true\", \"yes\", \"on\"}:\n+        return True\n+    if value in {\"0\", \"false\", \"off\", \"no\"}:\n+        return False\n+    return default\n+\n+\n+def _env_float(name: str, default: float) -> float:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    try:\n+        return float(raw)\n+    except (TypeError, ValueError):\n+        return default\n+\n+\n+def _env_int(name: str, default: int) -> int:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    try:\n+        return int(raw)\n+    except (TypeError, ValueError):\n+        return default\n+\n+\n+def coerce_int(value: Any) -> int | None:\n+    if value is None:\n+        return None\n+    if isinstance(value, bool):\n+        return None\n+    if isinstance(value, numbers.Integral):\n+        return int(value)\n+    if isinstance(value, numbers.Real):\n+        if math.isnan(value) or math.isinf(value):\n+            return None\n+        return int(value)\n+    if isinstance(value, str):\n+        cleaned = value.replace(\",\", \"\").strip()\n+        if not cleaned:\n+            return None\n+        try:\n+            return int(float(cleaned))\n+        except ValueError:\n+            return None\n+    return None\n+\n+\n+def format_count(value: Any) -> str | None:\n+    numeric = coerce_int(value)\n+    if numeric is None:\n+        return None\n+    return f\"{numeric:,}\"\n+\n+\n+SCENE_DETECT_DEFAULT = _env_flag(\"SCENE_DETECT\", True)\n+SCENE_THRESHOLD_DEFAULT = max(min(_env_float(\"SCENE_THRESHOLD\", 0.30), 2.0), 0.0)\n+SCENE_MIN_LEN_DEFAULT = max(_env_int(\"SCENE_MIN_LEN\", 12), 1)\n+SCENE_WARMUP_DETS_DEFAULT = max(_env_int(\"SCENE_WARMUP_DETS\", 3), 0)\n+\n+\n+def describe_error(url: str, exc: requests.RequestException) -> str:\n+    detail = str(exc)\n+    if isinstance(exc, requests.HTTPError) and exc.response is not None:\n+        try:\n+            detail = exc.response.text or exc.response.reason or detail\n+        except Exception:  # pragma: no cover\n+            detail = str(exc)\n+    return f\"{url} → {detail}\"\n+\n+\n+def _api_base() -> str:\n+    base = st.session_state.get(\"api_base\")\n+    if not base:\n+        raise RuntimeError(\"init_page() must be called before API access\")\n+    return base\n+\n+\n+def init_page(title: str = DEFAULT_TITLE) -> Dict[str, str]:\n+    st.set_page_config(page_title=title, layout=\"wide\")\n+    api_base = st.session_state.get(\"api_base\") or _env(\"SCREENALYTICS_API_URL\", \"http://localhost:8000\")\n+    st.session_state.setdefault(\"api_base\", api_base)\n+    backend = st.session_state.get(\"backend\") or _env(\"STORAGE_BACKEND\", \"local\").lower()\n+    st.session_state.setdefault(\"backend\", backend)\n+    bucket = st.session_state.get(\"bucket\") or (\n+        _env(\"AWS_S3_BUCKET\")\n+        or _env(\"SCREENALYTICS_OBJECT_STORE_BUCKET\")\n+        or (\"local\" if backend == \"local\" else \"\")\n+    )\n+    st.session_state.setdefault(\"bucket\", bucket)\n+\n+    query_ep_id = st.query_params.get(\"ep_id\", \"\")\n+    stored_ep_id = st.session_state.get(\"ep_id\")\n+    if stored_ep_id is None:\n+        st.session_state[\"ep_id\"] = query_ep_id\n+    elif query_ep_id and query_ep_id != stored_ep_id:\n+        st.session_state[\"ep_id\"] = query_ep_id\n+    elif stored_ep_id and (not query_ep_id or query_ep_id != stored_ep_id):","path":"apps/workspace-ui/ui_helpers.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Test is always false, because of [this condition](1).","created_at":"2025-11-11T20:00:03Z","updated_at":"2025-11-11T20:00:21Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565054","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565054"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565054"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565054/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":167,"original_line":167,"side":"RIGHT","author_association":"NONE","original_position":167,"position":167,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565074","pull_request_review_id":3449784939,"id":2515565074,"node_id":"PRRC_kwDOQRcQs86V8HoS","diff_hunk":"@@ -0,0 +1,1173 @@\n+from __future__ import annotations\n+\n+import base64\n+import html\n+import json\n+import math\n+import numbers\n+import os\n+import re\n+import time\n+from functools import lru_cache\n+from pathlib import Path\n+from typing import Any, Dict, Generator, List, Optional, Tuple\n+from urllib.parse import urlparse\n+\n+import requests\n+import streamlit as st\n+import streamlit.components.v1 as components\n+\n+DEFAULT_TITLE = \"SCREENALYTICS\"\n+DATA_ROOT = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+DEFAULT_STRIDE = 3\n+DEFAULT_DETECTOR = \"retinaface\"\n+DEFAULT_TRACKER = \"bytetrack\"\n+DEFAULT_DEVICE = \"auto\"\n+DEFAULT_DEVICE_LABEL = \"Auto\"\n+DEFAULT_DET_THRESH = 0.5\n+_LOCAL_MEDIA_CACHE_SIZE = 256\n+\n+# Thumbnail constants\n+THUMB_W, THUMB_H = 200, 250\n+_PLACEHOLDER = \"apps/workspace-ui/assets/placeholder_face.svg\"\n+\n+LABEL = {\n+    DEFAULT_DETECTOR: \"RetinaFace (recommended)\",\n+    \"yolov8face\": \"YOLOv8-face (alt)\",\n+    DEFAULT_TRACKER: \"ByteTrack (default)\",\n+    \"strongsort\": \"StrongSORT (ReID)\",\n+}\n+DEVICE_LABELS = [\"Auto\", \"CPU\", \"MPS\", \"CUDA\"]\n+DEVICE_VALUE_MAP = {\"Auto\": \"auto\", \"CPU\": \"cpu\", \"MPS\": \"mps\", \"CUDA\": \"cuda\"}\n+DETECTOR_OPTIONS = [\n+    (\"RetinaFace (recommended)\", DEFAULT_DETECTOR),\n+    (\"YOLOv8-face (alt)\", \"yolov8face\"),\n+]\n+DETECTOR_LABELS = [label for label, _ in DETECTOR_OPTIONS]\n+DETECTOR_VALUE_MAP = {label: value for label, value in DETECTOR_OPTIONS}\n+DETECTOR_LABEL_MAP = {value: label for label, value in DETECTOR_OPTIONS}\n+FACE_ONLY_DETECTORS = {\"retinaface\", \"yolov8face\"}\n+TRACKER_OPTIONS = [\n+    (\"ByteTrack (default)\", DEFAULT_TRACKER),\n+    (\"StrongSORT (ReID)\", \"strongsort\"),\n+]\n+TRACKER_LABELS = [label for label, _ in TRACKER_OPTIONS]\n+TRACKER_VALUE_MAP = {label: value for label, value in TRACKER_OPTIONS}\n+TRACKER_LABEL_MAP = {value: label for label, value in TRACKER_OPTIONS}\n+_EP_ID_REGEX = re.compile(r\"^(?P<show>.+)-s(?P<season>\\d{2})e(?P<episode>\\d{2})$\", re.IGNORECASE)\n+\n+\n+def _env(key: str, default: str = \"\") -> str:\n+    return os.environ.get(key, default)\n+\n+\n+def _env_flag(name: str, default: bool) -> bool:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    value = raw.strip().lower()\n+    if value in {\"1\", \"true\", \"yes\", \"on\"}:\n+        return True\n+    if value in {\"0\", \"false\", \"off\", \"no\"}:\n+        return False\n+    return default\n+\n+\n+def _env_float(name: str, default: float) -> float:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    try:\n+        return float(raw)\n+    except (TypeError, ValueError):\n+        return default\n+\n+\n+def _env_int(name: str, default: int) -> int:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    try:\n+        return int(raw)\n+    except (TypeError, ValueError):\n+        return default\n+\n+\n+def coerce_int(value: Any) -> int | None:\n+    if value is None:\n+        return None\n+    if isinstance(value, bool):\n+        return None\n+    if isinstance(value, numbers.Integral):\n+        return int(value)\n+    if isinstance(value, numbers.Real):\n+        if math.isnan(value) or math.isinf(value):\n+            return None\n+        return int(value)\n+    if isinstance(value, str):\n+        cleaned = value.replace(\",\", \"\").strip()\n+        if not cleaned:\n+            return None\n+        try:\n+            return int(float(cleaned))\n+        except ValueError:\n+            return None\n+    return None\n+\n+\n+def format_count(value: Any) -> str | None:\n+    numeric = coerce_int(value)\n+    if numeric is None:\n+        return None\n+    return f\"{numeric:,}\"\n+\n+\n+SCENE_DETECT_DEFAULT = _env_flag(\"SCENE_DETECT\", True)\n+SCENE_THRESHOLD_DEFAULT = max(min(_env_float(\"SCENE_THRESHOLD\", 0.30), 2.0), 0.0)\n+SCENE_MIN_LEN_DEFAULT = max(_env_int(\"SCENE_MIN_LEN\", 12), 1)\n+SCENE_WARMUP_DETS_DEFAULT = max(_env_int(\"SCENE_WARMUP_DETS\", 3), 0)\n+\n+\n+def describe_error(url: str, exc: requests.RequestException) -> str:\n+    detail = str(exc)\n+    if isinstance(exc, requests.HTTPError) and exc.response is not None:\n+        try:\n+            detail = exc.response.text or exc.response.reason or detail\n+        except Exception:  # pragma: no cover\n+            detail = str(exc)\n+    return f\"{url} → {detail}\"\n+\n+\n+def _api_base() -> str:\n+    base = st.session_state.get(\"api_base\")\n+    if not base:\n+        raise RuntimeError(\"init_page() must be called before API access\")\n+    return base\n+\n+\n+def init_page(title: str = DEFAULT_TITLE) -> Dict[str, str]:\n+    st.set_page_config(page_title=title, layout=\"wide\")\n+    api_base = st.session_state.get(\"api_base\") or _env(\"SCREENALYTICS_API_URL\", \"http://localhost:8000\")\n+    st.session_state.setdefault(\"api_base\", api_base)\n+    backend = st.session_state.get(\"backend\") or _env(\"STORAGE_BACKEND\", \"local\").lower()\n+    st.session_state.setdefault(\"backend\", backend)\n+    bucket = st.session_state.get(\"bucket\") or (\n+        _env(\"AWS_S3_BUCKET\")\n+        or _env(\"SCREENALYTICS_OBJECT_STORE_BUCKET\")\n+        or (\"local\" if backend == \"local\" else \"\")\n+    )\n+    st.session_state.setdefault(\"bucket\", bucket)\n+\n+    query_ep_id = st.query_params.get(\"ep_id\", \"\")\n+    stored_ep_id = st.session_state.get(\"ep_id\")\n+    if stored_ep_id is None:\n+        st.session_state[\"ep_id\"] = query_ep_id\n+    elif query_ep_id and query_ep_id != stored_ep_id:\n+        st.session_state[\"ep_id\"] = query_ep_id\n+    elif stored_ep_id and (not query_ep_id or query_ep_id != stored_ep_id):\n+        params = st.query_params\n+        params[\"ep_id\"] = stored_ep_id\n+        st.query_params = params\n+\n+    if \"device_default_label\" not in st.session_state:\n+        st.session_state[\"device_default_label\"] = _guess_device_label()\n+    st.session_state.setdefault(\"detector_choice\", DEFAULT_DETECTOR)\n+    st.session_state.setdefault(\"tracker_choice\", DEFAULT_TRACKER)\n+\n+    sidebar = st.sidebar\n+    sidebar.header(\"API\")\n+    sidebar.code(api_base)\n+    health_url = f\"{api_base}/healthz\"\n+    try:\n+        resp = requests.get(health_url, timeout=5)\n+        resp.raise_for_status()\n+        sidebar.success(\"/healthz OK\")\n+    except requests.RequestException as exc:\n+        sidebar.error(describe_error(health_url, exc))\n+    sidebar.caption(f\"Backend: {backend} | Bucket: {bucket}\")\n+\n+    return {\n+        \"api_base\": api_base,\n+        \"backend\": backend,\n+        \"bucket\": bucket,\n+        \"ep_id\": st.session_state.get(\"ep_id\", \"\"),\n+    }\n+\n+\n+def set_ep_id(ep_id: str, rerun: bool = True) -> None:\n+    if not ep_id:\n+        return\n+    current = st.session_state.get(\"ep_id\")\n+    if current == ep_id:\n+        params = st.query_params\n+        params[\"ep_id\"] = ep_id\n+        st.query_params = params\n+        return\n+    st.session_state[\"ep_id\"] = ep_id\n+    params = st.query_params\n+    params[\"ep_id\"] = ep_id\n+    st.query_params = params\n+    if rerun:\n+        st.rerun()\n+\n+\n+def get_ep_id() -> str:\n+    return st.session_state.get(\"ep_id\", \"\")\n+\n+\n+def api_get(path: str, **kwargs) -> Dict[str, Any]:\n+    base = st.session_state.get(\"api_base\")\n+    if not base:\n+        raise RuntimeError(\"init_page() must be called before API access\")\n+    timeout = kwargs.pop(\"timeout\", 60)\n+    resp = requests.get(f\"{base}{path}\", timeout=timeout, **kwargs)\n+    resp.raise_for_status()\n+    return resp.json()\n+\n+\n+def api_post(path: str, json: Dict[str, Any] | None = None, **kwargs) -> Dict[str, Any]:\n+    base = st.session_state.get(\"api_base\")\n+    if not base:\n+        raise RuntimeError(\"init_page() must be called before API access\")\n+    timeout = kwargs.pop(\"timeout\", 60)\n+    resp = requests.post(f\"{base}{path}\", json=json or {}, timeout=timeout, **kwargs)\n+    resp.raise_for_status()\n+    return resp.json()\n+\n+\n+def _episode_status_payload(ep_id: str) -> Dict[str, Any] | None:\n+    url = f\"{_api_base()}/episodes/{ep_id}/status\"\n+    try:\n+        resp = requests.get(url, timeout=15)\n+        resp.raise_for_status()\n+    except requests.RequestException:\n+        return None\n+    try:\n+        payload = resp.json()\n+    except ValueError:\n+        return None\n+    return payload if isinstance(payload, dict) else None\n+\n+\n+def get_episode_status(ep_id: str) -> Dict[str, Any] | None:\n+    return _episode_status_payload(ep_id)\n+\n+\n+def link_local(path: Path | str) -> str:\n+    return f\"`{path}`\"\n+\n+\n+@lru_cache(maxsize=_LOCAL_MEDIA_CACHE_SIZE)\n+def _data_url_cache(path_str: str) -> str | None:\n+    file_path = Path(path_str)\n+    try:\n+        data = file_path.read_bytes()\n+    except OSError:\n+        return None\n+    encoded = base64.b64encode(data).decode(\"ascii\")\n+    return f\"data:image/jpeg;base64,{encoded}\"\n+\n+\n+def ensure_media_url(path_or_url: str | Path | None) -> str | None:\n+    \"\"\"Return a browser-safe URL for local artifacts, falling back to the original string.\"\"\"\n+    if not path_or_url:\n+        return None\n+    value = str(path_or_url)\n+    parsed = urlparse(value)\n+    scheme = parsed.scheme.lower()\n+    if scheme in {\"http\", \"https\", \"data\"}:\n+        return value\n+    candidate_paths: List[Path] = []\n+    if scheme == \"file\":\n+        candidate_paths.append(Path(parsed.path))\n+    else:\n+        candidate_paths.append(Path(value))\n+    first = candidate_paths[0]\n+    if not first.is_absolute():\n+        candidate_paths.append((DATA_ROOT / first).expanduser())\n+    for candidate in candidate_paths:\n+        try:\n+            resolved = candidate.expanduser().resolve()\n+        except OSError:\n+            continue\n+        if resolved.is_file():\n+            cached = _data_url_cache(str(resolved))\n+            if cached:\n+                return cached\n+    return value\n+\n+\n+def human_size(num_bytes: int | None) -> str:\n+    if num_bytes is None:\n+        return \"?\"\n+    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n+    size = float(num_bytes)\n+    idx = 0\n+    while size >= 1024 and idx < len(units) - 1:\n+        size /= 1024\n+        idx += 1\n+    return f\"{size:.1f} {units[idx]}\"\n+\n+\n+def ds(rows: List[Dict[str, Any]]) -> None:\n+    if not rows:\n+        st.info(\"No rows yet.\")\n+    else:\n+        st.dataframe(rows, use_container_width=True)\n+\n+\n+def device_default_label() -> str:\n+    return st.session_state.get(\"device_default_label\", \"CPU\")\n+\n+\n+def device_label_index(label: str) -> int:\n+    try:\n+        return DEVICE_LABELS.index(label)\n+    except ValueError:\n+        return 0\n+\n+\n+def detector_default_value() -> str:\n+    return st.session_state.get(\"detector_choice\", DEFAULT_DETECTOR)\n+\n+\n+def detector_label_index(value: str | None) -> int:\n+    key = str(value).lower() if value else DEFAULT_DETECTOR\n+    label = DETECTOR_LABEL_MAP.get(key)\n+    if label in DETECTOR_LABELS:\n+        return DETECTOR_LABELS.index(label)\n+    return 0\n+\n+\n+def detector_label_from_value(value: str | None) -> str:\n+    if not value:\n+        return \"unknown\"\n+    value = value.lower()\n+    return DETECTOR_LABEL_MAP.get(value, value)\n+\n+\n+def remember_detector(value: str | None) -> None:\n+    key = (value or \"\").lower() if value else \"\"\n+    if key in FACE_ONLY_DETECTORS:\n+        st.session_state[\"detector_choice\"] = key\n+\n+\n+def tracker_default_value() -> str:\n+    return st.session_state.get(\"tracker_choice\", DEFAULT_TRACKER)\n+\n+\n+def tracker_label_index(value: str | None) -> int:\n+    key = str(value).lower() if value else DEFAULT_TRACKER\n+    label = TRACKER_LABEL_MAP.get(key)\n+    if label in TRACKER_LABELS:\n+        return TRACKER_LABELS.index(label)\n+    return 0\n+\n+\n+def tracker_label_from_value(value: str | None) -> str:\n+    if not value:\n+        return \"unknown\"\n+    return TRACKER_LABEL_MAP.get(value.lower(), value)\n+\n+\n+def remember_tracker(value: str | None) -> None:\n+    key = (value or \"\").lower() if value else \"\"\n+    if key in TRACKER_LABEL_MAP:\n+        st.session_state[\"tracker_choice\"] = key\n+\n+\n+def default_detect_track_payload(\n+    ep_id: str,\n+    *,\n+    stride: int | None = None,\n+    device: str | None = None,\n+    det_thresh: float | None = None,\n+) -> Dict[str, Any]:\n+    payload: Dict[str, Any] = {\n+        \"ep_id\": ep_id,\n+        \"stride\": int(stride if stride is not None else DEFAULT_STRIDE),\n+        \"device\": (device or DEFAULT_DEVICE).lower(),\n+        \"detector\": DEFAULT_DETECTOR,\n+        \"tracker\": DEFAULT_TRACKER,\n+        \"det_thresh\": float(det_thresh if det_thresh is not None else DEFAULT_DET_THRESH),\n+        \"save_frames\": True,\n+        \"save_crops\": True,\n+        \"jpeg_quality\": 85,\n+        \"scene_detect\": SCENE_DETECT_DEFAULT,\n+        \"scene_threshold\": SCENE_THRESHOLD_DEFAULT,\n+        \"scene_min_len\": SCENE_MIN_LEN_DEFAULT,\n+        \"scene_warmup_dets\": SCENE_WARMUP_DETS_DEFAULT,\n+    }\n+    return payload\n+\n+\n+def _guess_device_label() -> str:\n+    try:\n+        import torch  # type: ignore\n+\n+        if torch.cuda.is_available():  # pragma: no cover\n+            return \"Auto\"\n+        mps_backend = getattr(torch.backends, \"mps\", None)\n+        if mps_backend is not None and mps_backend.is_available():  # pragma: no cover\n+            return \"MPS\"\n+    except Exception:  # pragma: no cover\n+        pass\n+    return \"CPU\"\n+\n+\n+def parse_ep_id(ep_id: str) -> Optional[Dict[str, int | str]]:\n+    match = _EP_ID_REGEX.match(ep_id)\n+    if not match:\n+        return None\n+    show = match.group(\"show\")\n+    try:\n+        season = int(match.group(\"season\"))\n+        episode = int(match.group(\"episode\"))\n+    except ValueError:\n+        return None\n+    return {\"show\": show, \"season\": season, \"episode\": episode}\n+\n+\n+def _manifest_path(ep_id: str, filename: str) -> Path:\n+    return DATA_ROOT / \"manifests\" / ep_id / filename\n+\n+\n+def tracks_detector_value(ep_id: str) -> str | None:\n+    path = _manifest_path(ep_id, \"tracks.jsonl\")\n+    if not path.exists():\n+        return None\n+    try:\n+        with path.open(\"r\", encoding=\"utf-8\") as handle:\n+            for line in handle:\n+                line = line.strip()\n+                if not line:\n+                    continue\n+                try:\n+                    payload = json.loads(line)\n+                except json.JSONDecodeError:\n+                    continue\n+                detector = payload.get(\"detector\")\n+                if detector:\n+                    return str(detector).lower()\n+    except OSError:\n+        return None\n+    return None\n+\n+\n+def tracks_tracker_value(ep_id: str) -> str | None:\n+    path = _manifest_path(ep_id, \"tracks.jsonl\")\n+    if not path.exists():\n+        return None\n+    try:\n+        with path.open(\"r\", encoding=\"utf-8\") as handle:\n+            for line in handle:\n+                line = line.strip()\n+                if not line:\n+                    continue\n+                try:\n+                    payload = json.loads(line)\n+                except json.JSONDecodeError:\n+                    continue\n+                tracker = payload.get(\"tracker\")\n+                if tracker:\n+                    return str(tracker).lower()\n+    except OSError:\n+        return None\n+    return None\n+\n+\n+def detector_is_face_only(ep_id: str) -> bool:\n+    detector = tracks_detector_value(ep_id)\n+    if detector is None:\n+        path = _manifest_path(ep_id, \"tracks.jsonl\")\n+        return not path.exists()\n+    return detector.lower() in FACE_ONLY_DETECTORS\n+\n+\n+def tracks_detector_label(ep_id: str) -> str:\n+    return detector_label_from_value(tracks_detector_value(ep_id))\n+\n+\n+def tracks_tracker_label(ep_id: str) -> str:\n+    return tracker_label_from_value(tracks_tracker_value(ep_id))\n+\n+\n+def try_switch_page(page_path: str) -> None:\n+    try:\n+        st.switch_page(page_path)\n+    except Exception:\n+        st.info(\"Use the sidebar navigation to open the target page.\")\n+\n+\n+def format_mmss(seconds: float | int | None) -> str:\n+    if seconds is None or seconds < 0:\n+        return \"--:--\"\n+    total = int(seconds)\n+    minutes, secs = divmod(total, 60)\n+    return f\"{minutes:02d}:{secs:02d}\"\n+\n+\n+def progress_ratio(progress: Dict[str, Any]) -> float:\n+    frames_total = progress.get(\"frames_total\") or 0\n+    frames_done = progress.get(\"frames_done\") or 0\n+    if frames_total and frames_total > 0:\n+        return max(min(frames_done / frames_total, 1.0), 0.0)\n+    return 0.0\n+\n+\n+def eta_seconds(progress: Dict[str, Any]) -> float | None:\n+    secs_total = progress.get(\"secs_total\")\n+    secs_done = progress.get(\"secs_done\")\n+    if secs_total is not None and secs_done is not None:\n+        remaining = max(secs_total - secs_done, 0.0)\n+        return remaining\n+    frames_total = progress.get(\"frames_total\")\n+    frames_done = progress.get(\"frames_done\")\n+    fps = progress.get(\"fps_infer\") or progress.get(\"fps_detected\")\n+    if frames_total and frames_done is not None and fps and fps > 0:\n+        remaining_frames = max(frames_total - frames_done, 0)\n+        return remaining_frames / fps if remaining_frames >= 0 else None\n+    return None\n+\n+\n+def total_seconds_hint(progress: Dict[str, Any]) -> float | None:\n+    secs_total = progress.get(\"secs_total\")\n+    if secs_total is not None:\n+        return secs_total\n+    frames_total = progress.get(\"frames_total\")\n+    fps = progress.get(\"fps_infer\") or progress.get(\"analyzed_fps\") or progress.get(\"fps_detected\")\n+    if frames_total and fps and fps > 0:\n+        return frames_total / fps\n+    return None\n+\n+\n+def iter_sse_events(response: requests.Response) -> Generator[Tuple[str, Dict[str, Any]], None, None]:\n+    event_name = \"message\"\n+    data_lines: List[str] = []\n+    try:\n+        for raw_line in response.iter_lines(decode_unicode=True):\n+            if raw_line is None:\n+                continue\n+            line = raw_line.strip()\n+            if not line:\n+                if data_lines:\n+                    data_str = \"\\n\".join(data_lines)\n+                    try:\n+                        payload = json.loads(data_str)\n+                    except json.JSONDecodeError:\n+                        payload = {\"raw\": data_str}\n+                    yield event_name or \"message\", payload  # type: ignore[misc]\n+                event_name = \"message\"\n+                data_lines = []\n+                continue\n+            if line.startswith(\":\"):\n+                continue\n+            if line.startswith(\"event:\"):\n+                event_name = line.split(\":\", 1)[1].strip()\n+            elif line.startswith(\"data:\"):\n+                data_lines.append(line[5:].lstrip())\n+    finally:\n+        response.close()\n+\n+\n+def episode_artifact_prefixes(ep_id: str) -> Dict[str, str] | None:\n+    parsed = parse_ep_id(ep_id)\n+    if not parsed:\n+        return None\n+    show = parsed[\"show\"]\n+    season = int(parsed[\"season\"])  # type: ignore[arg-type]\n+    episode = int(parsed[\"episode\"])  # type: ignore[arg-type]\n+    return {\n+        \"frames\": f\"artifacts/frames/{show}/s{season:02d}/e{episode:02d}/frames/\",\n+        \"crops\": f\"artifacts/crops/{show}/s{season:02d}/e{episode:02d}/tracks/\",\n+        \"manifests\": f\"artifacts/manifests/{show}/s{season:02d}/e{episode:02d}/\",\n+    }\n+\n+\n+def update_progress_display(\n+    progress: Dict[str, Any],\n+    *,\n+    progress_bar,\n+    status_placeholder,\n+    detail_placeholder,\n+    requested_device: str,\n+    requested_detector: str | None,\n+    requested_tracker: str | None,\n+) -> tuple[str, str]:\n+    ratio = progress_ratio(progress)\n+    progress_bar.progress(ratio)\n+    status_line, frames_line = compose_progress_text(\n+        progress,\n+        requested_device=requested_device,\n+        requested_detector=requested_detector,\n+        requested_tracker=requested_tracker,\n+    )\n+    status_placeholder.write(status_line)\n+    detail_placeholder.caption(frames_line)\n+    return status_line, frames_line\n+\n+\n+def compose_progress_text(\n+    progress: Dict[str, Any],\n+    *,\n+    requested_device: str,\n+    requested_detector: str | None,\n+    requested_tracker: str | None,\n+) -> tuple[str, str]:\n+    video_time = progress.get(\"video_time\")\n+    video_total = progress.get(\"video_total\")\n+    phase = progress.get(\"phase\") or \"detect\"\n+    device_label = progress.get(\"device\") or requested_device or \"--\"\n+    resolved_detector_device = progress.get(\"resolved_device\")\n+    device_text = device_label\n+    if resolved_detector_device and resolved_detector_device != device_label:\n+        device_text = f\"{device_label} (detector={resolved_detector_device})\"\n+    raw_detector = progress.get(\"detector\") or requested_detector\n+    detector_label = detector_label_from_value(raw_detector) if raw_detector else \"--\"\n+    raw_tracker = progress.get(\"tracker\") or requested_tracker\n+    tracker_label = tracker_label_from_value(raw_tracker) if raw_tracker else \"--\"\n+    fps_value = progress.get(\"fps_infer\") or progress.get(\"analyzed_fps\") or progress.get(\"fps_detected\")\n+    fps_text = f\"{fps_value:.2f} fps\" if fps_value else \"--\"\n+    show_time = video_time is not None and video_total is not None\n+    time_prefix = \"\"\n+    if show_time:\n+        try:\n+            done_value = float(video_time)\n+            total_value = float(video_total)\n+        except (TypeError, ValueError):\n+            show_time = False","path":"apps/workspace-ui/ui_helpers.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Variable show_time is not used.","created_at":"2025-11-11T20:00:03Z","updated_at":"2025-11-11T20:00:21Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565074","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565074"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565074"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565074/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":638,"original_line":638,"side":"RIGHT","author_association":"NONE","original_position":638,"position":638,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565091","pull_request_review_id":3449784939,"id":2515565091,"node_id":"PRRC_kwDOQRcQs86V8Hoj","diff_hunk":"@@ -0,0 +1,3034 @@\n+#!/usr/bin/env python\n+\"\"\"Dev-only CLI to run detection → tracking for a single episode.\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import math\n+import os\n+import shutil\n+import sys\n+import time\n+from collections import Counter, defaultdict\n+from dataclasses import dataclass, field\n+from datetime import datetime\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, Iterator, List, Optional, Tuple, Callable\n+\n+import logging\n+from functools import lru_cache\n+\n+import numpy as np\n+\n+REPO_ROOT = Path(__file__).resolve().parents[1]\n+if str(REPO_ROOT) not in sys.path:\n+    sys.path.insert(0, str(REPO_ROOT))\n+\n+from apps.api.services.storage import (\n+    EpisodeContext,\n+    StorageService,\n+    artifact_prefixes,\n+    episode_context_from_id,\n+)\n+from py_screenalytics.artifacts import ensure_dirs, get_path\n+from tools._img_utils import clip_bbox, safe_crop, safe_imwrite, to_u8_bgr\n+from tools.debug_thumbs import init_debug_logger, debug_thumbs_enabled, NullLogger, JsonlLogger\n+\n+PIPELINE_VERSION = os.environ.get(\"SCREENALYTICS_PIPELINE_VERSION\", \"2025-11-11\")\n+APP_VERSION = os.environ.get(\"SCREENALYTICS_APP_VERSION\", PIPELINE_VERSION)\n+YOLO_MODEL_NAME = os.environ.get(\"SCREENALYTICS_YOLO_MODEL\", \"yolov8n.pt\")\n+TRACKER_CONFIG = os.environ.get(\"SCREENALYTICS_TRACKER_CONFIG\", \"bytetrack.yaml\")\n+TRACKER_NAME = Path(TRACKER_CONFIG).stem if TRACKER_CONFIG else \"bytetrack\"\n+YOLO_IMAGE_SIZE = int(os.environ.get(\"SCREENALYTICS_YOLO_IMGSZ\", 640))\n+YOLO_CONF_THRESHOLD = float(os.environ.get(\"SCREENALYTICS_YOLO_CONF\", 0.25))\n+YOLO_IOU_THRESHOLD = float(os.environ.get(\"SCREENALYTICS_YOLO_IOU\", 0.45))\n+PROGRESS_FRAME_STEP = int(os.environ.get(\"SCREENALYTICS_PROGRESS_FRAME_STEP\", 25))\n+LOGGER = logging.getLogger(\"episode_run\")\n+DATA_ROOT = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+DETECTOR_CHOICES = (\"retinaface\", \"yolov8face\")\n+DEFAULT_DETECTOR = DETECTOR_CHOICES[0]\n+TRACKER_CHOICES = (\"bytetrack\", \"strongsort\")\n+DEFAULT_TRACKER = TRACKER_CHOICES[0]\n+ARC_FACE_MODEL_NAME = os.environ.get(\"ARCFACE_MODEL\", \"arcface_r100_v1\")\n+RETINAFACE_MODEL_NAME = os.environ.get(\"RETINAFACE_MODEL\", \"retinaface_r50_v1\")\n+FACE_CLASS_LABEL = \"face\"\n+MIN_FACE_AREA = 20.0\n+FACE_RATIO_BOUNDS = (0.5, 2.0)\n+RETINAFACE_SCORE_THRESHOLD = 0.5\n+RETINAFACE_NMS = 0.45\n+\n+RUN_MARKERS_SUBDIR = \"runs\"\n+def _parse_retinaface_det_size(value: str | None) -> tuple[int, int] | None:\n+    if not value:\n+        return 640, 640\n+    tokens: list[str] = []\n+    buf = value.replace(\"x\", \",\").replace(\"X\", \",\")\n+    for part in buf.split(\",\"):\n+        part = part.strip()\n+        if part:\n+            tokens.append(part)\n+    if len(tokens) != 2:\n+        return 640, 640\n+    try:\n+        width = max(int(float(tokens[0])), 1)\n+        height = max(int(float(tokens[1])), 1)\n+        return width, height\n+    except ValueError:\n+        return 640, 640\n+\n+\n+RETINAFACE_DET_SIZE = _parse_retinaface_det_size(os.environ.get(\"RETINAFACE_DET_SIZE\"))\n+\n+\n+def _normalize_det_thresh(value: float | str | None) -> float:\n+    try:\n+        numeric = float(value) if value is not None else RETINAFACE_SCORE_THRESHOLD\n+    except (TypeError, ValueError):\n+        numeric = RETINAFACE_SCORE_THRESHOLD\n+    return min(max(numeric, 0.0), 1.0)\n+YOLO_FACE_CONF = 0.5\n+BYTE_TRACK_MIN_BOX_AREA = 20.0\n+DEFAULT_GMC_METHOD = os.environ.get(\"SCREENALYTICS_GMC_METHOD\", \"sparseOptFlow\")\n+DEFAULT_REID_MODEL = os.environ.get(\"SCREENALYTICS_REID_MODEL\", \"yolov8n-cls.pt\")\n+DEFAULT_REID_ENABLED = os.environ.get(\"SCREENALYTICS_REID_ENABLED\", \"1\").lower() in {\"1\", \"true\", \"yes\"}\n+RETINAFACE_HELP = \"RetinaFace weights missing or could not initialize. See README 'Models' or run scripts/fetch_models.py.\"\n+\n+\n+def _env_flag(name: str, default: bool) -> bool:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    value = raw.strip().lower()\n+    if value in {\"1\", \"true\", \"yes\", \"on\"}:\n+        return True\n+    if value in {\"0\", \"false\", \"off\", \"no\"}:\n+        return False\n+    return default\n+\n+\n+def _env_float(name: str, default: float) -> float:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    try:\n+        return float(raw)\n+    except (TypeError, ValueError):\n+        return default\n+\n+\n+def _env_int(name: str, default: int) -> int:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    try:\n+        return int(raw)\n+    except (TypeError, ValueError):\n+        return default\n+\n+\n+def _resolve_track_sample_limit(value: str | int | None) -> int | None:\n+    if value is None:\n+        return None\n+    if isinstance(value, str):\n+        text = value.strip().lower()\n+        if text in {\"\", \"none\", \"unlimited\", \"all\", \"off\", \"disable\"}:\n+            return None\n+        try:\n+            numeric = int(float(text))\n+        except ValueError:\n+            return None\n+    else:\n+        numeric = int(value)\n+    return numeric if numeric > 0 else None\n+\n+\n+def _set_track_sample_limit(value: int | None) -> None:\n+    global TRACK_SAMPLE_LIMIT\n+    TRACK_SAMPLE_LIMIT = _resolve_track_sample_limit(value)\n+\n+\n+TRACK_SAMPLE_LIMIT = _resolve_track_sample_limit(os.environ.get(\"SCREENALYTICS_TRACK_SAMPLE_LIMIT\"))\n+\n+\n+SCENE_DETECT_DEFAULT = _env_flag(\"SCENE_DETECT\", True)\n+SCENE_THRESHOLD_DEFAULT = max(min(_env_float(\"SCENE_THRESHOLD\", 0.30), 2.0), 0.0)\n+SCENE_MIN_LEN_DEFAULT = max(_env_int(\"SCENE_MIN_LEN\", 12), 1)\n+SCENE_WARMUP_DETS_DEFAULT = max(_env_int(\"SCENE_WARMUP_DETS\", 3), 0)\n+\n+\n+def _normalize_device_label(device: str | None) -> str:\n+    normalized = (device or \"cpu\").lower()\n+    if normalized in {\"0\", \"cuda\", \"gpu\"}:\n+        return \"cuda\"\n+    return normalized\n+\n+\n+def _onnx_providers_for(device: str | None) -> tuple[list[str], str]:\n+    normalized = (device or \"auto\").lower()\n+    providers: list[str] = [\"CPUExecutionProvider\"]\n+    resolved = \"cpu\"\n+    if normalized in {\"cuda\", \"0\", \"gpu\", \"auto\"}:\n+        try:\n+            import onnxruntime as ort  # type: ignore\n+\n+            available = ort.get_available_providers()\n+        except Exception:\n+            available = []\n+        if \"CUDAExecutionProvider\" in available:\n+            providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n+            resolved = \"cuda\"\n+            return providers, resolved\n+        if normalized in {\"cuda\", \"0\", \"gpu\"}:\n+            LOGGER.warning(\"CUDA requested for RetinaFace/ArcFace but CUDAExecutionProvider unavailable; falling back to CPU\")\n+    if normalized in {\"mps\", \"metal\", \"apple\"}:\n+        return [\"CPUExecutionProvider\"], \"cpu\"\n+    return providers, resolved\n+\n+\n+def _init_retinaface(model_name: str, device: str, score_thresh: float = RETINAFACE_SCORE_THRESHOLD) -> tuple[Any, str]:\n+    try:\n+        from insightface.model_zoo import get_model  # type: ignore\n+    except ImportError as exc:  # pragma: no cover - runtime guard\n+        raise RuntimeError(\"insightface is required for RetinaFace detection\") from exc\n+\n+    providers, resolved = _onnx_providers_for(device)\n+    model = get_model(model_name)\n+    if model is None:\n+        raise RuntimeError(\n+            f\"RetinaFace weights '{model_name}' not found. Install insightface models or run scripts/fetch_models.py.\"\n+        )\n+    ctx_id = 0 if resolved == \"cuda\" else -1\n+    # InsightFace 0.7.x configures detection threshold at prepare-time\n+    # (detect() no longer accepts a `threshold` kwarg).\n+    prepare_kwargs = {\n+        \"ctx_id\": ctx_id,\n+        \"providers\": providers,\n+        \"nms\": RETINAFACE_NMS,\n+        \"det_thresh\": float(score_thresh),\n+    }\n+    if RETINAFACE_DET_SIZE:\n+        prepare_kwargs[\"input_size\"] = RETINAFACE_DET_SIZE\n+    try:\n+        model.prepare(**prepare_kwargs)\n+    except TypeError:\n+        prepare_kwargs.pop(\"input_size\", None)\n+        model.prepare(**prepare_kwargs)\n+    return model, resolved\n+\n+\n+def _init_arcface(model_name: str, device: str):\n+    try:\n+        from insightface.model_zoo import get_model  # type: ignore\n+    except ImportError as exc:  # pragma: no cover\n+        raise RuntimeError(\"insightface is required for ArcFace embeddings\") from exc\n+\n+    providers, resolved = _onnx_providers_for(device)\n+    model = get_model(model_name)\n+    if model is None:\n+        raise RuntimeError(\n+            f\"ArcFace weights '{model_name}' not found. Install insightface models or run scripts/fetch_models.py.\"\n+        )\n+    ctx_id = 0 if resolved == \"cuda\" else -1\n+    model.prepare(ctx_id=ctx_id, providers=providers)\n+    return model, resolved\n+\n+\n+def ensure_retinaface_ready(device: str, det_thresh: float | None = None) -> tuple[bool, Optional[str], Optional[str]]:\n+    \"\"\"Lightweight readiness probe for API preflight checks.\"\"\"\n+\n+    model = None\n+    try:\n+        model, resolved = _init_retinaface(\n+            RETINAFACE_MODEL_NAME,\n+            device,\n+            det_thresh if det_thresh is not None else RETINAFACE_SCORE_THRESHOLD,\n+        )\n+    except Exception as exc:  # pragma: no cover - surfaced via API tests\n+        return False, str(exc), None\n+    finally:\n+        if model is not None:\n+            del model\n+    return True, None, resolved\n+\n+\n+def pick_device(explicit: str | None = None) -> str:\n+    \"\"\"Return the safest device available.\n+\n+    Order of preference: explicit override → CUDA → MPS → CPU.\n+    Values returned are what Ultralytics expects (\"cpu\", \"mps\", \"cuda\"/\"0\").\n+    \"\"\"\n+\n+    if explicit and explicit not in {\"auto\", \"\"}:\n+        return explicit\n+\n+    try:\n+        import torch  # type: ignore\n+\n+        if torch.cuda.is_available():  # pragma: no cover - depends on env\n+            return \"0\"\n+        mps_available = getattr(torch.backends, \"mps\", None)\n+        if mps_available is not None and mps_available.is_available():  # pragma: no cover - mac only\n+            return \"mps\"\n+    except Exception:  # pragma: no cover - torch import/runtime guard\n+        pass\n+\n+    return \"cpu\"\n+\n+\n+def _normalize_detector_choice(detector: str | None) -> str:\n+    if detector:\n+        value = detector.strip().lower()\n+        if value in DETECTOR_CHOICES:\n+            return value\n+    return DEFAULT_DETECTOR\n+\n+\n+def _normalize_tracker_choice(tracker: str | None) -> str:\n+    if tracker:\n+        value = tracker.strip().lower()\n+        if value in TRACKER_CHOICES:\n+            return value\n+    return DEFAULT_TRACKER\n+\n+\n+def _valid_face_box(bbox: np.ndarray, score: float, *, min_score: float, min_area: float) -> bool:\n+    width = bbox[2] - bbox[0]\n+    height = bbox[3] - bbox[1]\n+    area = max(width, 0.0) * max(height, 0.0)\n+    if score < min_score:\n+        return False\n+    if area < min_area:\n+        return False\n+    ratio = width / max(height, 1e-6)\n+    return FACE_RATIO_BOUNDS[0] <= ratio <= FACE_RATIO_BOUNDS[1]\n+\n+\n+def _nms_detections(\n+    detections: list[tuple[np.ndarray, float, np.ndarray | None]],\n+    thresh: float,\n+) -> list[tuple[np.ndarray, float, np.ndarray | None]]:\n+    ordered = sorted(range(len(detections)), key=lambda idx: detections[idx][1], reverse=True)\n+    keep: list[tuple[np.ndarray, float, np.ndarray | None]] = []\n+    while ordered:\n+        current_idx = ordered.pop(0)\n+        current = detections[current_idx]\n+        keep.append(current)\n+        remaining: list[int] = []\n+        for idx in ordered:\n+            iou = _bbox_iou(current[0].tolist(), detections[idx][0].tolist())\n+            if iou < thresh:\n+                remaining.append(idx)\n+        ordered = remaining\n+    return keep\n+\n+\n+@dataclass\n+class TrackAccumulator:\n+    track_id: int\n+    class_id: int | str\n+    first_ts: float\n+    last_ts: float\n+    frame_count: int = 0\n+    samples: List[dict] = field(default_factory=list)\n+\n+    def add(self, ts: float, frame_idx: int, bbox_xyxy: List[float], landmarks: List[float] | None = None) -> None:\n+        self.frame_count += 1\n+        self.last_ts = ts\n+        limit = TRACK_SAMPLE_LIMIT\n+        if limit is None or len(self.samples) < limit:\n+            self.samples.append(\n+                {\n+                    \"frame_idx\": frame_idx,\n+                    \"ts\": round(float(ts), 4),\n+                    \"bbox_xyxy\": [round(float(coord), 4) for coord in bbox_xyxy],\n+                    **({\"landmarks\": [round(float(val), 4) for val in landmarks]} if landmarks else {}),\n+                }\n+            )\n+\n+    def to_row(self) -> dict:\n+        row = {\n+            \"track_id\": self.track_id,\n+            \"class\": self.class_id,\n+            \"first_ts\": round(float(self.first_ts), 4),\n+            \"last_ts\": round(float(self.last_ts), 4),\n+            \"frame_count\": self.frame_count,\n+            \"pipeline_ver\": PIPELINE_VERSION,\n+        }\n+        if self.samples:\n+            row[\"bboxes_sampled\"] = self.samples\n+        return row\n+\n+\n+@dataclass\n+class DetectionSample:\n+    bbox: np.ndarray\n+    conf: float\n+    class_idx: int\n+    class_label: str\n+    landmarks: np.ndarray | None = None\n+    embedding: np.ndarray | None = None\n+\n+\n+@dataclass\n+class TrackedObject:\n+    track_id: int\n+    bbox: np.ndarray\n+    conf: float\n+    class_idx: int\n+    class_label: str\n+    det_index: int | None = None\n+    landmarks: np.ndarray | None = None\n+\n+\n+class _TrackerDetections:\n+    \"\"\"Lightweight structure that mimics ultralytics' Boxes for BYTETracker inputs.\"\"\"\n+\n+    def __init__(self, boxes: np.ndarray, scores: np.ndarray, classes: np.ndarray) -> None:\n+        self.xyxy = boxes.astype(np.float32)\n+        self.conf = scores.astype(np.float32)\n+        self.cls = classes.astype(np.float32)\n+        self._xywh: np.ndarray | None = None\n+\n+    @property\n+    def xywh(self) -> np.ndarray:\n+        if self._xywh is None:\n+            self._xywh = self.xyxy.copy()\n+            self._xywh[:, 2] = self._xywh[:, 2] - self._xywh[:, 0]\n+            self._xywh[:, 3] = self._xywh[:, 3] - self._xywh[:, 1]\n+            self._xywh[:, 0] = self._xywh[:, 0] + self._xywh[:, 2] / 2\n+            self._xywh[:, 1] = self._xywh[:, 1] + self._xywh[:, 3] / 2\n+        return self._xywh\n+\n+    @property\n+    def xywhr(self) -> np.ndarray:\n+        return self.xywh\n+\n+\n+def _tracker_inputs_from_samples(detections: list[DetectionSample]) -> _TrackerDetections:\n+    if detections:\n+        boxes = np.vstack([sample.bbox for sample in detections]).astype(np.float32)\n+        scores = np.asarray([sample.conf for sample in detections], dtype=np.float32)\n+        classes = np.asarray([sample.class_idx for sample in detections], dtype=np.float32)\n+        return _TrackerDetections(boxes, scores, classes)\n+    return _TrackerDetections(\n+        np.zeros((0, 4), dtype=np.float32),\n+        np.zeros(0, dtype=np.float32),\n+        np.zeros(0, dtype=np.float32),\n+    )\n+\n+\n+class ByteTrackAdapter:\n+    \"\"\"Wrapper around ultralytics BYTETracker for direct invocation.\"\"\"\n+\n+    def __init__(self, frame_rate: float = 30.0) -> None:\n+        self.frame_rate = max(frame_rate, 1)\n+        self._tracker = self._build_tracker()\n+\n+    def _build_tracker(self):\n+        from types import SimpleNamespace\n+\n+        from ultralytics.trackers.byte_tracker import BYTETracker\n+\n+        cfg = SimpleNamespace(\n+            tracker_type=\"bytetrack\",\n+            track_high_thresh=0.6,\n+            track_low_thresh=0.1,\n+            new_track_thresh=0.6,\n+            track_buffer=30,\n+            match_thresh=0.8,\n+            min_box_area=BYTE_TRACK_MIN_BOX_AREA,\n+        )\n+        return BYTETracker(cfg, frame_rate=self.frame_rate)\n+\n+    def update(self, detections: list[DetectionSample], frame_idx: int, image) -> list[TrackedObject]:\n+        det_struct = _tracker_inputs_from_samples(detections)\n+        tracks = self._tracker.update(det_struct, image)\n+        tracked: list[TrackedObject] = []\n+        if tracks.size == 0:\n+            return tracked\n+        for row in tracks:\n+            bbox = np.asarray(row[:4], dtype=np.float32)\n+            track_id = int(row[4])\n+            score = float(row[5])\n+            class_idx = int(row[6]) if len(row) > 6 else 0\n+            det_index = int(row[7]) if len(row) > 7 else None\n+            label = \"\"\n+            landmarks = None\n+            if det_index is not None and 0 <= det_index < len(detections):\n+                det = detections[det_index]\n+                label = det.class_label\n+                class_idx = det.class_idx\n+                landmarks = det.landmarks\n+            tracked.append(\n+                TrackedObject(\n+                    track_id=track_id,\n+                    bbox=bbox,\n+                    conf=score,\n+                    class_idx=class_idx,\n+                    class_label=label,\n+                    det_index=det_index,\n+                    landmarks=landmarks,\n+                )\n+            )\n+        return tracked\n+\n+    def reset(self) -> None:\n+        self._tracker = self._build_tracker()\n+\n+\n+class StrongSortAdapter:\n+    \"\"\"Adapter around Ultralytics BOT-SORT tracker (used as a StrongSORT-style ReID tracker).\"\"\"\n+\n+    def __init__(self, frame_rate: float = 30.0) -> None:\n+        self.frame_rate = max(frame_rate, 1)\n+        self._tracker = self._build_tracker()\n+\n+    def _build_tracker(self):\n+        from types import SimpleNamespace\n+\n+        try:\n+            from ultralytics.trackers.bot_sort import BOTSORT\n+        except ImportError as exc:  # pragma: no cover - dependency missing\n+            raise RuntimeError(\n+                \"StrongSORT tracker unavailable; ensure ultralytics>=8.2.70 is installed.\"\n+            ) from exc\n+\n+        cfg = SimpleNamespace(\n+            tracker_type=\"strongsort\",\n+            track_high_thresh=0.6,\n+            track_low_thresh=0.1,\n+            new_track_thresh=0.6,\n+            track_buffer=30,\n+            match_thresh=0.8,\n+            min_box_area=BYTE_TRACK_MIN_BOX_AREA,\n+            gmc_method=os.environ.get(\"SCREENALYTICS_GMC_METHOD\", DEFAULT_GMC_METHOD),\n+            proximity_thresh=float(os.environ.get(\"SCREENALYTICS_REID_PROXIMITY\", \"0.6\")),\n+            appearance_thresh=float(os.environ.get(\"SCREENALYTICS_REID_APPEARANCE\", \"0.7\")),\n+            with_reid=_env_flag(\"SCREENALYTICS_REID_ENABLED\", DEFAULT_REID_ENABLED),\n+            model=os.environ.get(\"SCREENALYTICS_REID_MODEL\", DEFAULT_REID_MODEL) or \"auto\",\n+            fuse_score=_env_flag(\"SCREENALYTICS_REID_FUSE_SCORE\", False),\n+        )\n+        return BOTSORT(cfg, frame_rate=self.frame_rate)\n+\n+    def update(self, detections: list[DetectionSample], frame_idx: int, image) -> list[TrackedObject]:\n+        det_struct = _tracker_inputs_from_samples(detections)\n+        tracks = self._tracker.update(det_struct, image)\n+        tracked: list[TrackedObject] = []\n+        if tracks.size == 0:\n+            return tracked\n+        for row in tracks:\n+            bbox = np.asarray(row[:4], dtype=np.float32)\n+            track_id = int(row[4])\n+            score = float(row[5])\n+            class_idx = int(row[6]) if len(row) > 6 else 0\n+            det_index = int(row[7]) if len(row) > 7 else None\n+            label = \"\"\n+            landmarks = None\n+            if det_index is not None and 0 <= det_index < len(detections):\n+                det = detections[det_index]\n+                label = det.class_label\n+                class_idx = det.class_idx\n+                landmarks = det.landmarks\n+            tracked.append(\n+                TrackedObject(\n+                    track_id=track_id,\n+                    bbox=bbox,\n+                    conf=score,\n+                    class_idx=class_idx,\n+                    class_label=label,\n+                    det_index=det_index,\n+                    landmarks=landmarks,\n+                )\n+            )\n+        return tracked\n+\n+    def reset(self) -> None:\n+        self._tracker = self._build_tracker()\n+\n+\n+class RetinaFaceDetectorBackend:\n+    def __init__(self, device: str, score_thresh: float = RETINAFACE_SCORE_THRESHOLD) -> None:\n+        self.device = device\n+        self.score_thresh = max(min(float(score_thresh or RETINAFACE_SCORE_THRESHOLD), 1.0), 0.0)\n+        self.min_area = MIN_FACE_AREA\n+        self._model = None\n+        self._resolved_device: Optional[str] = None\n+\n+    def _lazy_model(self):\n+        if self._model is not None:\n+            return self._model\n+        try:\n+            model, resolved = _init_retinaface(self.model_name, self.device, self.score_thresh)\n+        except Exception as exc:\n+            raise RuntimeError(f\"{RETINAFACE_HELP} ({exc})\") from exc\n+        self._resolved_device = resolved\n+        self._model = model\n+        return self._model\n+\n+    @property\n+    def model_name(self) -> str:\n+        return RETINAFACE_MODEL_NAME\n+\n+    @property\n+    def resolved_device(self) -> str:\n+        if self._resolved_device is None:\n+            self.ensure_ready()\n+        return self._resolved_device or \"cpu\"\n+\n+    def ensure_ready(self) -> None:\n+        self._lazy_model()\n+\n+    def detect(self, image) -> list[DetectionSample]:\n+        model = self._lazy_model()\n+        # Threshold + input size configured during model.prepare. Some InsightFace\n+        # RetinaFace builds still require an explicit input_size, so pass it when\n+        # available.\n+        detect_kwargs = {}\n+        input_size = getattr(model, \"input_size\", None) or RETINAFACE_DET_SIZE\n+        if input_size:\n+            detect_kwargs[\"input_size\"] = input_size\n+        bboxes, landmarks = model.detect(image, **detect_kwargs)\n+        if bboxes is None or len(bboxes) == 0:\n+            return []\n+        pending: list[tuple[np.ndarray, float, np.ndarray | None]] = []\n+        for idx in range(len(bboxes)):\n+            raw = bboxes[idx]\n+            score = float(raw[4]) if raw.shape[0] >= 5 else float(self.score_thresh)\n+            bbox = raw[:4].astype(np.float32)\n+            if not _valid_face_box(bbox, score, min_score=self.score_thresh, min_area=self.min_area):\n+                continue\n+            kps = None\n+            if landmarks is not None and idx < len(landmarks):\n+                kps = landmarks[idx].astype(np.float32).reshape(-1)\n+            pending.append((bbox, score, kps))\n+        filtered = _nms_detections(pending, RETINAFACE_NMS) if pending else []\n+        samples: list[DetectionSample] = []\n+        for bbox, score, kps in filtered:\n+            samples.append(\n+                DetectionSample(\n+                    bbox=bbox.astype(np.float32),\n+                    conf=score,\n+                    class_idx=0,\n+                    class_label=FACE_CLASS_LABEL,\n+                    landmarks=kps.copy() if isinstance(kps, np.ndarray) else None,\n+                )\n+            )\n+        return samples\n+\n+\n+class YoloFaceDetectorBackend:\n+    def __init__(self, device: str) -> None:\n+        from ultralytics import YOLO\n+\n+        self.device = device\n+        self.model_path = os.environ.get(\"SCREENALYTICS_YOLO_FACE_MODEL\", \"yolov8n-face.pt\")\n+        self._model = YOLO(self.model_path)\n+        self._resolved_device = _normalize_device_label(device)\n+\n+    @property\n+    def model_name(self) -> str:\n+        return self.model_path\n+\n+    @property\n+    def resolved_device(self) -> str:\n+        return self._resolved_device\n+\n+    def ensure_ready(self) -> None:\n+        _ = self._model\n+\n+    def detect(self, image) -> list[DetectionSample]:\n+        results = self._model.predict(\n+            source=image,\n+            imgsz=YOLO_IMAGE_SIZE,\n+            conf=YOLO_FACE_CONF,\n+            device=self.device,\n+            verbose=False,\n+        )\n+        samples: list[DetectionSample] = []\n+        if not results:\n+            return samples\n+        boxes = results[0].boxes\n+        if boxes is None or boxes.data is None or len(boxes) == 0:\n+            return samples\n+        xyxy = boxes.xyxy.cpu().numpy()\n+        scores = boxes.conf.cpu().numpy()\n+        for idx in range(len(xyxy)):\n+            bbox = xyxy[idx].astype(np.float32)\n+            score = float(scores[idx])\n+            if not _valid_face_box(bbox, score, min_score=YOLO_FACE_CONF, min_area=MIN_FACE_AREA):\n+                continue\n+            samples.append(\n+                DetectionSample(\n+                    bbox=bbox,\n+                    conf=score,\n+                    class_idx=0,\n+                    class_label=FACE_CLASS_LABEL,\n+                )\n+            )\n+        return samples\n+\n+\n+def _build_face_detector(detector: str, device: str, score_thresh: float = RETINAFACE_SCORE_THRESHOLD):\n+    if detector == \"yolov8face\":\n+        return YoloFaceDetectorBackend(device)\n+    return RetinaFaceDetectorBackend(device, score_thresh=score_thresh)\n+\n+\n+def _build_tracker_adapter(tracker: str, frame_rate: float) -> ByteTrackAdapter | StrongSortAdapter:\n+    if tracker == \"strongsort\":\n+        return StrongSortAdapter(frame_rate=frame_rate)\n+    return ByteTrackAdapter(frame_rate=frame_rate)\n+\n+\n+class ArcFaceEmbedder:\n+    def __init__(self, device: str) -> None:\n+        self.device = device\n+        self._model = None\n+        self._resolved_device: Optional[str] = None\n+\n+    def _lazy_model(self):\n+        if self._model is not None:\n+            return self._model\n+        try:\n+            model, resolved = _init_arcface(ARC_FACE_MODEL_NAME, self.device)\n+        except Exception as exc:\n+            raise RuntimeError(f\"ArcFace init failed: {exc}. Install insightface + models or run scripts/fetch_models.py.\") from exc\n+        self._resolved_device = resolved\n+        self._model = model\n+        return self._model\n+\n+    def ensure_ready(self) -> None:\n+        self._lazy_model()\n+\n+    @property\n+    def resolved_device(self) -> str:\n+        if self._resolved_device is None:\n+            self.ensure_ready()\n+        return self._resolved_device or \"cpu\"\n+\n+    def encode(self, crops: list[np.ndarray]) -> np.ndarray:\n+        if not crops:\n+            return np.zeros((0, 512), dtype=np.float32)\n+        model = self._lazy_model()\n+        embeddings: list[np.ndarray] = []\n+        for crop in crops:\n+            if crop is None or crop.size == 0:\n+                embeddings.append(np.zeros(512, dtype=np.float32))\n+                continue\n+            resized = _resize_for_arcface(crop)\n+            feat = model.get_feat(resized)\n+            vec = np.asarray(feat, dtype=np.float32)\n+            norm = np.linalg.norm(vec)\n+            if norm > 0:\n+                vec = vec / norm\n+            embeddings.append(vec)\n+        return np.vstack(embeddings)\n+\n+\n+def _resize_for_arcface(image):\n+    import cv2  # type: ignore\n+\n+    target = (112, 112)\n+    resized = cv2.resize(image, target)\n+    return resized\n+\n+\n+def _prepare_face_crop(\n+    image, bbox: list[float], landmarks: list[float] | None, margin: float = 0.15\n+) -> tuple[np.ndarray | None, str | None]:\n+    import numpy as _np\n+\n+    if landmarks and len(landmarks) >= 10:\n+        try:\n+            from insightface.utils import face_align  # type: ignore\n+\n+            pts = _np.asarray(landmarks, dtype=_np.float32).reshape(-1, 2)\n+            aligned = face_align.norm_crop(image, landmark=pts)\n+            return to_u8_bgr(aligned), None\n+        except Exception:\n+            pass\n+    x1, y1, x2, y2 = bbox\n+    width = max(x2 - x1, 1.0)\n+    height = max(y2 - y1, 1.0)\n+    expand_x = width * margin\n+    expand_y = height * margin\n+    expanded_box = [\n+        x1 - expand_x,\n+        y1 - expand_y,\n+        x2 + expand_x,\n+        y2 + expand_y,\n+    ]\n+    crop, _, err = safe_crop(image, expanded_box)\n+    if crop is None:\n+        return None, err or \"crop_failed\"\n+    return crop, None\n+\n+\n+def _make_skip_face_row(\n+    ep_id: str,\n+    track_id: int,\n+    frame_idx: int,\n+    ts_val: float,\n+    bbox: list[float],\n+    detector_choice: str,\n+    reason: str,\n+    *,\n+    crop_rel_path: str | None = None,\n+    crop_s3_key: str | None = None,\n+    thumb_rel_path: str | None = None,\n+    thumb_s3_key: str | None = None,\n+) -> Dict[str, Any]:\n+    row: Dict[str, Any] = {\n+        \"ep_id\": ep_id,\n+        \"face_id\": f\"face_{track_id:04d}_{frame_idx:06d}\",\n+        \"track_id\": track_id,\n+        \"frame_idx\": frame_idx,\n+        \"ts\": ts_val,\n+        \"bbox_xyxy\": bbox,\n+        \"detector\": detector_choice,\n+        \"pipeline_ver\": PIPELINE_VERSION,\n+        \"skip\": reason,\n+    }\n+    if crop_rel_path:\n+        row[\"crop_rel_path\"] = crop_rel_path\n+    if crop_s3_key:\n+        row[\"crop_s3_key\"] = crop_s3_key\n+    if thumb_rel_path:\n+        row[\"thumb_rel_path\"] = thumb_rel_path\n+    if thumb_s3_key:\n+        row[\"thumb_s3_key\"] = thumb_s3_key\n+    return row\n+\n+\n+class TrackRecorder:\n+    \"\"\"Maintains exported track ids, metrics, and sampled boxes.\"\"\"\n+\n+    def __init__(self, *, max_gap: int, remap_ids: bool) -> None:\n+        self.max_gap = max(1, int(max_gap))\n+        self.remap_ids = remap_ids\n+        self._next_export_id = 1\n+        self._mapping: dict[int, dict[str, int]] = {}\n+        self._active_exports: set[int] = set()\n+        self._accumulators: dict[int, TrackAccumulator] = {}\n+        self.metrics = {\"tracks_born\": 0, \"tracks_lost\": 0, \"id_switches\": 0}\n+\n+    def _spawn_export_id(self) -> int:\n+        export_id = self._next_export_id\n+        self._next_export_id += 1\n+        self._active_exports.add(export_id)\n+        self.metrics[\"tracks_born\"] += 1\n+        return export_id\n+\n+    def _complete_track(self, export_id: int) -> None:\n+        if export_id in self._active_exports:\n+            self._active_exports.remove(export_id)\n+            self.metrics[\"tracks_lost\"] += 1\n+\n+    def record(\n+        self,\n+        *,\n+        tracker_track_id: int,\n+        frame_idx: int,\n+        ts: float,\n+        bbox: list[float] | np.ndarray,\n+        class_label: int | str,\n+        landmarks: list[float] | None = None,\n+    ) -> int:\n+        if isinstance(bbox, np.ndarray):\n+            bbox_values = bbox.tolist()\n+        else:\n+            bbox_values = bbox\n+        export_id: int\n+        mapping = self._mapping.get(tracker_track_id)\n+        if self.remap_ids:\n+            start_new = mapping is None\n+            if mapping is not None:\n+                gap = frame_idx - mapping.get(\"last_frame\", frame_idx)\n+                if gap > self.max_gap:\n+                    self.metrics[\"id_switches\"] += 1\n+                    self._complete_track(mapping[\"export_id\"])\n+                    start_new = True\n+            if start_new:\n+                export_id = self._spawn_export_id()\n+            else:\n+                export_id = mapping[\"export_id\"]\n+            self._mapping[tracker_track_id] = {\"export_id\": export_id, \"last_frame\": frame_idx}\n+        else:\n+            if mapping is None:\n+                export_id = tracker_track_id\n+                self._active_exports.add(export_id)\n+                self._mapping[tracker_track_id] = {\"export_id\": export_id, \"last_frame\": frame_idx}\n+                self.metrics[\"tracks_born\"] += 1\n+            else:\n+                export_id = mapping[\"export_id\"]\n+                mapping[\"last_frame\"] = frame_idx\n+        track = self._accumulators.get(export_id)\n+        if track is None:\n+            track = TrackAccumulator(track_id=export_id, class_id=class_label, first_ts=ts, last_ts=ts)\n+            self._accumulators[export_id] = track\n+        track.add(ts, frame_idx, bbox_values, landmarks=landmarks)\n+        return export_id\n+\n+    def finalize(self) -> None:\n+        for export_id in list(self._active_exports):\n+            self._complete_track(export_id)\n+        self._mapping.clear()\n+\n+    def rows(self) -> list[dict]:\n+        payload: list[dict] = []\n+        for track in sorted(self._accumulators.values(), key=lambda item: item.track_id):\n+            payload.append(track.to_row())\n+        return payload\n+\n+    def top_long_tracks(self, limit: int = 5) -> list[dict]:\n+        longest = sorted(self._accumulators.values(), key=lambda item: item.frame_count, reverse=True)[:limit]\n+        return [\n+            {\"track_id\": track.track_id, \"frame_count\": track.frame_count}\n+            for track in longest\n+            if track.frame_count > 0\n+        ]\n+\n+\n+def _bbox_iou(box_a: List[float], box_b: List[float]) -> float:\n+    ax1, ay1, ax2, ay2 = box_a\n+    bx1, by1, bx2, by2 = box_b\n+    inter_x1 = max(ax1, bx1)\n+    inter_y1 = max(ay1, by1)\n+    inter_x2 = min(ax2, bx2)\n+    inter_y2 = min(ay2, by2)\n+    inter_w = max(inter_x2 - inter_x1, 0.0)\n+    inter_h = max(inter_y2 - inter_y1, 0.0)\n+    inter_area = inter_w * inter_h\n+    if inter_area <= 0:\n+        return 0.0\n+    area_a = max(ax2 - ax1, 0.0) * max(ay2 - ay1, 0.0)\n+    area_b = max(bx2 - bx1, 0.0) * max(by2 - by1, 0.0)\n+    denom = area_a + area_b - inter_area\n+    if denom <= 0:\n+        return 0.0\n+    return inter_area / denom\n+\n+\n+def _try_import(module: str):\n+    try:\n+        return __import__(module)\n+    except ImportError:\n+        return None\n+\n+\n+def sanitize_xyxy(x1: float, y1: float, x2: float, y2: float, width: int, height: int) -> tuple[int, int, int, int] | None:\n+    \"\"\"Round + clamp XYXY boxes to integer pixel coordinates, skipping empty windows.\"\"\"\n+    if width <= 0 or height <= 0:\n+        return None\n+    x1_int = int(max(0, min(round(x1), width - 1)))\n+    y1_int = int(max(0, min(round(y1), height - 1)))\n+    x2_int = int(max(0, min(round(x2), width)))\n+    y2_int = int(max(0, min(round(y2), height)))\n+    if x2_int <= x1_int or y2_int <= y1_int:\n+        return None\n+    return x1_int, y1_int, x2_int, y2_int\n+\n+\n+def _image_stats(image) -> tuple[float, float, float]:\n+    arr = np.asarray(image)\n+    if arr.size == 0:\n+        return 0.0, 0.0, 0.0\n+    return float(np.nanmin(arr)), float(np.nanmax(arr)), float(np.nanmean(arr))\n+\n+\n+def _normalize_to_uint8(image: np.ndarray) -> np.ndarray:\n+    arr = np.asarray(image)\n+    if arr.size == 0:\n+        return arr\n+    if arr.dtype == np.uint8:\n+        return arr\n+    if np.issubdtype(arr.dtype, np.floating):\n+        mn = float(np.nanmin(arr))\n+        mx = float(np.nanmax(arr))\n+        if mx <= 1.0 and mn >= 0.0:\n+            arr = (arr * 255.0).round()\n+        elif mn >= -1.0 and mx <= 1.0:\n+            arr = ((arr + 1.0) * 127.5).round()\n+        arr = np.nan_to_num(arr, nan=0.0, posinf=255.0, neginf=0.0)\n+        return np.clip(arr, 0, 255).astype(np.uint8)\n+    if np.issubdtype(arr.dtype, np.integer):\n+        arr = np.clip(arr.astype(np.int64), 0, 255)\n+        return arr.astype(np.uint8)\n+    return arr.astype(np.uint8, copy=False)\n+\n+\n+def save_jpeg(path: str | Path, image, *, quality: int = 85, color: str = \"bgr\") -> None:\n+    \"\"\"Normalize + persist an image to JPEG, ensuring non-blank uint8 BGR data.\"\"\"\n+    import cv2  # type: ignore\n+\n+    arr = np.asarray(image)\n+    if arr.size == 0:\n+        raise ValueError(f\"Cannot save empty image to {path}\")\n+    arr = np.ascontiguousarray(_normalize_to_uint8(arr))\n+    if arr.ndim == 2:\n+        arr = cv2.cvtColor(arr, cv2.COLOR_GRAY2BGR)\n+    elif arr.ndim == 3 and arr.shape[2] == 3:\n+        mode = color.lower()\n+        if mode == \"rgb\":\n+            arr = cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)\n+        elif mode not in {\"bgr\", \"rgb\"}:\n+            raise ValueError(f\"Unsupported color mode '{color}'\")\n+    else:\n+        raise ValueError(f\"Unsupported image shape for JPEG write: {arr.shape}\")\n+    arr = np.ascontiguousarray(arr)\n+    jpeg_quality = max(1, min(int(quality or 85), 100))\n+    out_path = Path(path)\n+    out_path.parent.mkdir(parents=True, exist_ok=True)\n+    ok = cv2.imwrite(str(out_path), arr, [cv2.IMWRITE_JPEG_QUALITY, jpeg_quality])\n+    if not ok:\n+        raise RuntimeError(f\"cv2.imwrite failed for {out_path}\")\n+\n+\n+class ThumbWriter:\n+    def __init__(self, ep_id: str, size: int = 256, jpeg_quality: int = 85) -> None:\n+        self.ep_id = ep_id\n+        self.size = size\n+        self.jpeg_quality = max(1, min(int(jpeg_quality or 85), 100))\n+        self.root_dir = get_path(ep_id, \"frames_root\") / \"thumbs\"\n+        self.root_dir.mkdir(parents=True, exist_ok=True)\n+        self._stat_samples = 0\n+        self._stat_limit = 10\n+        try:\n+            import cv2  # type: ignore\n+\n+            self._cv2 = cv2\n+        except ImportError:\n+            self._cv2 = None\n+\n+    def write(self, image, bbox: List[float], track_id: int, frame_idx: int) -> tuple[str | None, Path | None]:\n+        if self._cv2 is None or image is None:\n+            return None, None\n+        crop, clipped_bbox, err = safe_crop(image, bbox)\n+        if crop is None:\n+            LOGGER.debug(\"Skipping thumb track=%s frame=%s reason=%s\", track_id, frame_idx, err)\n+            return None, None\n+        thumb = self._letterbox(crop)\n+        if self._stat_samples < self._stat_limit:\n+            mn, mx, mean = _image_stats(thumb)\n+            LOGGER.info(\"thumb stats track=%s frame=%s min=%.3f max=%.3f mean=%.3f\", track_id, frame_idx, mn, mx, mean)\n+            if mx - mn < 1e-6:\n+                LOGGER.warning(\"Nearly constant thumb track=%s frame=%s\", track_id, frame_idx)\n+            self._stat_samples += 1\n+        rel_path = Path(f\"track_{track_id:04d}/thumb_{frame_idx:06d}.jpg\")\n+        abs_path = self.root_dir / rel_path\n+        ok, reason = safe_imwrite(abs_path, thumb, self.jpeg_quality)\n+        if not ok:\n+            LOGGER.warning(\"Failed to write thumb %s: %s\", abs_path, reason)\n+            return None, None\n+        return rel_path.as_posix(), abs_path\n+\n+    def _letterbox(self, crop):\n+        if self._cv2 is None:\n+            return np.zeros((self.size, self.size, 3), dtype=np.uint8)\n+        if crop.size == 0:\n+            crop = np.zeros((self.size, self.size, 3), dtype=np.uint8)\n+        h, w = crop.shape[:2]\n+        scale = min(self.size / max(w, 1), self.size / max(h, 1))\n+        new_w = max(int(w * scale), 1)\n+        new_h = max(int(h * scale), 1)\n+        resized = self._cv2.resize(crop, (new_w, new_h))\n+        canvas = np.zeros((self.size, self.size, 3), dtype=np.uint8)\n+        top = (self.size - new_h) // 2\n+        left = (self.size - new_w) // 2\n+        canvas[top : top + new_h, left : left + new_w] = resized\n+        return canvas\n+\n+\n+def _faces_embed_path(ep_id: str) -> Path:\n+    embed_dir = DATA_ROOT / \"embeds\" / ep_id\n+    embed_dir.mkdir(parents=True, exist_ok=True)\n+    return embed_dir / \"faces.npy\"\n+class ProgressEmitter:\n+    \"\"\"Emit structured progress to stdout + optional file for SSE/polling.\"\"\"\n+\n+    VERSION = 2\n+\n+    def __init__(\n+        self,\n+        ep_id: str,\n+        file_path: str | Path | None,\n+        *,\n+        frames_total: int,\n+        secs_total: float | None,\n+        stride: int,\n+        fps_detected: float | None,\n+        fps_requested: float | None,\n+        frame_interval: int | None = None,\n+        run_id: str | None = None,\n+    ) -> None:\n+        import uuid\n+        self.ep_id = ep_id\n+        self.run_id = run_id or str(uuid.uuid4())\n+        self.path = Path(file_path).expanduser() if file_path else None\n+        if self.path:\n+            self.path.parent.mkdir(parents=True, exist_ok=True)\n+        self.frames_total = max(int(frames_total or 0), 0)\n+        self.secs_total = float(secs_total) if secs_total else None\n+        self.stride = max(int(stride), 1)\n+        self.fps_detected = float(fps_detected) if fps_detected else None\n+        self.fps_requested = float(fps_requested) if fps_requested else None\n+        default_interval = PROGRESS_FRAME_STEP\n+        chosen_interval = frame_interval if frame_interval is not None else default_interval\n+        self._frame_interval = max(int(chosen_interval), 1)\n+        self._start_ts = time.time()\n+        self._last_frames = 0\n+        self._last_phase: str | None = None\n+        self._last_step: str | None = None\n+        self._device: str | None = None\n+        self._detector: str | None = None\n+        self._tracker: str | None = None\n+        self._resolved_device: str | None = None\n+        self._closed = False\n+\n+    def _now(self) -> str:\n+        return datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n+\n+    def _should_emit(self, frames_done: int, phase: str, step: str | None, force: bool) -> bool:\n+        if force:\n+            return True\n+        if phase != self._last_phase:\n+            return True\n+        if step != self._last_step:\n+            return True\n+        return (frames_done - self._last_frames) >= self._frame_interval\n+\n+    def _compose_payload(\n+        self,\n+        frames_done: int,\n+        phase: str,\n+        device: str | None,\n+        summary: Dict[str, object] | None,\n+        error: str | None,\n+        detector: str | None,\n+        tracker: str | None,\n+        resolved_device: str | None,\n+        extra: Dict[str, Any] | None = None,\n+    ) -> Dict[str, object]:\n+        secs_done = time.time() - self._start_ts\n+        fps_infer = None\n+        if secs_done > 0 and frames_done >= 0:\n+            fps_infer = frames_done / secs_done\n+        payload: Dict[str, object] = {\n+            \"progress_version\": self.VERSION,\n+            \"ep_id\": self.ep_id,\n+            \"run_id\": self.run_id,\n+            \"phase\": phase,\n+            \"frames_done\": frames_done,\n+            \"frames_total\": self.frames_total,\n+            \"secs_done\": round(float(secs_done), 3),\n+            \"secs_total\": round(float(self.secs_total), 3) if self.secs_total else None,\n+            \"device\": device or self._device,\n+            \"fps_infer\": round(float(fps_infer), 3) if fps_infer else None,\n+            \"fps_detected\": round(float(self.fps_detected), 3) if self.fps_detected else None,\n+            \"fps_requested\": round(float(self.fps_requested), 3) if self.fps_requested else None,\n+            \"stride\": self.stride,\n+            \"updated_at\": self._now(),\n+            \"detector\": detector or self._detector,\n+            \"tracker\": tracker or self._tracker,\n+            \"resolved_device\": resolved_device or self._resolved_device,\n+        }\n+        if summary:\n+            payload[\"summary\"] = summary\n+        if error:\n+            payload[\"error\"] = error\n+        if extra:\n+            payload.update(extra)\n+        return payload\n+\n+    def _write_payload(self, payload: Dict[str, object]) -> None:\n+        line = json.dumps(payload, sort_keys=True)\n+        print(line, flush=True)\n+\n+        # Structured logging for episode-wide grep\n+        phase = payload.get(\"phase\", \"\")\n+        step = payload.get(\"step\", \"\")\n+        frames = payload.get(\"frames_done\", 0)\n+        total = payload.get(\"frames_total\", 0)\n+        vt = payload.get(\"video_time\")\n+        vtotal = payload.get(\"video_total\")\n+        fps = payload.get(\"fps_infer\")\n+        run_id_short = self.run_id[:8] if self.run_id else \"unknown\"\n+\n+        if vt is not None and vtotal is not None:\n+            LOGGER.info(\n+                \"[job=%s run=%s phase=%s step=%s frames=%s/%s vt=%.1f/%.1f fps=%.2f]\",\n+                self.ep_id, run_id_short, phase, step, frames, total, vt, vtotal, fps or 0.0\n+            )\n+        else:\n+            LOGGER.info(\n+                \"[job=%s run=%s phase=%s step=%s frames=%s/%s fps=%.2f]\",\n+                self.ep_id, run_id_short, phase, step, frames, total, fps or 0.0\n+            )\n+\n+        if self.path:\n+            tmp_path = self.path.with_suffix(\".tmp\")\n+            tmp_path.write_text(line, encoding=\"utf-8\")\n+            tmp_path.replace(self.path)\n+\n+    def emit(\n+        self,\n+        frames_done: int,\n+        *,\n+        phase: str,\n+        device: str | None = None,\n+        summary: Dict[str, object] | None = None,\n+        error: str | None = None,\n+        force: bool = False,\n+        detector: str | None = None,\n+        tracker: str | None = None,\n+        resolved_device: str | None = None,\n+        extra: Dict[str, Any] | None = None,\n+        **fields: Any,\n+    ) -> None:\n+        if self._closed:\n+            return\n+        frames_done = max(int(frames_done), 0)\n+        if self.frames_total and frames_done > self.frames_total:\n+            frames_done = self.frames_total\n+\n+        # Extract step from extra dict if present\n+        step = None\n+        if extra and \"step\" in extra:\n+            step = extra.get(\"step\")\n+\n+        if not self._should_emit(frames_done, phase, step, force):\n+            return\n+        if device is not None:\n+            self._device = device\n+        if detector is not None:\n+            self._detector = detector\n+        if tracker is not None:\n+            self._tracker = tracker\n+        if resolved_device is not None:\n+            self._resolved_device = resolved_device\n+        combined_extra: Dict[str, Any] = {} if extra is None else dict(extra)\n+        if fields:\n+            combined_extra.update(fields)\n+        payload = self._compose_payload(\n+            frames_done,\n+            phase,\n+            device,\n+            summary,\n+            error,\n+            detector,\n+            tracker,\n+            resolved_device,\n+            combined_extra or None,\n+        )\n+        self._write_payload(payload)\n+        self._last_frames = frames_done\n+        self._last_phase = phase\n+        self._last_step = step\n+\n+    def complete(\n+        self,\n+        summary: Dict[str, object],\n+        device: str | None = None,\n+        detector: str | None = None,\n+        tracker: str | None = None,\n+        resolved_device: str | None = None,\n+        *,\n+        step: str | None = None,\n+        extra: Dict[str, Any] | None = None,\n+    ) -> None:\n+        final_frames = self.frames_total or summary.get(\"frames_sampled\") or self._last_frames\n+        final_frames = int(final_frames or 0)\n+        completion_extra: Dict[str, Any] = {} if extra is None else dict(extra)\n+        if step:\n+            completion_extra[\"step\"] = step\n+        self.emit(\n+            final_frames,\n+            phase=\"done\",\n+            device=device,\n+            summary=summary,\n+            force=True,\n+            detector=detector,\n+            tracker=tracker,\n+            resolved_device=resolved_device,\n+            extra=completion_extra or None,\n+        )\n+\n+    def fail(self, error: str) -> None:\n+        self.emit(self._last_frames, phase=\"error\", error=error, force=True, tracker=self._tracker)\n+\n+    @property\n+    def target_frames(self) -> int:\n+        return self.frames_total or 0\n+\n+    def close(self) -> None:\n+        self._closed = True\n+\n+\n+def _non_video_phase_meta(step: str | None = None) -> Dict[str, Any]:\n+    meta: Dict[str, Any] = {\"video_time\": None, \"video_total\": None}\n+    if step:\n+        meta[\"step\"] = step\n+    return meta\n+\n+\n+def _video_phase_meta(frames_done: int, frames_total: int | None, fps: float | None, step: str | None = None) -> Dict[str, Any]:\n+    meta: Dict[str, Any] = {}\n+    if fps and fps > 0 and frames_total and frames_total > 0:\n+        video_total = frames_total / fps\n+        video_time = min(frames_done / fps, video_total)\n+        meta[\"video_total\"] = round(video_total, 3)\n+        meta[\"video_time\"] = round(video_time, 3)\n+    else:\n+        meta[\"video_time\"] = None\n+        meta[\"video_total\"] = None\n+    if step:\n+        meta[\"step\"] = step\n+    return meta\n+\n+\n+def _utcnow_iso() -> str:\n+    return datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n+\n+\n+def _write_run_marker(ep_id: str, phase: str, payload: Dict[str, Any]) -> None:\n+    manifests_dir = get_path(ep_id, \"detections\").parent\n+    run_dir = manifests_dir / RUN_MARKERS_SUBDIR\n+    run_dir.mkdir(parents=True, exist_ok=True)\n+    marker_path = run_dir / f\"{phase}.json\"\n+    marker_path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n+\n+\n+class FrameExporter:\n+    \"\"\"Handles optional frame + crop JPEG exports for S3 sync.\"\"\"\n+\n+    def __init__(\n+        self,\n+        ep_id: str,\n+        *,\n+        save_frames: bool,\n+        save_crops: bool,\n+        jpeg_quality: int,\n+        debug_logger: JsonlLogger | NullLogger | None = None,\n+    ) -> None:\n+        self.ep_id = ep_id\n+        self.save_frames = save_frames\n+        self.save_crops = save_crops\n+        self.jpeg_quality = max(1, min(int(jpeg_quality or 85), 100))\n+        self.root_dir = get_path(ep_id, \"frames_root\")\n+        self.frames_dir = self.root_dir / \"frames\"\n+        self.crops_dir = self.root_dir / \"crops\"\n+        if self.save_frames:\n+            self.frames_dir.mkdir(parents=True, exist_ok=True)\n+        if self.save_crops:\n+            self.crops_dir.mkdir(parents=True, exist_ok=True)\n+        self.frames_written = 0\n+        self.crops_written = 0\n+        self._track_indexes: Dict[int, Dict[str, Dict[str, Any]]] = defaultdict(dict)\n+        self._stat_samples = 0\n+        self._stat_limit = 10\n+        self._crop_attempts = 0\n+        self._crop_error_counts: Counter[str] = Counter()\n+        self._fail_fast_threshold = 0.10\n+        self._fail_fast_min_attempts = 10\n+        self._fail_fast_reasons = {\"near_uniform_gray\", \"tiny_file\"}\n+        self.debug_logger = debug_logger\n+\n+    def _log_image_stats(self, kind: str, path: Path, image) -> None:\n+        if self._stat_samples >= self._stat_limit:\n+            return\n+        mn, mx, mean = _image_stats(image)\n+        LOGGER.info(\"%s stats %s min=%.3f max=%.3f mean=%.3f\", kind, path, mn, mx, mean)\n+        if mx - mn < 1e-6:\n+            LOGGER.warning(\"Nearly constant %s %s mn=%.6f mx=%.6f mean=%.6f\", kind, path, mn, mx, mean)\n+        self._stat_samples += 1\n+\n+    def export(self, frame_idx: int, image, crops: List[Tuple[int, List[float]]], ts: float | None = None) -> None:\n+        if not (self.save_frames or self.save_crops):\n+            return\n+        if self.save_frames:\n+            frame_path = self.frames_dir / f\"frame_{frame_idx:06d}.jpg\"\n+            try:\n+                self._log_image_stats(\"frame\", frame_path, image)\n+                save_jpeg(frame_path, image, quality=self.jpeg_quality, color=\"bgr\")\n+                self.frames_written += 1\n+            except Exception as exc:  # pragma: no cover - best effort\n+                LOGGER.warning(\"Failed to save frame %s: %s\", frame_path, exc)\n+        if self.save_crops and crops:\n+            for track_id, bbox in crops:\n+                if track_id is None:\n+                    continue\n+                crop_path = self.crop_abs_path(track_id, frame_idx)\n+                try:\n+                    saved = self._write_crop(image, bbox, crop_path, track_id, frame_idx)\n+                except Exception as exc:  # pragma: no cover - best effort\n+                    LOGGER.warning(\"Failed to save crop %s: %s\", crop_path, exc)\n+                    self._register_crop_attempt(\"exception\")\n+                    saved = False\n+                if saved:\n+                    self.crops_written += 1\n+                    self._record_crop_index(track_id, frame_idx, ts)\n+\n+    def crop_component(self, track_id: int, frame_idx: int) -> str:\n+        return f\"track_{track_id:04d}/frame_{frame_idx:06d}.jpg\"\n+\n+    def crop_rel_path(self, track_id: int, frame_idx: int) -> str:\n+        return f\"crops/{self.crop_component(track_id, frame_idx)}\"\n+\n+    def crop_abs_path(self, track_id: int, frame_idx: int) -> Path:\n+        return self.crops_dir / self.crop_component(track_id, frame_idx)\n+\n+    def _record_crop_index(self, track_id: int, frame_idx: int, ts: float | None) -> None:\n+        if not self.save_crops:\n+            return\n+        key = self.crop_component(track_id, frame_idx)\n+        entry = {\n+            \"key\": key,\n+            \"frame_idx\": int(frame_idx),\n+            \"ts\": round(float(ts), 4) if ts is not None else None,\n+        }\n+        self._track_indexes.setdefault(track_id, {})[key] = entry\n+\n+    def write_indexes(self) -> None:\n+        if not self.save_crops or not self._track_indexes:\n+            return\n+        for track_id, entries in self._track_indexes.items():\n+            if not entries:\n+                continue\n+            track_dir = self.crops_dir / f\"track_{track_id:04d}\"\n+            if not track_dir.exists():\n+                continue\n+            ordered = sorted(entries.values(), key=lambda item: item[\"frame_idx\"])\n+            index_path = track_dir / \"index.json\"\n+            try:\n+                index_path.write_text(json.dumps(ordered, indent=2), encoding=\"utf-8\")\n+            except Exception as exc:  # pragma: no cover - best effort\n+                LOGGER.warning(\"Failed to write crop index %s: %s\", index_path, exc)\n+\n+    def _register_crop_attempt(self, reason: str | None) -> None:\n+        self._crop_attempts += 1\n+        if reason:\n+            self._crop_error_counts[reason] += 1\n+        if reason in self._fail_fast_reasons:\n+            self._maybe_fail_fast()\n+\n+    def _maybe_fail_fast(self) -> None:\n+        if self._crop_attempts < self._fail_fast_min_attempts:\n+            return\n+        bad = sum(self._crop_error_counts.get(reason, 0) for reason in self._fail_fast_reasons)\n+        ratio = bad / max(self._crop_attempts, 1)\n+        if ratio >= self._fail_fast_threshold:\n+            raise RuntimeError(\n+                f\"Too many invalid crops ({bad}/{self._crop_attempts}, {ratio:.1%}); aborting export\"\n+            )\n+\n+    def _write_crop(\n+        self,\n+        image,\n+        bbox: List[float],\n+        crop_path: Path,\n+        track_id: int,\n+        frame_idx: int,\n+    ) -> bool:\n+        start = time.time()\n+        bbox_vals = [float(val) for val in bbox]\n+        crop, clipped_bbox, crop_err = safe_crop(image, bbox_vals)\n+        debug_payload: Dict[str, Any] | None = None\n+        if self.debug_logger:\n+            debug_payload = {\n+                \"track_id\": track_id,\n+                \"frame_idx\": frame_idx,\n+                \"out\": str(crop_path),\n+                \"bbox\": bbox_vals,\n+                \"clipped_bbox\": list(clipped_bbox) if clipped_bbox else None,\n+                \"err_before_save\": crop_err,\n+            }\n+        if crop is None:\n+            self._register_crop_attempt(crop_err or \"no_crop\")\n+            if debug_payload is not None:\n+                debug_payload.update(\n+                    {\n+                        \"save_ok\": False,\n+                        \"save_err\": crop_err or \"no_crop\",\n+                        \"ms\": int((time.time() - start) * 1000),\n+                    }\n+                )\n+                self._emit_debug(debug_payload)\n+            return False\n+\n+        ok, save_err = safe_imwrite(crop_path, crop, self.jpeg_quality)\n+        reason = save_err if not ok else None\n+        self._register_crop_attempt(reason)\n+\n+        if debug_payload is not None:\n+            mn, mx, mean = _image_stats(crop)\n+            debug_payload.update(\n+                {\n+                    \"shape\": tuple(int(x) for x in crop.shape),\n+                    \"dtype\": str(crop.dtype),\n+                    \"min\": mn,\n+                    \"max\": mx,\n+                    \"mean\": mean,\n+                    \"save_ok\": bool(ok),\n+                    \"save_err\": save_err,\n+                    \"file_size\": crop_path.stat().st_size if ok and crop_path.exists() else None,\n+                    \"ms\": int((time.time() - start) * 1000),\n+                }\n+            )\n+            self._emit_debug(debug_payload)\n+\n+        if not ok and save_err:\n+            LOGGER.warning(\"Failed to save crop %s: %s\", crop_path, save_err)\n+        return bool(ok)\n+\n+    def _emit_debug(self, payload: Dict[str, Any]) -> None:\n+        if not self.debug_logger:\n+            return\n+        try:\n+            self.debug_logger(payload)\n+        except Exception:  # pragma: no cover - best effort diagnostics\n+            pass\n+\n+\n+class FrameDecoder:\n+    \"\"\"Random-access video frame reader.\"\"\"\n+\n+    def __init__(self, video_path: Path) -> None:\n+        import cv2  # type: ignore\n+\n+        self._cv2 = cv2\n+        self._cap = cv2.VideoCapture(str(video_path))\n+        if not self._cap.isOpened():\n+            raise FileNotFoundError(f\"Unable to open video {video_path}\")\n+\n+    def read(self, frame_idx: int):\n+        self._cap.set(self._cv2.CAP_PROP_POS_FRAMES, max(int(frame_idx), 0))\n+        ok, frame = self._cap.read()\n+        if not ok:\n+            raise RuntimeError(f\"Failed to decode frame {frame_idx}\")\n+        return frame\n+\n+    def close(self) -> None:\n+        if self._cap is not None:\n+            self._cap.release()\n+            self._cap = None\n+\n+    def __del__(self) -> None:  # pragma: no cover - defensive\n+        try:\n+            self.close()\n+        except Exception:\n+            pass\n+\n+\n+def _copy_video(src: Path, dest: Path) -> None:\n+    dest.parent.mkdir(parents=True, exist_ok=True)\n+    if src.resolve() == dest.resolve():\n+        return\n+    shutil.copy2(src, dest)\n+\n+\n+def _estimate_duration(frame_count: int, fps: float) -> float | None:\n+    if frame_count > 0 and fps > 0:\n+        return frame_count / fps\n+    return None\n+\n+\n+def _estimate_frame_budget(\n+    *,\n+    stride: int,\n+    target_fps: float | None,\n+    detected_fps: float,\n+    duration_sec: float | None,\n+    frame_count: int,\n+) -> int:\n+    stride = max(stride, 1)\n+    fps_source = target_fps if target_fps and target_fps > 0 else detected_fps\n+    if fps_source and fps_source > 0 and duration_sec:\n+        value = int(math.ceil((fps_source * duration_sec) / stride))\n+    elif frame_count > 0:\n+        value = int(math.ceil(frame_count / stride))\n+    else:\n+        value = 0\n+    return max(value, 1)\n+\n+\n+def _episode_ctx(ep_id: str) -> EpisodeContext | None:\n+    try:\n+        return episode_context_from_id(ep_id)\n+    except ValueError:\n+        LOGGER.warning(\"Unable to parse episode id '%s'; artifact prefixes unavailable\", ep_id)\n+        return None\n+ \n+\n+def _storage_context(ep_id: str) -> tuple[StorageService | None, EpisodeContext | None, Dict[str, str] | None]:\n+    storage_backend = os.environ.get(\"STORAGE_BACKEND\", \"local\").lower()\n+    storage: StorageService | None = None\n+    if storage_backend in {\"s3\", \"minio\"}:\n+        try:\n+            storage = StorageService()\n+        except Exception as exc:  # pragma: no cover - best effort init\n+            LOGGER.warning(\"Storage init failed (%s); disabling uploads\", exc)\n+            storage = None\n+    ep_ctx = _episode_ctx(ep_id)\n+    prefixes = artifact_prefixes(ep_ctx) if ep_ctx else None\n+    return storage, ep_ctx, prefixes\n+\n+\n+def _sync_artifacts_to_s3(\n+    ep_id: str,\n+    storage: StorageService | None,\n+    ep_ctx: EpisodeContext | None,\n+    exporter: FrameExporter | None,\n+    thumb_dir: Path | None = None,\n+) -> Dict[str, int]:\n+    stats = {\"manifests\": 0, \"frames\": 0, \"crops\": 0, \"thumbs_tracks\": 0, \"thumbs_identities\": 0}\n+    if storage is None or ep_ctx is None or not storage.s3_enabled() or not storage.write_enabled:\n+        return stats\n+    prefixes = artifact_prefixes(ep_ctx)\n+    manifests_dir = get_path(ep_id, \"detections\").parent\n+    stats[\"manifests\"] = storage.upload_dir(manifests_dir, prefixes[\"manifests\"])\n+    frames_root = get_path(ep_id, \"frames_root\")\n+    frames_dir = frames_root / \"frames\"\n+    crops_dir = frames_root / \"crops\"\n+    if exporter and exporter.save_frames and exporter.frames_dir.exists():\n+        stats[\"frames\"] = storage.upload_dir(exporter.frames_dir, prefixes[\"frames\"])\n+    elif frames_dir.exists():\n+        stats[\"frames\"] = storage.upload_dir(frames_dir, prefixes[\"frames\"])\n+    if exporter and exporter.save_crops and exporter.crops_dir.exists():\n+        stats[\"crops\"] = storage.upload_dir(exporter.crops_dir, prefixes[\"crops\"])\n+    elif crops_dir.exists():\n+        stats[\"crops\"] = storage.upload_dir(crops_dir, prefixes[\"crops\"])\n+    if thumb_dir is not None and thumb_dir.exists():\n+        identities_dir = thumb_dir / \"identities\"\n+        stats[\"thumbs_tracks\"] = storage.upload_dir(\n+            thumb_dir,\n+            prefixes.get(\"thumbs_tracks\", \"\"),\n+            skip_subdirs=(\"identities\",),\n+        )\n+        if identities_dir.exists():\n+            stats[\"thumbs_identities\"] = storage.upload_dir(\n+                identities_dir,\n+                prefixes.get(\"thumbs_identities\", \"\"),\n+            )\n+    return stats\n+\n+\n+def _report_s3_upload(\n+    progress: ProgressEmitter | None,\n+    stats: Dict[str, int],\n+    *,\n+    device: str | None,\n+    detector: str | None,\n+    tracker: str | None,\n+    resolved_device: str | None,\n+) -> None:\n+    if not progress:\n+        return\n+    if not any(stats.values()):\n+        return\n+    frames = progress.target_frames or 0\n+    progress.emit(\n+        frames,\n+        phase=\"mirror_s3\",\n+        device=device,\n+        summary={\"s3_uploads\": stats},\n+        detector=detector,\n+        tracker=tracker,\n+        resolved_device=resolved_device,\n+        force=True,\n+    )\n+\n+\n+def parse_args(argv: Iterable[str] | None = None) -> argparse.Namespace:\n+    parser = argparse.ArgumentParser(description=\"Run detection + tracking locally.\")\n+    parser.add_argument(\"--ep-id\", required=True, help=\"Episode identifier\")\n+    parser.add_argument(\"--video\", help=\"Path to source video (required for detect/track runs)\")\n+    parser.add_argument(\"--stride\", type=int, default=5, help=\"Frame stride for detection sampling\")\n+    parser.add_argument(\n+        \"--fps\",\n+        type=float,\n+        help=\"Optional target FPS for downsampling before detection\",\n+    )\n+    parser.add_argument(\n+        \"--device\",\n+        choices=[\"auto\", \"cpu\", \"mps\", \"cuda\"],\n+        default=\"auto\",\n+        help=\"Execution device override (auto→CUDA/MPS/CPU)\",\n+    )\n+    parser.add_argument(\n+        \"--detector\",\n+        choices=list(DETECTOR_CHOICES),\n+        default=DEFAULT_DETECTOR,\n+        help=\"Face detector backend (RetinaFace high quality, YOLOv8-face fast)\",\n+    )\n+    parser.add_argument(\n+        \"--tracker\",\n+        choices=list(TRACKER_CHOICES),\n+        default=DEFAULT_TRACKER,\n+        help=\"Tracker backend (ByteTrack default, StrongSORT optional for occlusions)\",\n+    )\n+    parser.add_argument(\n+        \"--scene-detect\",\n+        choices=[\"on\", \"off\"],\n+        default=\"on\" if SCENE_DETECT_DEFAULT else \"off\",\n+        help=\"Enable histogram-based scene cut detection\",\n+    )\n+    parser.add_argument(\n+        \"--scene-threshold\",\n+        type=float,\n+        default=SCENE_THRESHOLD_DEFAULT,\n+        help=\"Scene-cut threshold (1 - HSV histogram correlation, 0-2 range)\",\n+    )\n+    parser.add_argument(\n+        \"--scene-min-len\",\n+        type=int,\n+        default=SCENE_MIN_LEN_DEFAULT,\n+        help=\"Minimum frames between scene cuts\",\n+    )\n+    parser.add_argument(\n+        \"--scene-warmup-dets\",\n+        type=int,\n+        default=SCENE_WARMUP_DETS_DEFAULT,\n+        help=\"Frames of forced detection after each cut\",\n+    )\n+    parser.add_argument(\n+        \"--det-thresh\",\n+        type=float,\n+        default=RETINAFACE_SCORE_THRESHOLD,\n+        help=\"RetinaFace detection score threshold (0-1, default 0.5)\",\n+    )\n+    parser.add_argument(\n+        \"--max-gap\",\n+        type=int,\n+        default=30,\n+        help=\"Maximum frame gap before splitting a track\",\n+    )\n+    parser.add_argument(\n+        \"--track-sample-limit\",\n+        type=int,\n+        default=None,\n+        help=\"Optional max samples stored per track (0→all detections, default)\",\n+    )\n+    parser.add_argument(\"--thumb-size\", type=int, default=256, help=\"Square thumbnail size for faces\")\n+    parser.add_argument(\n+        \"--out-root\",\n+        help=\"Data root override (defaults to SCREENALYTICS_DATA_ROOT or ./data)\",\n+    )\n+    parser.add_argument(\"--progress-file\", help=\"Progress JSON file to update during processing\")\n+    parser.add_argument(\"--save-frames\", action=\"store_true\", help=\"Save sampled frame JPGs under data/frames/{ep_id}\")\n+    parser.add_argument(\"--save-crops\", action=\"store_true\", help=\"Save per-track crops (requires --save-frames or track IDs)\")\n+    parser.add_argument(\"--jpeg-quality\", type=int, default=85, help=\"JPEG quality for frame exports (1-100)\")\n+    parser.add_argument(\"--faces-embed\", action=\"store_true\", help=\"Run faces embedding stage only\")\n+    parser.add_argument(\"--cluster\", action=\"store_true\", help=\"Run clustering stage only\")\n+    parser.add_argument(\n+        \"--cluster-thresh\",\n+        type=float,\n+        default=0.6,\n+        help=\"Agglomerative cosine distance threshold for clustering\",\n+    )\n+    parser.add_argument(\n+        \"--min-cluster-size\",\n+        type=int,\n+        default=2,\n+        help=\"Minimum tracks per identity before splitting into singletons\",\n+    )\n+    return parser.parse_args(argv)\n+\n+\n+def main(argv: Iterable[str] | None = None) -> int:\n+    args = parse_args(argv)\n+    if hasattr(args, \"det_thresh\"):\n+        args.det_thresh = _normalize_det_thresh(getattr(args, \"det_thresh\", RETINAFACE_SCORE_THRESHOLD))\n+    scene_flag = str(getattr(args, \"scene_detect\", \"on\")).strip().lower()\n+    args.scene_detect = scene_flag not in {\"0\", \"false\", \"off\", \"no\"}\n+    args.scene_threshold = max(min(float(getattr(args, \"scene_threshold\", SCENE_THRESHOLD_DEFAULT)), 2.0), 0.0)\n+    args.scene_min_len = max(int(getattr(args, \"scene_min_len\", SCENE_MIN_LEN_DEFAULT)), 1)\n+    args.scene_warmup_dets = max(int(getattr(args, \"scene_warmup_dets\", SCENE_WARMUP_DETS_DEFAULT)), 0)\n+    cli_track_limit = getattr(args, \"track_sample_limit\", None)\n+    if cli_track_limit is not None:\n+        _set_track_sample_limit(cli_track_limit)\n+        if TRACK_SAMPLE_LIMIT is None:\n+            LOGGER.info(\"Track sampling disabled; persisting all detections per track.\")\n+        else:\n+            LOGGER.info(\"Track sampling limited to the first %s detections per track.\", TRACK_SAMPLE_LIMIT)\n+    data_root = (\n+        Path(args.out_root).expanduser()\n+        if args.out_root\n+        else Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+    )\n+    os.environ[\"SCREENALYTICS_DATA_ROOT\"] = str(data_root)\n+    ensure_dirs(args.ep_id)\n+    storage, ep_ctx, s3_prefixes = _storage_context(args.ep_id)\n+\n+    phase_flags = [flag for flag in (args.faces_embed, args.cluster) if flag]\n+    if len(phase_flags) > 1:\n+        raise ValueError(\"Specify at most one of --faces-embed/--cluster per run\")\n+\n+    if args.faces_embed:\n+        summary = _run_faces_embed_stage(args, storage, ep_ctx, s3_prefixes)\n+    elif args.cluster:\n+        summary = _run_cluster_stage(args, storage, ep_ctx, s3_prefixes)\n+    else:\n+        summary = _run_detect_track_stage(args, storage, ep_ctx, s3_prefixes)\n+\n+    stage = summary.get(\"stage\", \"detect_track\")\n+    device_label = summary.get(\"device\")\n+    analyzed_fps = summary.get(\"analyzed_fps\")\n+    log_msg = f\"stage={stage}\"\n+    if device_label:\n+        log_msg += f\" device={device_label}\"\n+    if analyzed_fps:\n+        log_msg += f\" analyzed_fps={analyzed_fps:.3f}\"\n+    print(f\"[episode_run] {log_msg}\", file=sys.stderr)\n+    print(\"[episode_run] summary\", summary, file=sys.stderr)\n+    return 0\n+\n+\n+def _effective_stride(stride: int, target_fps: float | None, source_fps: float) -> int:\n+    stride = max(stride, 1)\n+    if target_fps and target_fps > 0 and source_fps > 0:\n+        fps_stride = max(int(round(source_fps / target_fps)), 1)\n+        stride = max(stride, fps_stride)\n+    return stride\n+\n+\n+def _probe_video(video_path: Path) -> Tuple[float, int]:\n+    import cv2  # type: ignore\n+\n+    cap = cv2.VideoCapture(str(video_path))\n+    if not cap.isOpened():\n+        raise FileNotFoundError(f\"Unable to open video {video_path}\")\n+    fps = cap.get(cv2.CAP_PROP_FPS) or 0.0\n+    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n+    cap.release()\n+    if fps <= 0:\n+        fps = 30.0\n+    return fps, frame_count\n+\n+\n+def _detect_fps(video_path: Path) -> float:\n+    fps, _ = _probe_video(video_path)\n+    if fps <= 0:\n+        fps = 24.0\n+    return fps\n+\n+\n+def detect_scene_cuts(\n+    video_path: str | Path,\n+    *,\n+    thr: float = SCENE_THRESHOLD_DEFAULT,\n+    min_len: int = SCENE_MIN_LEN_DEFAULT,\n+    progress: ProgressEmitter | None = None,\n+) -> list[int]:\n+    \"\"\"Lightweight HSV histogram scene-cut detector.\"\"\"\n+\n+    import cv2  # type: ignore\n+\n+    threshold = max(min(float(thr or SCENE_THRESHOLD_DEFAULT), 2.0), 0.0)\n+    min_gap = max(int(min_len or SCENE_MIN_LEN_DEFAULT), 1)\n+    cap = cv2.VideoCapture(str(video_path))\n+    if not cap.isOpened():\n+        return []\n+    cuts: list[int] = []\n+    prev_hist = None\n+    last_cut = -10**9\n+    idx = 0\n+    if progress:\n+        progress.emit(\n+            0,\n+            phase=\"scene_detect:cut\",\n+            summary={\"threshold\": round(float(threshold), 3), \"min_len\": min_gap},\n+            extra=_non_video_phase_meta(\"start\"),\n+            force=True,\n+        )\n+    target_frames = progress.target_frames if progress else 0\n+    emit_interval = 50  # Emit every 50 frames to reduce spam\n+    while True:\n+        ok, frame = cap.read()\n+        if not ok:\n+            break\n+        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n+        hist = cv2.calcHist([hsv], [0, 1], None, [32, 32], [0, 180, 0, 256])\n+        hist = cv2.normalize(hist, None).flatten()\n+        if prev_hist is not None:\n+            diff = 1.0 - float(cv2.compareHist(prev_hist, hist, cv2.HISTCMP_CORREL))\n+            if diff > threshold and (idx - last_cut) >= min_gap:\n+                cuts.append(idx)\n+                last_cut = idx\n+        # Emit sparse updates (every 50 frames) instead of on every cut\n+        if progress and (idx % emit_interval == 0 or idx == target_frames - 1):\n+            frames_done = idx if idx >= 0 else 0\n+            if target_frames:\n+                frames_done = min(target_frames, frames_done)\n+            progress.emit(\n+                frames_done,\n+                phase=\"scene_detect:cut\",\n+                summary={\"count\": len(cuts)},\n+                extra=_non_video_phase_meta(),\n+            )\n+        prev_hist = hist\n+        idx += 1\n+    cap.release()\n+    if progress:\n+        final_frames = target_frames or idx\n+        progress.emit(\n+            final_frames,\n+            phase=\"scene_detect:cut\",\n+            summary={\"cuts\": len(cuts)},\n+            force=True,\n+            extra=_non_video_phase_meta(\"done\"),\n+        )\n+    return cuts\n+\n+\n+def _run_full_pipeline(\n+    args: argparse.Namespace,\n+    video_dest: Path,\n+    *,\n+    source_fps: float,\n+    progress: ProgressEmitter | None = None,\n+    target_fps: float | None = None,\n+    frame_exporter: FrameExporter | None = None,\n+    total_frames: int | None = None,\n+    video_fps: float | None = None,\n+) -> Tuple[int, int, int, str, str, float | None, Dict[str, Any], Dict[str, Any]]:\n+    import cv2  # type: ignore\n+\n+    analyzed_fps = target_fps or source_fps\n+    if not analyzed_fps or analyzed_fps <= 0:\n+        analyzed_fps = _detect_fps(video_dest)\n+    frame_stride = _effective_stride(args.stride, target_fps or analyzed_fps, source_fps)\n+    ts_fps = analyzed_fps if analyzed_fps and analyzed_fps > 0 else max(args.fps or 30.0, 1.0)\n+    frames_goal = None\n+    if total_frames and total_frames > 0:\n+        frames_goal = int(total_frames)\n+    elif progress and progress.target_frames:\n+        frames_goal = progress.target_frames\n+    video_clock_fps = video_fps if video_fps and video_fps > 0 else (source_fps if source_fps > 0 else None)\n+\n+    def _progress_value(frame_index: int, *, include_current: bool = False, step: str | None = None) -> tuple[int, Dict[str, Any]]:\n+        base = frame_index + (1 if include_current else 0)\n+        if base < 0:\n+            base = 0\n+        total = frames_goal or base\n+        value = base\n+        if frames_goal:\n+            value = min(frames_goal, base)\n+        meta = _video_phase_meta(value, total if total > 0 else None, video_clock_fps, step=step)\n+        return value, meta\n+    device = pick_device(args.device)\n+    detector_choice = _normalize_detector_choice(getattr(args, \"detector\", None))\n+    tracker_choice = _normalize_tracker_choice(getattr(args, \"tracker\", None))","path":"tools/episode_run.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"This assignment to 'tracker_choice' is unnecessary as it is [redefined](1) before this value is used.","created_at":"2025-11-11T20:00:03Z","updated_at":"2025-11-11T20:00:21Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565091","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565091"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565091"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565091/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":1921,"original_line":1921,"side":"RIGHT","author_association":"NONE","original_position":1921,"position":1921,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565101","pull_request_review_id":3449784939,"id":2515565101,"node_id":"PRRC_kwDOQRcQs86V8Hot","diff_hunk":"@@ -0,0 +1,3034 @@\n+#!/usr/bin/env python\n+\"\"\"Dev-only CLI to run detection → tracking for a single episode.\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import math\n+import os\n+import shutil\n+import sys\n+import time\n+from collections import Counter, defaultdict\n+from dataclasses import dataclass, field\n+from datetime import datetime\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, Iterator, List, Optional, Tuple, Callable\n+\n+import logging\n+from functools import lru_cache\n+\n+import numpy as np\n+\n+REPO_ROOT = Path(__file__).resolve().parents[1]\n+if str(REPO_ROOT) not in sys.path:\n+    sys.path.insert(0, str(REPO_ROOT))\n+\n+from apps.api.services.storage import (\n+    EpisodeContext,\n+    StorageService,\n+    artifact_prefixes,\n+    episode_context_from_id,\n+)\n+from py_screenalytics.artifacts import ensure_dirs, get_path\n+from tools._img_utils import clip_bbox, safe_crop, safe_imwrite, to_u8_bgr\n+from tools.debug_thumbs import init_debug_logger, debug_thumbs_enabled, NullLogger, JsonlLogger\n+\n+PIPELINE_VERSION = os.environ.get(\"SCREENALYTICS_PIPELINE_VERSION\", \"2025-11-11\")\n+APP_VERSION = os.environ.get(\"SCREENALYTICS_APP_VERSION\", PIPELINE_VERSION)\n+YOLO_MODEL_NAME = os.environ.get(\"SCREENALYTICS_YOLO_MODEL\", \"yolov8n.pt\")\n+TRACKER_CONFIG = os.environ.get(\"SCREENALYTICS_TRACKER_CONFIG\", \"bytetrack.yaml\")\n+TRACKER_NAME = Path(TRACKER_CONFIG).stem if TRACKER_CONFIG else \"bytetrack\"\n+YOLO_IMAGE_SIZE = int(os.environ.get(\"SCREENALYTICS_YOLO_IMGSZ\", 640))\n+YOLO_CONF_THRESHOLD = float(os.environ.get(\"SCREENALYTICS_YOLO_CONF\", 0.25))\n+YOLO_IOU_THRESHOLD = float(os.environ.get(\"SCREENALYTICS_YOLO_IOU\", 0.45))\n+PROGRESS_FRAME_STEP = int(os.environ.get(\"SCREENALYTICS_PROGRESS_FRAME_STEP\", 25))\n+LOGGER = logging.getLogger(\"episode_run\")\n+DATA_ROOT = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+DETECTOR_CHOICES = (\"retinaface\", \"yolov8face\")\n+DEFAULT_DETECTOR = DETECTOR_CHOICES[0]\n+TRACKER_CHOICES = (\"bytetrack\", \"strongsort\")\n+DEFAULT_TRACKER = TRACKER_CHOICES[0]\n+ARC_FACE_MODEL_NAME = os.environ.get(\"ARCFACE_MODEL\", \"arcface_r100_v1\")\n+RETINAFACE_MODEL_NAME = os.environ.get(\"RETINAFACE_MODEL\", \"retinaface_r50_v1\")\n+FACE_CLASS_LABEL = \"face\"\n+MIN_FACE_AREA = 20.0\n+FACE_RATIO_BOUNDS = (0.5, 2.0)\n+RETINAFACE_SCORE_THRESHOLD = 0.5\n+RETINAFACE_NMS = 0.45\n+\n+RUN_MARKERS_SUBDIR = \"runs\"\n+def _parse_retinaface_det_size(value: str | None) -> tuple[int, int] | None:\n+    if not value:\n+        return 640, 640\n+    tokens: list[str] = []\n+    buf = value.replace(\"x\", \",\").replace(\"X\", \",\")\n+    for part in buf.split(\",\"):\n+        part = part.strip()\n+        if part:\n+            tokens.append(part)\n+    if len(tokens) != 2:\n+        return 640, 640\n+    try:\n+        width = max(int(float(tokens[0])), 1)\n+        height = max(int(float(tokens[1])), 1)\n+        return width, height\n+    except ValueError:\n+        return 640, 640\n+\n+\n+RETINAFACE_DET_SIZE = _parse_retinaface_det_size(os.environ.get(\"RETINAFACE_DET_SIZE\"))\n+\n+\n+def _normalize_det_thresh(value: float | str | None) -> float:\n+    try:\n+        numeric = float(value) if value is not None else RETINAFACE_SCORE_THRESHOLD\n+    except (TypeError, ValueError):\n+        numeric = RETINAFACE_SCORE_THRESHOLD\n+    return min(max(numeric, 0.0), 1.0)\n+YOLO_FACE_CONF = 0.5\n+BYTE_TRACK_MIN_BOX_AREA = 20.0\n+DEFAULT_GMC_METHOD = os.environ.get(\"SCREENALYTICS_GMC_METHOD\", \"sparseOptFlow\")\n+DEFAULT_REID_MODEL = os.environ.get(\"SCREENALYTICS_REID_MODEL\", \"yolov8n-cls.pt\")\n+DEFAULT_REID_ENABLED = os.environ.get(\"SCREENALYTICS_REID_ENABLED\", \"1\").lower() in {\"1\", \"true\", \"yes\"}\n+RETINAFACE_HELP = \"RetinaFace weights missing or could not initialize. See README 'Models' or run scripts/fetch_models.py.\"\n+\n+\n+def _env_flag(name: str, default: bool) -> bool:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    value = raw.strip().lower()\n+    if value in {\"1\", \"true\", \"yes\", \"on\"}:\n+        return True\n+    if value in {\"0\", \"false\", \"off\", \"no\"}:\n+        return False\n+    return default\n+\n+\n+def _env_float(name: str, default: float) -> float:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    try:\n+        return float(raw)\n+    except (TypeError, ValueError):\n+        return default\n+\n+\n+def _env_int(name: str, default: int) -> int:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    try:\n+        return int(raw)\n+    except (TypeError, ValueError):\n+        return default\n+\n+\n+def _resolve_track_sample_limit(value: str | int | None) -> int | None:\n+    if value is None:\n+        return None\n+    if isinstance(value, str):\n+        text = value.strip().lower()\n+        if text in {\"\", \"none\", \"unlimited\", \"all\", \"off\", \"disable\"}:\n+            return None\n+        try:\n+            numeric = int(float(text))\n+        except ValueError:\n+            return None\n+    else:\n+        numeric = int(value)\n+    return numeric if numeric > 0 else None\n+\n+\n+def _set_track_sample_limit(value: int | None) -> None:\n+    global TRACK_SAMPLE_LIMIT\n+    TRACK_SAMPLE_LIMIT = _resolve_track_sample_limit(value)\n+\n+\n+TRACK_SAMPLE_LIMIT = _resolve_track_sample_limit(os.environ.get(\"SCREENALYTICS_TRACK_SAMPLE_LIMIT\"))\n+\n+\n+SCENE_DETECT_DEFAULT = _env_flag(\"SCENE_DETECT\", True)\n+SCENE_THRESHOLD_DEFAULT = max(min(_env_float(\"SCENE_THRESHOLD\", 0.30), 2.0), 0.0)\n+SCENE_MIN_LEN_DEFAULT = max(_env_int(\"SCENE_MIN_LEN\", 12), 1)\n+SCENE_WARMUP_DETS_DEFAULT = max(_env_int(\"SCENE_WARMUP_DETS\", 3), 0)\n+\n+\n+def _normalize_device_label(device: str | None) -> str:\n+    normalized = (device or \"cpu\").lower()\n+    if normalized in {\"0\", \"cuda\", \"gpu\"}:\n+        return \"cuda\"\n+    return normalized\n+\n+\n+def _onnx_providers_for(device: str | None) -> tuple[list[str], str]:\n+    normalized = (device or \"auto\").lower()\n+    providers: list[str] = [\"CPUExecutionProvider\"]\n+    resolved = \"cpu\"\n+    if normalized in {\"cuda\", \"0\", \"gpu\", \"auto\"}:\n+        try:\n+            import onnxruntime as ort  # type: ignore\n+\n+            available = ort.get_available_providers()\n+        except Exception:\n+            available = []\n+        if \"CUDAExecutionProvider\" in available:\n+            providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n+            resolved = \"cuda\"\n+            return providers, resolved\n+        if normalized in {\"cuda\", \"0\", \"gpu\"}:\n+            LOGGER.warning(\"CUDA requested for RetinaFace/ArcFace but CUDAExecutionProvider unavailable; falling back to CPU\")\n+    if normalized in {\"mps\", \"metal\", \"apple\"}:\n+        return [\"CPUExecutionProvider\"], \"cpu\"\n+    return providers, resolved\n+\n+\n+def _init_retinaface(model_name: str, device: str, score_thresh: float = RETINAFACE_SCORE_THRESHOLD) -> tuple[Any, str]:\n+    try:\n+        from insightface.model_zoo import get_model  # type: ignore\n+    except ImportError as exc:  # pragma: no cover - runtime guard\n+        raise RuntimeError(\"insightface is required for RetinaFace detection\") from exc\n+\n+    providers, resolved = _onnx_providers_for(device)\n+    model = get_model(model_name)\n+    if model is None:\n+        raise RuntimeError(\n+            f\"RetinaFace weights '{model_name}' not found. Install insightface models or run scripts/fetch_models.py.\"\n+        )\n+    ctx_id = 0 if resolved == \"cuda\" else -1\n+    # InsightFace 0.7.x configures detection threshold at prepare-time\n+    # (detect() no longer accepts a `threshold` kwarg).\n+    prepare_kwargs = {\n+        \"ctx_id\": ctx_id,\n+        \"providers\": providers,\n+        \"nms\": RETINAFACE_NMS,\n+        \"det_thresh\": float(score_thresh),\n+    }\n+    if RETINAFACE_DET_SIZE:\n+        prepare_kwargs[\"input_size\"] = RETINAFACE_DET_SIZE\n+    try:\n+        model.prepare(**prepare_kwargs)\n+    except TypeError:\n+        prepare_kwargs.pop(\"input_size\", None)\n+        model.prepare(**prepare_kwargs)\n+    return model, resolved\n+\n+\n+def _init_arcface(model_name: str, device: str):\n+    try:\n+        from insightface.model_zoo import get_model  # type: ignore\n+    except ImportError as exc:  # pragma: no cover\n+        raise RuntimeError(\"insightface is required for ArcFace embeddings\") from exc\n+\n+    providers, resolved = _onnx_providers_for(device)\n+    model = get_model(model_name)\n+    if model is None:\n+        raise RuntimeError(\n+            f\"ArcFace weights '{model_name}' not found. Install insightface models or run scripts/fetch_models.py.\"\n+        )\n+    ctx_id = 0 if resolved == \"cuda\" else -1\n+    model.prepare(ctx_id=ctx_id, providers=providers)\n+    return model, resolved\n+\n+\n+def ensure_retinaface_ready(device: str, det_thresh: float | None = None) -> tuple[bool, Optional[str], Optional[str]]:\n+    \"\"\"Lightweight readiness probe for API preflight checks.\"\"\"\n+\n+    model = None\n+    try:\n+        model, resolved = _init_retinaface(\n+            RETINAFACE_MODEL_NAME,\n+            device,\n+            det_thresh if det_thresh is not None else RETINAFACE_SCORE_THRESHOLD,\n+        )\n+    except Exception as exc:  # pragma: no cover - surfaced via API tests\n+        return False, str(exc), None\n+    finally:\n+        if model is not None:\n+            del model\n+    return True, None, resolved\n+\n+\n+def pick_device(explicit: str | None = None) -> str:\n+    \"\"\"Return the safest device available.\n+\n+    Order of preference: explicit override → CUDA → MPS → CPU.\n+    Values returned are what Ultralytics expects (\"cpu\", \"mps\", \"cuda\"/\"0\").\n+    \"\"\"\n+\n+    if explicit and explicit not in {\"auto\", \"\"}:\n+        return explicit\n+\n+    try:\n+        import torch  # type: ignore\n+\n+        if torch.cuda.is_available():  # pragma: no cover - depends on env\n+            return \"0\"\n+        mps_available = getattr(torch.backends, \"mps\", None)\n+        if mps_available is not None and mps_available.is_available():  # pragma: no cover - mac only\n+            return \"mps\"\n+    except Exception:  # pragma: no cover - torch import/runtime guard\n+        pass\n+\n+    return \"cpu\"\n+\n+\n+def _normalize_detector_choice(detector: str | None) -> str:\n+    if detector:\n+        value = detector.strip().lower()\n+        if value in DETECTOR_CHOICES:\n+            return value\n+    return DEFAULT_DETECTOR\n+\n+\n+def _normalize_tracker_choice(tracker: str | None) -> str:\n+    if tracker:\n+        value = tracker.strip().lower()\n+        if value in TRACKER_CHOICES:\n+            return value\n+    return DEFAULT_TRACKER\n+\n+\n+def _valid_face_box(bbox: np.ndarray, score: float, *, min_score: float, min_area: float) -> bool:\n+    width = bbox[2] - bbox[0]\n+    height = bbox[3] - bbox[1]\n+    area = max(width, 0.0) * max(height, 0.0)\n+    if score < min_score:\n+        return False\n+    if area < min_area:\n+        return False\n+    ratio = width / max(height, 1e-6)\n+    return FACE_RATIO_BOUNDS[0] <= ratio <= FACE_RATIO_BOUNDS[1]\n+\n+\n+def _nms_detections(\n+    detections: list[tuple[np.ndarray, float, np.ndarray | None]],\n+    thresh: float,\n+) -> list[tuple[np.ndarray, float, np.ndarray | None]]:\n+    ordered = sorted(range(len(detections)), key=lambda idx: detections[idx][1], reverse=True)\n+    keep: list[tuple[np.ndarray, float, np.ndarray | None]] = []\n+    while ordered:\n+        current_idx = ordered.pop(0)\n+        current = detections[current_idx]\n+        keep.append(current)\n+        remaining: list[int] = []\n+        for idx in ordered:\n+            iou = _bbox_iou(current[0].tolist(), detections[idx][0].tolist())\n+            if iou < thresh:\n+                remaining.append(idx)\n+        ordered = remaining\n+    return keep\n+\n+\n+@dataclass\n+class TrackAccumulator:\n+    track_id: int\n+    class_id: int | str\n+    first_ts: float\n+    last_ts: float\n+    frame_count: int = 0\n+    samples: List[dict] = field(default_factory=list)\n+\n+    def add(self, ts: float, frame_idx: int, bbox_xyxy: List[float], landmarks: List[float] | None = None) -> None:\n+        self.frame_count += 1\n+        self.last_ts = ts\n+        limit = TRACK_SAMPLE_LIMIT\n+        if limit is None or len(self.samples) < limit:\n+            self.samples.append(\n+                {\n+                    \"frame_idx\": frame_idx,\n+                    \"ts\": round(float(ts), 4),\n+                    \"bbox_xyxy\": [round(float(coord), 4) for coord in bbox_xyxy],\n+                    **({\"landmarks\": [round(float(val), 4) for val in landmarks]} if landmarks else {}),\n+                }\n+            )\n+\n+    def to_row(self) -> dict:\n+        row = {\n+            \"track_id\": self.track_id,\n+            \"class\": self.class_id,\n+            \"first_ts\": round(float(self.first_ts), 4),\n+            \"last_ts\": round(float(self.last_ts), 4),\n+            \"frame_count\": self.frame_count,\n+            \"pipeline_ver\": PIPELINE_VERSION,\n+        }\n+        if self.samples:\n+            row[\"bboxes_sampled\"] = self.samples\n+        return row\n+\n+\n+@dataclass\n+class DetectionSample:\n+    bbox: np.ndarray\n+    conf: float\n+    class_idx: int\n+    class_label: str\n+    landmarks: np.ndarray | None = None\n+    embedding: np.ndarray | None = None\n+\n+\n+@dataclass\n+class TrackedObject:\n+    track_id: int\n+    bbox: np.ndarray\n+    conf: float\n+    class_idx: int\n+    class_label: str\n+    det_index: int | None = None\n+    landmarks: np.ndarray | None = None\n+\n+\n+class _TrackerDetections:\n+    \"\"\"Lightweight structure that mimics ultralytics' Boxes for BYTETracker inputs.\"\"\"\n+\n+    def __init__(self, boxes: np.ndarray, scores: np.ndarray, classes: np.ndarray) -> None:\n+        self.xyxy = boxes.astype(np.float32)\n+        self.conf = scores.astype(np.float32)\n+        self.cls = classes.astype(np.float32)\n+        self._xywh: np.ndarray | None = None\n+\n+    @property\n+    def xywh(self) -> np.ndarray:\n+        if self._xywh is None:\n+            self._xywh = self.xyxy.copy()\n+            self._xywh[:, 2] = self._xywh[:, 2] - self._xywh[:, 0]\n+            self._xywh[:, 3] = self._xywh[:, 3] - self._xywh[:, 1]\n+            self._xywh[:, 0] = self._xywh[:, 0] + self._xywh[:, 2] / 2\n+            self._xywh[:, 1] = self._xywh[:, 1] + self._xywh[:, 3] / 2\n+        return self._xywh\n+\n+    @property\n+    def xywhr(self) -> np.ndarray:\n+        return self.xywh\n+\n+\n+def _tracker_inputs_from_samples(detections: list[DetectionSample]) -> _TrackerDetections:\n+    if detections:\n+        boxes = np.vstack([sample.bbox for sample in detections]).astype(np.float32)\n+        scores = np.asarray([sample.conf for sample in detections], dtype=np.float32)\n+        classes = np.asarray([sample.class_idx for sample in detections], dtype=np.float32)\n+        return _TrackerDetections(boxes, scores, classes)\n+    return _TrackerDetections(\n+        np.zeros((0, 4), dtype=np.float32),\n+        np.zeros(0, dtype=np.float32),\n+        np.zeros(0, dtype=np.float32),\n+    )\n+\n+\n+class ByteTrackAdapter:\n+    \"\"\"Wrapper around ultralytics BYTETracker for direct invocation.\"\"\"\n+\n+    def __init__(self, frame_rate: float = 30.0) -> None:\n+        self.frame_rate = max(frame_rate, 1)\n+        self._tracker = self._build_tracker()\n+\n+    def _build_tracker(self):\n+        from types import SimpleNamespace\n+\n+        from ultralytics.trackers.byte_tracker import BYTETracker\n+\n+        cfg = SimpleNamespace(\n+            tracker_type=\"bytetrack\",\n+            track_high_thresh=0.6,\n+            track_low_thresh=0.1,\n+            new_track_thresh=0.6,\n+            track_buffer=30,\n+            match_thresh=0.8,\n+            min_box_area=BYTE_TRACK_MIN_BOX_AREA,\n+        )\n+        return BYTETracker(cfg, frame_rate=self.frame_rate)\n+\n+    def update(self, detections: list[DetectionSample], frame_idx: int, image) -> list[TrackedObject]:\n+        det_struct = _tracker_inputs_from_samples(detections)\n+        tracks = self._tracker.update(det_struct, image)\n+        tracked: list[TrackedObject] = []\n+        if tracks.size == 0:\n+            return tracked\n+        for row in tracks:\n+            bbox = np.asarray(row[:4], dtype=np.float32)\n+            track_id = int(row[4])\n+            score = float(row[5])\n+            class_idx = int(row[6]) if len(row) > 6 else 0\n+            det_index = int(row[7]) if len(row) > 7 else None\n+            label = \"\"\n+            landmarks = None\n+            if det_index is not None and 0 <= det_index < len(detections):\n+                det = detections[det_index]\n+                label = det.class_label\n+                class_idx = det.class_idx\n+                landmarks = det.landmarks\n+            tracked.append(\n+                TrackedObject(\n+                    track_id=track_id,\n+                    bbox=bbox,\n+                    conf=score,\n+                    class_idx=class_idx,\n+                    class_label=label,\n+                    det_index=det_index,\n+                    landmarks=landmarks,\n+                )\n+            )\n+        return tracked\n+\n+    def reset(self) -> None:\n+        self._tracker = self._build_tracker()\n+\n+\n+class StrongSortAdapter:\n+    \"\"\"Adapter around Ultralytics BOT-SORT tracker (used as a StrongSORT-style ReID tracker).\"\"\"\n+\n+    def __init__(self, frame_rate: float = 30.0) -> None:\n+        self.frame_rate = max(frame_rate, 1)\n+        self._tracker = self._build_tracker()\n+\n+    def _build_tracker(self):\n+        from types import SimpleNamespace\n+\n+        try:\n+            from ultralytics.trackers.bot_sort import BOTSORT\n+        except ImportError as exc:  # pragma: no cover - dependency missing\n+            raise RuntimeError(\n+                \"StrongSORT tracker unavailable; ensure ultralytics>=8.2.70 is installed.\"\n+            ) from exc\n+\n+        cfg = SimpleNamespace(\n+            tracker_type=\"strongsort\",\n+            track_high_thresh=0.6,\n+            track_low_thresh=0.1,\n+            new_track_thresh=0.6,\n+            track_buffer=30,\n+            match_thresh=0.8,\n+            min_box_area=BYTE_TRACK_MIN_BOX_AREA,\n+            gmc_method=os.environ.get(\"SCREENALYTICS_GMC_METHOD\", DEFAULT_GMC_METHOD),\n+            proximity_thresh=float(os.environ.get(\"SCREENALYTICS_REID_PROXIMITY\", \"0.6\")),\n+            appearance_thresh=float(os.environ.get(\"SCREENALYTICS_REID_APPEARANCE\", \"0.7\")),\n+            with_reid=_env_flag(\"SCREENALYTICS_REID_ENABLED\", DEFAULT_REID_ENABLED),\n+            model=os.environ.get(\"SCREENALYTICS_REID_MODEL\", DEFAULT_REID_MODEL) or \"auto\",\n+            fuse_score=_env_flag(\"SCREENALYTICS_REID_FUSE_SCORE\", False),\n+        )\n+        return BOTSORT(cfg, frame_rate=self.frame_rate)\n+\n+    def update(self, detections: list[DetectionSample], frame_idx: int, image) -> list[TrackedObject]:\n+        det_struct = _tracker_inputs_from_samples(detections)\n+        tracks = self._tracker.update(det_struct, image)\n+        tracked: list[TrackedObject] = []\n+        if tracks.size == 0:\n+            return tracked\n+        for row in tracks:\n+            bbox = np.asarray(row[:4], dtype=np.float32)\n+            track_id = int(row[4])\n+            score = float(row[5])\n+            class_idx = int(row[6]) if len(row) > 6 else 0\n+            det_index = int(row[7]) if len(row) > 7 else None\n+            label = \"\"\n+            landmarks = None\n+            if det_index is not None and 0 <= det_index < len(detections):\n+                det = detections[det_index]\n+                label = det.class_label\n+                class_idx = det.class_idx\n+                landmarks = det.landmarks\n+            tracked.append(\n+                TrackedObject(\n+                    track_id=track_id,\n+                    bbox=bbox,\n+                    conf=score,\n+                    class_idx=class_idx,\n+                    class_label=label,\n+                    det_index=det_index,\n+                    landmarks=landmarks,\n+                )\n+            )\n+        return tracked\n+\n+    def reset(self) -> None:\n+        self._tracker = self._build_tracker()\n+\n+\n+class RetinaFaceDetectorBackend:\n+    def __init__(self, device: str, score_thresh: float = RETINAFACE_SCORE_THRESHOLD) -> None:\n+        self.device = device\n+        self.score_thresh = max(min(float(score_thresh or RETINAFACE_SCORE_THRESHOLD), 1.0), 0.0)\n+        self.min_area = MIN_FACE_AREA\n+        self._model = None\n+        self._resolved_device: Optional[str] = None\n+\n+    def _lazy_model(self):\n+        if self._model is not None:\n+            return self._model\n+        try:\n+            model, resolved = _init_retinaface(self.model_name, self.device, self.score_thresh)\n+        except Exception as exc:\n+            raise RuntimeError(f\"{RETINAFACE_HELP} ({exc})\") from exc\n+        self._resolved_device = resolved\n+        self._model = model\n+        return self._model\n+\n+    @property\n+    def model_name(self) -> str:\n+        return RETINAFACE_MODEL_NAME\n+\n+    @property\n+    def resolved_device(self) -> str:\n+        if self._resolved_device is None:\n+            self.ensure_ready()\n+        return self._resolved_device or \"cpu\"\n+\n+    def ensure_ready(self) -> None:\n+        self._lazy_model()\n+\n+    def detect(self, image) -> list[DetectionSample]:\n+        model = self._lazy_model()\n+        # Threshold + input size configured during model.prepare. Some InsightFace\n+        # RetinaFace builds still require an explicit input_size, so pass it when\n+        # available.\n+        detect_kwargs = {}\n+        input_size = getattr(model, \"input_size\", None) or RETINAFACE_DET_SIZE\n+        if input_size:\n+            detect_kwargs[\"input_size\"] = input_size\n+        bboxes, landmarks = model.detect(image, **detect_kwargs)\n+        if bboxes is None or len(bboxes) == 0:\n+            return []\n+        pending: list[tuple[np.ndarray, float, np.ndarray | None]] = []\n+        for idx in range(len(bboxes)):\n+            raw = bboxes[idx]\n+            score = float(raw[4]) if raw.shape[0] >= 5 else float(self.score_thresh)\n+            bbox = raw[:4].astype(np.float32)\n+            if not _valid_face_box(bbox, score, min_score=self.score_thresh, min_area=self.min_area):\n+                continue\n+            kps = None\n+            if landmarks is not None and idx < len(landmarks):\n+                kps = landmarks[idx].astype(np.float32).reshape(-1)\n+            pending.append((bbox, score, kps))\n+        filtered = _nms_detections(pending, RETINAFACE_NMS) if pending else []\n+        samples: list[DetectionSample] = []\n+        for bbox, score, kps in filtered:\n+            samples.append(\n+                DetectionSample(\n+                    bbox=bbox.astype(np.float32),\n+                    conf=score,\n+                    class_idx=0,\n+                    class_label=FACE_CLASS_LABEL,\n+                    landmarks=kps.copy() if isinstance(kps, np.ndarray) else None,\n+                )\n+            )\n+        return samples\n+\n+\n+class YoloFaceDetectorBackend:\n+    def __init__(self, device: str) -> None:\n+        from ultralytics import YOLO\n+\n+        self.device = device\n+        self.model_path = os.environ.get(\"SCREENALYTICS_YOLO_FACE_MODEL\", \"yolov8n-face.pt\")\n+        self._model = YOLO(self.model_path)\n+        self._resolved_device = _normalize_device_label(device)\n+\n+    @property\n+    def model_name(self) -> str:\n+        return self.model_path\n+\n+    @property\n+    def resolved_device(self) -> str:\n+        return self._resolved_device\n+\n+    def ensure_ready(self) -> None:\n+        _ = self._model\n+\n+    def detect(self, image) -> list[DetectionSample]:\n+        results = self._model.predict(\n+            source=image,\n+            imgsz=YOLO_IMAGE_SIZE,\n+            conf=YOLO_FACE_CONF,\n+            device=self.device,\n+            verbose=False,\n+        )\n+        samples: list[DetectionSample] = []\n+        if not results:\n+            return samples\n+        boxes = results[0].boxes\n+        if boxes is None or boxes.data is None or len(boxes) == 0:\n+            return samples\n+        xyxy = boxes.xyxy.cpu().numpy()\n+        scores = boxes.conf.cpu().numpy()\n+        for idx in range(len(xyxy)):\n+            bbox = xyxy[idx].astype(np.float32)\n+            score = float(scores[idx])\n+            if not _valid_face_box(bbox, score, min_score=YOLO_FACE_CONF, min_area=MIN_FACE_AREA):\n+                continue\n+            samples.append(\n+                DetectionSample(\n+                    bbox=bbox,\n+                    conf=score,\n+                    class_idx=0,\n+                    class_label=FACE_CLASS_LABEL,\n+                )\n+            )\n+        return samples\n+\n+\n+def _build_face_detector(detector: str, device: str, score_thresh: float = RETINAFACE_SCORE_THRESHOLD):\n+    if detector == \"yolov8face\":\n+        return YoloFaceDetectorBackend(device)\n+    return RetinaFaceDetectorBackend(device, score_thresh=score_thresh)\n+\n+\n+def _build_tracker_adapter(tracker: str, frame_rate: float) -> ByteTrackAdapter | StrongSortAdapter:\n+    if tracker == \"strongsort\":\n+        return StrongSortAdapter(frame_rate=frame_rate)\n+    return ByteTrackAdapter(frame_rate=frame_rate)\n+\n+\n+class ArcFaceEmbedder:\n+    def __init__(self, device: str) -> None:\n+        self.device = device\n+        self._model = None\n+        self._resolved_device: Optional[str] = None\n+\n+    def _lazy_model(self):\n+        if self._model is not None:\n+            return self._model\n+        try:\n+            model, resolved = _init_arcface(ARC_FACE_MODEL_NAME, self.device)\n+        except Exception as exc:\n+            raise RuntimeError(f\"ArcFace init failed: {exc}. Install insightface + models or run scripts/fetch_models.py.\") from exc\n+        self._resolved_device = resolved\n+        self._model = model\n+        return self._model\n+\n+    def ensure_ready(self) -> None:\n+        self._lazy_model()\n+\n+    @property\n+    def resolved_device(self) -> str:\n+        if self._resolved_device is None:\n+            self.ensure_ready()\n+        return self._resolved_device or \"cpu\"\n+\n+    def encode(self, crops: list[np.ndarray]) -> np.ndarray:\n+        if not crops:\n+            return np.zeros((0, 512), dtype=np.float32)\n+        model = self._lazy_model()\n+        embeddings: list[np.ndarray] = []\n+        for crop in crops:\n+            if crop is None or crop.size == 0:\n+                embeddings.append(np.zeros(512, dtype=np.float32))\n+                continue\n+            resized = _resize_for_arcface(crop)\n+            feat = model.get_feat(resized)\n+            vec = np.asarray(feat, dtype=np.float32)\n+            norm = np.linalg.norm(vec)\n+            if norm > 0:\n+                vec = vec / norm\n+            embeddings.append(vec)\n+        return np.vstack(embeddings)\n+\n+\n+def _resize_for_arcface(image):\n+    import cv2  # type: ignore\n+\n+    target = (112, 112)\n+    resized = cv2.resize(image, target)\n+    return resized\n+\n+\n+def _prepare_face_crop(\n+    image, bbox: list[float], landmarks: list[float] | None, margin: float = 0.15\n+) -> tuple[np.ndarray | None, str | None]:\n+    import numpy as _np\n+\n+    if landmarks and len(landmarks) >= 10:\n+        try:\n+            from insightface.utils import face_align  # type: ignore\n+\n+            pts = _np.asarray(landmarks, dtype=_np.float32).reshape(-1, 2)\n+            aligned = face_align.norm_crop(image, landmark=pts)\n+            return to_u8_bgr(aligned), None\n+        except Exception:\n+            pass\n+    x1, y1, x2, y2 = bbox\n+    width = max(x2 - x1, 1.0)\n+    height = max(y2 - y1, 1.0)\n+    expand_x = width * margin\n+    expand_y = height * margin\n+    expanded_box = [\n+        x1 - expand_x,\n+        y1 - expand_y,\n+        x2 + expand_x,\n+        y2 + expand_y,\n+    ]\n+    crop, _, err = safe_crop(image, expanded_box)\n+    if crop is None:\n+        return None, err or \"crop_failed\"\n+    return crop, None\n+\n+\n+def _make_skip_face_row(\n+    ep_id: str,\n+    track_id: int,\n+    frame_idx: int,\n+    ts_val: float,\n+    bbox: list[float],\n+    detector_choice: str,\n+    reason: str,\n+    *,\n+    crop_rel_path: str | None = None,\n+    crop_s3_key: str | None = None,\n+    thumb_rel_path: str | None = None,\n+    thumb_s3_key: str | None = None,\n+) -> Dict[str, Any]:\n+    row: Dict[str, Any] = {\n+        \"ep_id\": ep_id,\n+        \"face_id\": f\"face_{track_id:04d}_{frame_idx:06d}\",\n+        \"track_id\": track_id,\n+        \"frame_idx\": frame_idx,\n+        \"ts\": ts_val,\n+        \"bbox_xyxy\": bbox,\n+        \"detector\": detector_choice,\n+        \"pipeline_ver\": PIPELINE_VERSION,\n+        \"skip\": reason,\n+    }\n+    if crop_rel_path:\n+        row[\"crop_rel_path\"] = crop_rel_path\n+    if crop_s3_key:\n+        row[\"crop_s3_key\"] = crop_s3_key\n+    if thumb_rel_path:\n+        row[\"thumb_rel_path\"] = thumb_rel_path\n+    if thumb_s3_key:\n+        row[\"thumb_s3_key\"] = thumb_s3_key\n+    return row\n+\n+\n+class TrackRecorder:\n+    \"\"\"Maintains exported track ids, metrics, and sampled boxes.\"\"\"\n+\n+    def __init__(self, *, max_gap: int, remap_ids: bool) -> None:\n+        self.max_gap = max(1, int(max_gap))\n+        self.remap_ids = remap_ids\n+        self._next_export_id = 1\n+        self._mapping: dict[int, dict[str, int]] = {}\n+        self._active_exports: set[int] = set()\n+        self._accumulators: dict[int, TrackAccumulator] = {}\n+        self.metrics = {\"tracks_born\": 0, \"tracks_lost\": 0, \"id_switches\": 0}\n+\n+    def _spawn_export_id(self) -> int:\n+        export_id = self._next_export_id\n+        self._next_export_id += 1\n+        self._active_exports.add(export_id)\n+        self.metrics[\"tracks_born\"] += 1\n+        return export_id\n+\n+    def _complete_track(self, export_id: int) -> None:\n+        if export_id in self._active_exports:\n+            self._active_exports.remove(export_id)\n+            self.metrics[\"tracks_lost\"] += 1\n+\n+    def record(\n+        self,\n+        *,\n+        tracker_track_id: int,\n+        frame_idx: int,\n+        ts: float,\n+        bbox: list[float] | np.ndarray,\n+        class_label: int | str,\n+        landmarks: list[float] | None = None,\n+    ) -> int:\n+        if isinstance(bbox, np.ndarray):\n+            bbox_values = bbox.tolist()\n+        else:\n+            bbox_values = bbox\n+        export_id: int\n+        mapping = self._mapping.get(tracker_track_id)\n+        if self.remap_ids:\n+            start_new = mapping is None\n+            if mapping is not None:\n+                gap = frame_idx - mapping.get(\"last_frame\", frame_idx)\n+                if gap > self.max_gap:\n+                    self.metrics[\"id_switches\"] += 1\n+                    self._complete_track(mapping[\"export_id\"])\n+                    start_new = True\n+            if start_new:\n+                export_id = self._spawn_export_id()\n+            else:\n+                export_id = mapping[\"export_id\"]\n+            self._mapping[tracker_track_id] = {\"export_id\": export_id, \"last_frame\": frame_idx}\n+        else:\n+            if mapping is None:\n+                export_id = tracker_track_id\n+                self._active_exports.add(export_id)\n+                self._mapping[tracker_track_id] = {\"export_id\": export_id, \"last_frame\": frame_idx}\n+                self.metrics[\"tracks_born\"] += 1\n+            else:\n+                export_id = mapping[\"export_id\"]\n+                mapping[\"last_frame\"] = frame_idx\n+        track = self._accumulators.get(export_id)\n+        if track is None:\n+            track = TrackAccumulator(track_id=export_id, class_id=class_label, first_ts=ts, last_ts=ts)\n+            self._accumulators[export_id] = track\n+        track.add(ts, frame_idx, bbox_values, landmarks=landmarks)\n+        return export_id\n+\n+    def finalize(self) -> None:\n+        for export_id in list(self._active_exports):\n+            self._complete_track(export_id)\n+        self._mapping.clear()\n+\n+    def rows(self) -> list[dict]:\n+        payload: list[dict] = []\n+        for track in sorted(self._accumulators.values(), key=lambda item: item.track_id):\n+            payload.append(track.to_row())\n+        return payload\n+\n+    def top_long_tracks(self, limit: int = 5) -> list[dict]:\n+        longest = sorted(self._accumulators.values(), key=lambda item: item.frame_count, reverse=True)[:limit]\n+        return [\n+            {\"track_id\": track.track_id, \"frame_count\": track.frame_count}\n+            for track in longest\n+            if track.frame_count > 0\n+        ]\n+\n+\n+def _bbox_iou(box_a: List[float], box_b: List[float]) -> float:\n+    ax1, ay1, ax2, ay2 = box_a\n+    bx1, by1, bx2, by2 = box_b\n+    inter_x1 = max(ax1, bx1)\n+    inter_y1 = max(ay1, by1)\n+    inter_x2 = min(ax2, bx2)\n+    inter_y2 = min(ay2, by2)\n+    inter_w = max(inter_x2 - inter_x1, 0.0)\n+    inter_h = max(inter_y2 - inter_y1, 0.0)\n+    inter_area = inter_w * inter_h\n+    if inter_area <= 0:\n+        return 0.0\n+    area_a = max(ax2 - ax1, 0.0) * max(ay2 - ay1, 0.0)\n+    area_b = max(bx2 - bx1, 0.0) * max(by2 - by1, 0.0)\n+    denom = area_a + area_b - inter_area\n+    if denom <= 0:\n+        return 0.0\n+    return inter_area / denom\n+\n+\n+def _try_import(module: str):\n+    try:\n+        return __import__(module)\n+    except ImportError:\n+        return None\n+\n+\n+def sanitize_xyxy(x1: float, y1: float, x2: float, y2: float, width: int, height: int) -> tuple[int, int, int, int] | None:\n+    \"\"\"Round + clamp XYXY boxes to integer pixel coordinates, skipping empty windows.\"\"\"\n+    if width <= 0 or height <= 0:\n+        return None\n+    x1_int = int(max(0, min(round(x1), width - 1)))\n+    y1_int = int(max(0, min(round(y1), height - 1)))\n+    x2_int = int(max(0, min(round(x2), width)))\n+    y2_int = int(max(0, min(round(y2), height)))\n+    if x2_int <= x1_int or y2_int <= y1_int:\n+        return None\n+    return x1_int, y1_int, x2_int, y2_int\n+\n+\n+def _image_stats(image) -> tuple[float, float, float]:\n+    arr = np.asarray(image)\n+    if arr.size == 0:\n+        return 0.0, 0.0, 0.0\n+    return float(np.nanmin(arr)), float(np.nanmax(arr)), float(np.nanmean(arr))\n+\n+\n+def _normalize_to_uint8(image: np.ndarray) -> np.ndarray:\n+    arr = np.asarray(image)\n+    if arr.size == 0:\n+        return arr\n+    if arr.dtype == np.uint8:\n+        return arr\n+    if np.issubdtype(arr.dtype, np.floating):\n+        mn = float(np.nanmin(arr))\n+        mx = float(np.nanmax(arr))\n+        if mx <= 1.0 and mn >= 0.0:\n+            arr = (arr * 255.0).round()\n+        elif mn >= -1.0 and mx <= 1.0:\n+            arr = ((arr + 1.0) * 127.5).round()\n+        arr = np.nan_to_num(arr, nan=0.0, posinf=255.0, neginf=0.0)\n+        return np.clip(arr, 0, 255).astype(np.uint8)\n+    if np.issubdtype(arr.dtype, np.integer):\n+        arr = np.clip(arr.astype(np.int64), 0, 255)\n+        return arr.astype(np.uint8)\n+    return arr.astype(np.uint8, copy=False)\n+\n+\n+def save_jpeg(path: str | Path, image, *, quality: int = 85, color: str = \"bgr\") -> None:\n+    \"\"\"Normalize + persist an image to JPEG, ensuring non-blank uint8 BGR data.\"\"\"\n+    import cv2  # type: ignore\n+\n+    arr = np.asarray(image)\n+    if arr.size == 0:\n+        raise ValueError(f\"Cannot save empty image to {path}\")\n+    arr = np.ascontiguousarray(_normalize_to_uint8(arr))\n+    if arr.ndim == 2:\n+        arr = cv2.cvtColor(arr, cv2.COLOR_GRAY2BGR)\n+    elif arr.ndim == 3 and arr.shape[2] == 3:\n+        mode = color.lower()\n+        if mode == \"rgb\":\n+            arr = cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)\n+        elif mode not in {\"bgr\", \"rgb\"}:\n+            raise ValueError(f\"Unsupported color mode '{color}'\")\n+    else:\n+        raise ValueError(f\"Unsupported image shape for JPEG write: {arr.shape}\")\n+    arr = np.ascontiguousarray(arr)\n+    jpeg_quality = max(1, min(int(quality or 85), 100))\n+    out_path = Path(path)\n+    out_path.parent.mkdir(parents=True, exist_ok=True)\n+    ok = cv2.imwrite(str(out_path), arr, [cv2.IMWRITE_JPEG_QUALITY, jpeg_quality])\n+    if not ok:\n+        raise RuntimeError(f\"cv2.imwrite failed for {out_path}\")\n+\n+\n+class ThumbWriter:\n+    def __init__(self, ep_id: str, size: int = 256, jpeg_quality: int = 85) -> None:\n+        self.ep_id = ep_id\n+        self.size = size\n+        self.jpeg_quality = max(1, min(int(jpeg_quality or 85), 100))\n+        self.root_dir = get_path(ep_id, \"frames_root\") / \"thumbs\"\n+        self.root_dir.mkdir(parents=True, exist_ok=True)\n+        self._stat_samples = 0\n+        self._stat_limit = 10\n+        try:\n+            import cv2  # type: ignore\n+\n+            self._cv2 = cv2\n+        except ImportError:\n+            self._cv2 = None\n+\n+    def write(self, image, bbox: List[float], track_id: int, frame_idx: int) -> tuple[str | None, Path | None]:\n+        if self._cv2 is None or image is None:\n+            return None, None\n+        crop, clipped_bbox, err = safe_crop(image, bbox)\n+        if crop is None:\n+            LOGGER.debug(\"Skipping thumb track=%s frame=%s reason=%s\", track_id, frame_idx, err)\n+            return None, None\n+        thumb = self._letterbox(crop)\n+        if self._stat_samples < self._stat_limit:\n+            mn, mx, mean = _image_stats(thumb)\n+            LOGGER.info(\"thumb stats track=%s frame=%s min=%.3f max=%.3f mean=%.3f\", track_id, frame_idx, mn, mx, mean)\n+            if mx - mn < 1e-6:\n+                LOGGER.warning(\"Nearly constant thumb track=%s frame=%s\", track_id, frame_idx)\n+            self._stat_samples += 1\n+        rel_path = Path(f\"track_{track_id:04d}/thumb_{frame_idx:06d}.jpg\")\n+        abs_path = self.root_dir / rel_path\n+        ok, reason = safe_imwrite(abs_path, thumb, self.jpeg_quality)\n+        if not ok:\n+            LOGGER.warning(\"Failed to write thumb %s: %s\", abs_path, reason)\n+            return None, None\n+        return rel_path.as_posix(), abs_path\n+\n+    def _letterbox(self, crop):\n+        if self._cv2 is None:\n+            return np.zeros((self.size, self.size, 3), dtype=np.uint8)\n+        if crop.size == 0:\n+            crop = np.zeros((self.size, self.size, 3), dtype=np.uint8)\n+        h, w = crop.shape[:2]\n+        scale = min(self.size / max(w, 1), self.size / max(h, 1))\n+        new_w = max(int(w * scale), 1)\n+        new_h = max(int(h * scale), 1)\n+        resized = self._cv2.resize(crop, (new_w, new_h))\n+        canvas = np.zeros((self.size, self.size, 3), dtype=np.uint8)\n+        top = (self.size - new_h) // 2\n+        left = (self.size - new_w) // 2\n+        canvas[top : top + new_h, left : left + new_w] = resized\n+        return canvas\n+\n+\n+def _faces_embed_path(ep_id: str) -> Path:\n+    embed_dir = DATA_ROOT / \"embeds\" / ep_id\n+    embed_dir.mkdir(parents=True, exist_ok=True)\n+    return embed_dir / \"faces.npy\"\n+class ProgressEmitter:\n+    \"\"\"Emit structured progress to stdout + optional file for SSE/polling.\"\"\"\n+\n+    VERSION = 2\n+\n+    def __init__(\n+        self,\n+        ep_id: str,\n+        file_path: str | Path | None,\n+        *,\n+        frames_total: int,\n+        secs_total: float | None,\n+        stride: int,\n+        fps_detected: float | None,\n+        fps_requested: float | None,\n+        frame_interval: int | None = None,\n+        run_id: str | None = None,\n+    ) -> None:\n+        import uuid\n+        self.ep_id = ep_id\n+        self.run_id = run_id or str(uuid.uuid4())\n+        self.path = Path(file_path).expanduser() if file_path else None\n+        if self.path:\n+            self.path.parent.mkdir(parents=True, exist_ok=True)\n+        self.frames_total = max(int(frames_total or 0), 0)\n+        self.secs_total = float(secs_total) if secs_total else None\n+        self.stride = max(int(stride), 1)\n+        self.fps_detected = float(fps_detected) if fps_detected else None\n+        self.fps_requested = float(fps_requested) if fps_requested else None\n+        default_interval = PROGRESS_FRAME_STEP\n+        chosen_interval = frame_interval if frame_interval is not None else default_interval\n+        self._frame_interval = max(int(chosen_interval), 1)\n+        self._start_ts = time.time()\n+        self._last_frames = 0\n+        self._last_phase: str | None = None\n+        self._last_step: str | None = None\n+        self._device: str | None = None\n+        self._detector: str | None = None\n+        self._tracker: str | None = None\n+        self._resolved_device: str | None = None\n+        self._closed = False\n+\n+    def _now(self) -> str:\n+        return datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n+\n+    def _should_emit(self, frames_done: int, phase: str, step: str | None, force: bool) -> bool:\n+        if force:\n+            return True\n+        if phase != self._last_phase:\n+            return True\n+        if step != self._last_step:\n+            return True\n+        return (frames_done - self._last_frames) >= self._frame_interval\n+\n+    def _compose_payload(\n+        self,\n+        frames_done: int,\n+        phase: str,\n+        device: str | None,\n+        summary: Dict[str, object] | None,\n+        error: str | None,\n+        detector: str | None,\n+        tracker: str | None,\n+        resolved_device: str | None,\n+        extra: Dict[str, Any] | None = None,\n+    ) -> Dict[str, object]:\n+        secs_done = time.time() - self._start_ts\n+        fps_infer = None\n+        if secs_done > 0 and frames_done >= 0:\n+            fps_infer = frames_done / secs_done\n+        payload: Dict[str, object] = {\n+            \"progress_version\": self.VERSION,\n+            \"ep_id\": self.ep_id,\n+            \"run_id\": self.run_id,\n+            \"phase\": phase,\n+            \"frames_done\": frames_done,\n+            \"frames_total\": self.frames_total,\n+            \"secs_done\": round(float(secs_done), 3),\n+            \"secs_total\": round(float(self.secs_total), 3) if self.secs_total else None,\n+            \"device\": device or self._device,\n+            \"fps_infer\": round(float(fps_infer), 3) if fps_infer else None,\n+            \"fps_detected\": round(float(self.fps_detected), 3) if self.fps_detected else None,\n+            \"fps_requested\": round(float(self.fps_requested), 3) if self.fps_requested else None,\n+            \"stride\": self.stride,\n+            \"updated_at\": self._now(),\n+            \"detector\": detector or self._detector,\n+            \"tracker\": tracker or self._tracker,\n+            \"resolved_device\": resolved_device or self._resolved_device,\n+        }\n+        if summary:\n+            payload[\"summary\"] = summary\n+        if error:\n+            payload[\"error\"] = error\n+        if extra:\n+            payload.update(extra)\n+        return payload\n+\n+    def _write_payload(self, payload: Dict[str, object]) -> None:\n+        line = json.dumps(payload, sort_keys=True)\n+        print(line, flush=True)\n+\n+        # Structured logging for episode-wide grep\n+        phase = payload.get(\"phase\", \"\")\n+        step = payload.get(\"step\", \"\")\n+        frames = payload.get(\"frames_done\", 0)\n+        total = payload.get(\"frames_total\", 0)\n+        vt = payload.get(\"video_time\")\n+        vtotal = payload.get(\"video_total\")\n+        fps = payload.get(\"fps_infer\")\n+        run_id_short = self.run_id[:8] if self.run_id else \"unknown\"\n+\n+        if vt is not None and vtotal is not None:\n+            LOGGER.info(\n+                \"[job=%s run=%s phase=%s step=%s frames=%s/%s vt=%.1f/%.1f fps=%.2f]\",\n+                self.ep_id, run_id_short, phase, step, frames, total, vt, vtotal, fps or 0.0\n+            )\n+        else:\n+            LOGGER.info(\n+                \"[job=%s run=%s phase=%s step=%s frames=%s/%s fps=%.2f]\",\n+                self.ep_id, run_id_short, phase, step, frames, total, fps or 0.0\n+            )\n+\n+        if self.path:\n+            tmp_path = self.path.with_suffix(\".tmp\")\n+            tmp_path.write_text(line, encoding=\"utf-8\")\n+            tmp_path.replace(self.path)\n+\n+    def emit(\n+        self,\n+        frames_done: int,\n+        *,\n+        phase: str,\n+        device: str | None = None,\n+        summary: Dict[str, object] | None = None,\n+        error: str | None = None,\n+        force: bool = False,\n+        detector: str | None = None,\n+        tracker: str | None = None,\n+        resolved_device: str | None = None,\n+        extra: Dict[str, Any] | None = None,\n+        **fields: Any,\n+    ) -> None:\n+        if self._closed:\n+            return\n+        frames_done = max(int(frames_done), 0)\n+        if self.frames_total and frames_done > self.frames_total:\n+            frames_done = self.frames_total\n+\n+        # Extract step from extra dict if present\n+        step = None\n+        if extra and \"step\" in extra:\n+            step = extra.get(\"step\")\n+\n+        if not self._should_emit(frames_done, phase, step, force):\n+            return\n+        if device is not None:\n+            self._device = device\n+        if detector is not None:\n+            self._detector = detector\n+        if tracker is not None:\n+            self._tracker = tracker\n+        if resolved_device is not None:\n+            self._resolved_device = resolved_device\n+        combined_extra: Dict[str, Any] = {} if extra is None else dict(extra)\n+        if fields:\n+            combined_extra.update(fields)\n+        payload = self._compose_payload(\n+            frames_done,\n+            phase,\n+            device,\n+            summary,\n+            error,\n+            detector,\n+            tracker,\n+            resolved_device,\n+            combined_extra or None,\n+        )\n+        self._write_payload(payload)\n+        self._last_frames = frames_done\n+        self._last_phase = phase\n+        self._last_step = step\n+\n+    def complete(\n+        self,\n+        summary: Dict[str, object],\n+        device: str | None = None,\n+        detector: str | None = None,\n+        tracker: str | None = None,\n+        resolved_device: str | None = None,\n+        *,\n+        step: str | None = None,\n+        extra: Dict[str, Any] | None = None,\n+    ) -> None:\n+        final_frames = self.frames_total or summary.get(\"frames_sampled\") or self._last_frames\n+        final_frames = int(final_frames or 0)\n+        completion_extra: Dict[str, Any] = {} if extra is None else dict(extra)\n+        if step:\n+            completion_extra[\"step\"] = step\n+        self.emit(\n+            final_frames,\n+            phase=\"done\",\n+            device=device,\n+            summary=summary,\n+            force=True,\n+            detector=detector,\n+            tracker=tracker,\n+            resolved_device=resolved_device,\n+            extra=completion_extra or None,\n+        )\n+\n+    def fail(self, error: str) -> None:\n+        self.emit(self._last_frames, phase=\"error\", error=error, force=True, tracker=self._tracker)\n+\n+    @property\n+    def target_frames(self) -> int:\n+        return self.frames_total or 0\n+\n+    def close(self) -> None:\n+        self._closed = True\n+\n+\n+def _non_video_phase_meta(step: str | None = None) -> Dict[str, Any]:\n+    meta: Dict[str, Any] = {\"video_time\": None, \"video_total\": None}\n+    if step:\n+        meta[\"step\"] = step\n+    return meta\n+\n+\n+def _video_phase_meta(frames_done: int, frames_total: int | None, fps: float | None, step: str | None = None) -> Dict[str, Any]:\n+    meta: Dict[str, Any] = {}\n+    if fps and fps > 0 and frames_total and frames_total > 0:\n+        video_total = frames_total / fps\n+        video_time = min(frames_done / fps, video_total)\n+        meta[\"video_total\"] = round(video_total, 3)\n+        meta[\"video_time\"] = round(video_time, 3)\n+    else:\n+        meta[\"video_time\"] = None\n+        meta[\"video_total\"] = None\n+    if step:\n+        meta[\"step\"] = step\n+    return meta\n+\n+\n+def _utcnow_iso() -> str:\n+    return datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n+\n+\n+def _write_run_marker(ep_id: str, phase: str, payload: Dict[str, Any]) -> None:\n+    manifests_dir = get_path(ep_id, \"detections\").parent\n+    run_dir = manifests_dir / RUN_MARKERS_SUBDIR\n+    run_dir.mkdir(parents=True, exist_ok=True)\n+    marker_path = run_dir / f\"{phase}.json\"\n+    marker_path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n+\n+\n+class FrameExporter:\n+    \"\"\"Handles optional frame + crop JPEG exports for S3 sync.\"\"\"\n+\n+    def __init__(\n+        self,\n+        ep_id: str,\n+        *,\n+        save_frames: bool,\n+        save_crops: bool,\n+        jpeg_quality: int,\n+        debug_logger: JsonlLogger | NullLogger | None = None,\n+    ) -> None:\n+        self.ep_id = ep_id\n+        self.save_frames = save_frames\n+        self.save_crops = save_crops\n+        self.jpeg_quality = max(1, min(int(jpeg_quality or 85), 100))\n+        self.root_dir = get_path(ep_id, \"frames_root\")\n+        self.frames_dir = self.root_dir / \"frames\"\n+        self.crops_dir = self.root_dir / \"crops\"\n+        if self.save_frames:\n+            self.frames_dir.mkdir(parents=True, exist_ok=True)\n+        if self.save_crops:\n+            self.crops_dir.mkdir(parents=True, exist_ok=True)\n+        self.frames_written = 0\n+        self.crops_written = 0\n+        self._track_indexes: Dict[int, Dict[str, Dict[str, Any]]] = defaultdict(dict)\n+        self._stat_samples = 0\n+        self._stat_limit = 10\n+        self._crop_attempts = 0\n+        self._crop_error_counts: Counter[str] = Counter()\n+        self._fail_fast_threshold = 0.10\n+        self._fail_fast_min_attempts = 10\n+        self._fail_fast_reasons = {\"near_uniform_gray\", \"tiny_file\"}\n+        self.debug_logger = debug_logger\n+\n+    def _log_image_stats(self, kind: str, path: Path, image) -> None:\n+        if self._stat_samples >= self._stat_limit:\n+            return\n+        mn, mx, mean = _image_stats(image)\n+        LOGGER.info(\"%s stats %s min=%.3f max=%.3f mean=%.3f\", kind, path, mn, mx, mean)\n+        if mx - mn < 1e-6:\n+            LOGGER.warning(\"Nearly constant %s %s mn=%.6f mx=%.6f mean=%.6f\", kind, path, mn, mx, mean)\n+        self._stat_samples += 1\n+\n+    def export(self, frame_idx: int, image, crops: List[Tuple[int, List[float]]], ts: float | None = None) -> None:\n+        if not (self.save_frames or self.save_crops):\n+            return\n+        if self.save_frames:\n+            frame_path = self.frames_dir / f\"frame_{frame_idx:06d}.jpg\"\n+            try:\n+                self._log_image_stats(\"frame\", frame_path, image)\n+                save_jpeg(frame_path, image, quality=self.jpeg_quality, color=\"bgr\")\n+                self.frames_written += 1\n+            except Exception as exc:  # pragma: no cover - best effort\n+                LOGGER.warning(\"Failed to save frame %s: %s\", frame_path, exc)\n+        if self.save_crops and crops:\n+            for track_id, bbox in crops:\n+                if track_id is None:\n+                    continue\n+                crop_path = self.crop_abs_path(track_id, frame_idx)\n+                try:\n+                    saved = self._write_crop(image, bbox, crop_path, track_id, frame_idx)\n+                except Exception as exc:  # pragma: no cover - best effort\n+                    LOGGER.warning(\"Failed to save crop %s: %s\", crop_path, exc)\n+                    self._register_crop_attempt(\"exception\")\n+                    saved = False\n+                if saved:\n+                    self.crops_written += 1\n+                    self._record_crop_index(track_id, frame_idx, ts)\n+\n+    def crop_component(self, track_id: int, frame_idx: int) -> str:\n+        return f\"track_{track_id:04d}/frame_{frame_idx:06d}.jpg\"\n+\n+    def crop_rel_path(self, track_id: int, frame_idx: int) -> str:\n+        return f\"crops/{self.crop_component(track_id, frame_idx)}\"\n+\n+    def crop_abs_path(self, track_id: int, frame_idx: int) -> Path:\n+        return self.crops_dir / self.crop_component(track_id, frame_idx)\n+\n+    def _record_crop_index(self, track_id: int, frame_idx: int, ts: float | None) -> None:\n+        if not self.save_crops:\n+            return\n+        key = self.crop_component(track_id, frame_idx)\n+        entry = {\n+            \"key\": key,\n+            \"frame_idx\": int(frame_idx),\n+            \"ts\": round(float(ts), 4) if ts is not None else None,\n+        }\n+        self._track_indexes.setdefault(track_id, {})[key] = entry\n+\n+    def write_indexes(self) -> None:\n+        if not self.save_crops or not self._track_indexes:\n+            return\n+        for track_id, entries in self._track_indexes.items():\n+            if not entries:\n+                continue\n+            track_dir = self.crops_dir / f\"track_{track_id:04d}\"\n+            if not track_dir.exists():\n+                continue\n+            ordered = sorted(entries.values(), key=lambda item: item[\"frame_idx\"])\n+            index_path = track_dir / \"index.json\"\n+            try:\n+                index_path.write_text(json.dumps(ordered, indent=2), encoding=\"utf-8\")\n+            except Exception as exc:  # pragma: no cover - best effort\n+                LOGGER.warning(\"Failed to write crop index %s: %s\", index_path, exc)\n+\n+    def _register_crop_attempt(self, reason: str | None) -> None:\n+        self._crop_attempts += 1\n+        if reason:\n+            self._crop_error_counts[reason] += 1\n+        if reason in self._fail_fast_reasons:\n+            self._maybe_fail_fast()\n+\n+    def _maybe_fail_fast(self) -> None:\n+        if self._crop_attempts < self._fail_fast_min_attempts:\n+            return\n+        bad = sum(self._crop_error_counts.get(reason, 0) for reason in self._fail_fast_reasons)\n+        ratio = bad / max(self._crop_attempts, 1)\n+        if ratio >= self._fail_fast_threshold:\n+            raise RuntimeError(\n+                f\"Too many invalid crops ({bad}/{self._crop_attempts}, {ratio:.1%}); aborting export\"\n+            )\n+\n+    def _write_crop(\n+        self,\n+        image,\n+        bbox: List[float],\n+        crop_path: Path,\n+        track_id: int,\n+        frame_idx: int,\n+    ) -> bool:\n+        start = time.time()\n+        bbox_vals = [float(val) for val in bbox]\n+        crop, clipped_bbox, crop_err = safe_crop(image, bbox_vals)\n+        debug_payload: Dict[str, Any] | None = None\n+        if self.debug_logger:\n+            debug_payload = {\n+                \"track_id\": track_id,\n+                \"frame_idx\": frame_idx,\n+                \"out\": str(crop_path),\n+                \"bbox\": bbox_vals,\n+                \"clipped_bbox\": list(clipped_bbox) if clipped_bbox else None,\n+                \"err_before_save\": crop_err,\n+            }\n+        if crop is None:\n+            self._register_crop_attempt(crop_err or \"no_crop\")\n+            if debug_payload is not None:\n+                debug_payload.update(\n+                    {\n+                        \"save_ok\": False,\n+                        \"save_err\": crop_err or \"no_crop\",\n+                        \"ms\": int((time.time() - start) * 1000),\n+                    }\n+                )\n+                self._emit_debug(debug_payload)\n+            return False\n+\n+        ok, save_err = safe_imwrite(crop_path, crop, self.jpeg_quality)\n+        reason = save_err if not ok else None\n+        self._register_crop_attempt(reason)\n+\n+        if debug_payload is not None:\n+            mn, mx, mean = _image_stats(crop)\n+            debug_payload.update(\n+                {\n+                    \"shape\": tuple(int(x) for x in crop.shape),\n+                    \"dtype\": str(crop.dtype),\n+                    \"min\": mn,\n+                    \"max\": mx,\n+                    \"mean\": mean,\n+                    \"save_ok\": bool(ok),\n+                    \"save_err\": save_err,\n+                    \"file_size\": crop_path.stat().st_size if ok and crop_path.exists() else None,\n+                    \"ms\": int((time.time() - start) * 1000),\n+                }\n+            )\n+            self._emit_debug(debug_payload)\n+\n+        if not ok and save_err:\n+            LOGGER.warning(\"Failed to save crop %s: %s\", crop_path, save_err)\n+        return bool(ok)\n+\n+    def _emit_debug(self, payload: Dict[str, Any]) -> None:\n+        if not self.debug_logger:\n+            return\n+        try:\n+            self.debug_logger(payload)\n+        except Exception:  # pragma: no cover - best effort diagnostics\n+            pass\n+\n+\n+class FrameDecoder:\n+    \"\"\"Random-access video frame reader.\"\"\"\n+\n+    def __init__(self, video_path: Path) -> None:\n+        import cv2  # type: ignore\n+\n+        self._cv2 = cv2\n+        self._cap = cv2.VideoCapture(str(video_path))\n+        if not self._cap.isOpened():\n+            raise FileNotFoundError(f\"Unable to open video {video_path}\")\n+\n+    def read(self, frame_idx: int):\n+        self._cap.set(self._cv2.CAP_PROP_POS_FRAMES, max(int(frame_idx), 0))\n+        ok, frame = self._cap.read()\n+        if not ok:\n+            raise RuntimeError(f\"Failed to decode frame {frame_idx}\")\n+        return frame\n+\n+    def close(self) -> None:\n+        if self._cap is not None:\n+            self._cap.release()\n+            self._cap = None\n+\n+    def __del__(self) -> None:  # pragma: no cover - defensive\n+        try:\n+            self.close()\n+        except Exception:\n+            pass\n+\n+\n+def _copy_video(src: Path, dest: Path) -> None:\n+    dest.parent.mkdir(parents=True, exist_ok=True)\n+    if src.resolve() == dest.resolve():\n+        return\n+    shutil.copy2(src, dest)\n+\n+\n+def _estimate_duration(frame_count: int, fps: float) -> float | None:\n+    if frame_count > 0 and fps > 0:\n+        return frame_count / fps\n+    return None\n+\n+\n+def _estimate_frame_budget(\n+    *,\n+    stride: int,\n+    target_fps: float | None,\n+    detected_fps: float,\n+    duration_sec: float | None,\n+    frame_count: int,\n+) -> int:\n+    stride = max(stride, 1)\n+    fps_source = target_fps if target_fps and target_fps > 0 else detected_fps\n+    if fps_source and fps_source > 0 and duration_sec:\n+        value = int(math.ceil((fps_source * duration_sec) / stride))\n+    elif frame_count > 0:\n+        value = int(math.ceil(frame_count / stride))\n+    else:\n+        value = 0\n+    return max(value, 1)\n+\n+\n+def _episode_ctx(ep_id: str) -> EpisodeContext | None:\n+    try:\n+        return episode_context_from_id(ep_id)\n+    except ValueError:\n+        LOGGER.warning(\"Unable to parse episode id '%s'; artifact prefixes unavailable\", ep_id)\n+        return None\n+ \n+\n+def _storage_context(ep_id: str) -> tuple[StorageService | None, EpisodeContext | None, Dict[str, str] | None]:\n+    storage_backend = os.environ.get(\"STORAGE_BACKEND\", \"local\").lower()\n+    storage: StorageService | None = None\n+    if storage_backend in {\"s3\", \"minio\"}:\n+        try:\n+            storage = StorageService()\n+        except Exception as exc:  # pragma: no cover - best effort init\n+            LOGGER.warning(\"Storage init failed (%s); disabling uploads\", exc)\n+            storage = None\n+    ep_ctx = _episode_ctx(ep_id)\n+    prefixes = artifact_prefixes(ep_ctx) if ep_ctx else None\n+    return storage, ep_ctx, prefixes\n+\n+\n+def _sync_artifacts_to_s3(\n+    ep_id: str,\n+    storage: StorageService | None,\n+    ep_ctx: EpisodeContext | None,\n+    exporter: FrameExporter | None,\n+    thumb_dir: Path | None = None,\n+) -> Dict[str, int]:\n+    stats = {\"manifests\": 0, \"frames\": 0, \"crops\": 0, \"thumbs_tracks\": 0, \"thumbs_identities\": 0}\n+    if storage is None or ep_ctx is None or not storage.s3_enabled() or not storage.write_enabled:\n+        return stats\n+    prefixes = artifact_prefixes(ep_ctx)\n+    manifests_dir = get_path(ep_id, \"detections\").parent\n+    stats[\"manifests\"] = storage.upload_dir(manifests_dir, prefixes[\"manifests\"])\n+    frames_root = get_path(ep_id, \"frames_root\")\n+    frames_dir = frames_root / \"frames\"\n+    crops_dir = frames_root / \"crops\"\n+    if exporter and exporter.save_frames and exporter.frames_dir.exists():\n+        stats[\"frames\"] = storage.upload_dir(exporter.frames_dir, prefixes[\"frames\"])\n+    elif frames_dir.exists():\n+        stats[\"frames\"] = storage.upload_dir(frames_dir, prefixes[\"frames\"])\n+    if exporter and exporter.save_crops and exporter.crops_dir.exists():\n+        stats[\"crops\"] = storage.upload_dir(exporter.crops_dir, prefixes[\"crops\"])\n+    elif crops_dir.exists():\n+        stats[\"crops\"] = storage.upload_dir(crops_dir, prefixes[\"crops\"])\n+    if thumb_dir is not None and thumb_dir.exists():\n+        identities_dir = thumb_dir / \"identities\"\n+        stats[\"thumbs_tracks\"] = storage.upload_dir(\n+            thumb_dir,\n+            prefixes.get(\"thumbs_tracks\", \"\"),\n+            skip_subdirs=(\"identities\",),\n+        )\n+        if identities_dir.exists():\n+            stats[\"thumbs_identities\"] = storage.upload_dir(\n+                identities_dir,\n+                prefixes.get(\"thumbs_identities\", \"\"),\n+            )\n+    return stats\n+\n+\n+def _report_s3_upload(\n+    progress: ProgressEmitter | None,\n+    stats: Dict[str, int],\n+    *,\n+    device: str | None,\n+    detector: str | None,\n+    tracker: str | None,\n+    resolved_device: str | None,\n+) -> None:\n+    if not progress:\n+        return\n+    if not any(stats.values()):\n+        return\n+    frames = progress.target_frames or 0\n+    progress.emit(\n+        frames,\n+        phase=\"mirror_s3\",\n+        device=device,\n+        summary={\"s3_uploads\": stats},\n+        detector=detector,\n+        tracker=tracker,\n+        resolved_device=resolved_device,\n+        force=True,\n+    )\n+\n+\n+def parse_args(argv: Iterable[str] | None = None) -> argparse.Namespace:\n+    parser = argparse.ArgumentParser(description=\"Run detection + tracking locally.\")\n+    parser.add_argument(\"--ep-id\", required=True, help=\"Episode identifier\")\n+    parser.add_argument(\"--video\", help=\"Path to source video (required for detect/track runs)\")\n+    parser.add_argument(\"--stride\", type=int, default=5, help=\"Frame stride for detection sampling\")\n+    parser.add_argument(\n+        \"--fps\",\n+        type=float,\n+        help=\"Optional target FPS for downsampling before detection\",\n+    )\n+    parser.add_argument(\n+        \"--device\",\n+        choices=[\"auto\", \"cpu\", \"mps\", \"cuda\"],\n+        default=\"auto\",\n+        help=\"Execution device override (auto→CUDA/MPS/CPU)\",\n+    )\n+    parser.add_argument(\n+        \"--detector\",\n+        choices=list(DETECTOR_CHOICES),\n+        default=DEFAULT_DETECTOR,\n+        help=\"Face detector backend (RetinaFace high quality, YOLOv8-face fast)\",\n+    )\n+    parser.add_argument(\n+        \"--tracker\",\n+        choices=list(TRACKER_CHOICES),\n+        default=DEFAULT_TRACKER,\n+        help=\"Tracker backend (ByteTrack default, StrongSORT optional for occlusions)\",\n+    )\n+    parser.add_argument(\n+        \"--scene-detect\",\n+        choices=[\"on\", \"off\"],\n+        default=\"on\" if SCENE_DETECT_DEFAULT else \"off\",\n+        help=\"Enable histogram-based scene cut detection\",\n+    )\n+    parser.add_argument(\n+        \"--scene-threshold\",\n+        type=float,\n+        default=SCENE_THRESHOLD_DEFAULT,\n+        help=\"Scene-cut threshold (1 - HSV histogram correlation, 0-2 range)\",\n+    )\n+    parser.add_argument(\n+        \"--scene-min-len\",\n+        type=int,\n+        default=SCENE_MIN_LEN_DEFAULT,\n+        help=\"Minimum frames between scene cuts\",\n+    )\n+    parser.add_argument(\n+        \"--scene-warmup-dets\",\n+        type=int,\n+        default=SCENE_WARMUP_DETS_DEFAULT,\n+        help=\"Frames of forced detection after each cut\",\n+    )\n+    parser.add_argument(\n+        \"--det-thresh\",\n+        type=float,\n+        default=RETINAFACE_SCORE_THRESHOLD,\n+        help=\"RetinaFace detection score threshold (0-1, default 0.5)\",\n+    )\n+    parser.add_argument(\n+        \"--max-gap\",\n+        type=int,\n+        default=30,\n+        help=\"Maximum frame gap before splitting a track\",\n+    )\n+    parser.add_argument(\n+        \"--track-sample-limit\",\n+        type=int,\n+        default=None,\n+        help=\"Optional max samples stored per track (0→all detections, default)\",\n+    )\n+    parser.add_argument(\"--thumb-size\", type=int, default=256, help=\"Square thumbnail size for faces\")\n+    parser.add_argument(\n+        \"--out-root\",\n+        help=\"Data root override (defaults to SCREENALYTICS_DATA_ROOT or ./data)\",\n+    )\n+    parser.add_argument(\"--progress-file\", help=\"Progress JSON file to update during processing\")\n+    parser.add_argument(\"--save-frames\", action=\"store_true\", help=\"Save sampled frame JPGs under data/frames/{ep_id}\")\n+    parser.add_argument(\"--save-crops\", action=\"store_true\", help=\"Save per-track crops (requires --save-frames or track IDs)\")\n+    parser.add_argument(\"--jpeg-quality\", type=int, default=85, help=\"JPEG quality for frame exports (1-100)\")\n+    parser.add_argument(\"--faces-embed\", action=\"store_true\", help=\"Run faces embedding stage only\")\n+    parser.add_argument(\"--cluster\", action=\"store_true\", help=\"Run clustering stage only\")\n+    parser.add_argument(\n+        \"--cluster-thresh\",\n+        type=float,\n+        default=0.6,\n+        help=\"Agglomerative cosine distance threshold for clustering\",\n+    )\n+    parser.add_argument(\n+        \"--min-cluster-size\",\n+        type=int,\n+        default=2,\n+        help=\"Minimum tracks per identity before splitting into singletons\",\n+    )\n+    return parser.parse_args(argv)\n+\n+\n+def main(argv: Iterable[str] | None = None) -> int:\n+    args = parse_args(argv)\n+    if hasattr(args, \"det_thresh\"):\n+        args.det_thresh = _normalize_det_thresh(getattr(args, \"det_thresh\", RETINAFACE_SCORE_THRESHOLD))\n+    scene_flag = str(getattr(args, \"scene_detect\", \"on\")).strip().lower()\n+    args.scene_detect = scene_flag not in {\"0\", \"false\", \"off\", \"no\"}\n+    args.scene_threshold = max(min(float(getattr(args, \"scene_threshold\", SCENE_THRESHOLD_DEFAULT)), 2.0), 0.0)\n+    args.scene_min_len = max(int(getattr(args, \"scene_min_len\", SCENE_MIN_LEN_DEFAULT)), 1)\n+    args.scene_warmup_dets = max(int(getattr(args, \"scene_warmup_dets\", SCENE_WARMUP_DETS_DEFAULT)), 0)\n+    cli_track_limit = getattr(args, \"track_sample_limit\", None)\n+    if cli_track_limit is not None:\n+        _set_track_sample_limit(cli_track_limit)\n+        if TRACK_SAMPLE_LIMIT is None:\n+            LOGGER.info(\"Track sampling disabled; persisting all detections per track.\")\n+        else:\n+            LOGGER.info(\"Track sampling limited to the first %s detections per track.\", TRACK_SAMPLE_LIMIT)\n+    data_root = (\n+        Path(args.out_root).expanduser()\n+        if args.out_root\n+        else Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+    )\n+    os.environ[\"SCREENALYTICS_DATA_ROOT\"] = str(data_root)\n+    ensure_dirs(args.ep_id)\n+    storage, ep_ctx, s3_prefixes = _storage_context(args.ep_id)\n+\n+    phase_flags = [flag for flag in (args.faces_embed, args.cluster) if flag]\n+    if len(phase_flags) > 1:\n+        raise ValueError(\"Specify at most one of --faces-embed/--cluster per run\")\n+\n+    if args.faces_embed:\n+        summary = _run_faces_embed_stage(args, storage, ep_ctx, s3_prefixes)\n+    elif args.cluster:\n+        summary = _run_cluster_stage(args, storage, ep_ctx, s3_prefixes)\n+    else:\n+        summary = _run_detect_track_stage(args, storage, ep_ctx, s3_prefixes)\n+\n+    stage = summary.get(\"stage\", \"detect_track\")\n+    device_label = summary.get(\"device\")\n+    analyzed_fps = summary.get(\"analyzed_fps\")\n+    log_msg = f\"stage={stage}\"\n+    if device_label:\n+        log_msg += f\" device={device_label}\"\n+    if analyzed_fps:\n+        log_msg += f\" analyzed_fps={analyzed_fps:.3f}\"\n+    print(f\"[episode_run] {log_msg}\", file=sys.stderr)\n+    print(\"[episode_run] summary\", summary, file=sys.stderr)\n+    return 0\n+\n+\n+def _effective_stride(stride: int, target_fps: float | None, source_fps: float) -> int:\n+    stride = max(stride, 1)\n+    if target_fps and target_fps > 0 and source_fps > 0:\n+        fps_stride = max(int(round(source_fps / target_fps)), 1)\n+        stride = max(stride, fps_stride)\n+    return stride\n+\n+\n+def _probe_video(video_path: Path) -> Tuple[float, int]:\n+    import cv2  # type: ignore\n+\n+    cap = cv2.VideoCapture(str(video_path))\n+    if not cap.isOpened():\n+        raise FileNotFoundError(f\"Unable to open video {video_path}\")\n+    fps = cap.get(cv2.CAP_PROP_FPS) or 0.0\n+    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n+    cap.release()\n+    if fps <= 0:\n+        fps = 30.0\n+    return fps, frame_count\n+\n+\n+def _detect_fps(video_path: Path) -> float:\n+    fps, _ = _probe_video(video_path)\n+    if fps <= 0:\n+        fps = 24.0\n+    return fps\n+\n+\n+def detect_scene_cuts(\n+    video_path: str | Path,\n+    *,\n+    thr: float = SCENE_THRESHOLD_DEFAULT,\n+    min_len: int = SCENE_MIN_LEN_DEFAULT,\n+    progress: ProgressEmitter | None = None,\n+) -> list[int]:\n+    \"\"\"Lightweight HSV histogram scene-cut detector.\"\"\"\n+\n+    import cv2  # type: ignore\n+\n+    threshold = max(min(float(thr or SCENE_THRESHOLD_DEFAULT), 2.0), 0.0)\n+    min_gap = max(int(min_len or SCENE_MIN_LEN_DEFAULT), 1)\n+    cap = cv2.VideoCapture(str(video_path))\n+    if not cap.isOpened():\n+        return []\n+    cuts: list[int] = []\n+    prev_hist = None\n+    last_cut = -10**9\n+    idx = 0\n+    if progress:\n+        progress.emit(\n+            0,\n+            phase=\"scene_detect:cut\",\n+            summary={\"threshold\": round(float(threshold), 3), \"min_len\": min_gap},\n+            extra=_non_video_phase_meta(\"start\"),\n+            force=True,\n+        )\n+    target_frames = progress.target_frames if progress else 0\n+    emit_interval = 50  # Emit every 50 frames to reduce spam\n+    while True:\n+        ok, frame = cap.read()\n+        if not ok:\n+            break\n+        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n+        hist = cv2.calcHist([hsv], [0, 1], None, [32, 32], [0, 180, 0, 256])\n+        hist = cv2.normalize(hist, None).flatten()\n+        if prev_hist is not None:\n+            diff = 1.0 - float(cv2.compareHist(prev_hist, hist, cv2.HISTCMP_CORREL))\n+            if diff > threshold and (idx - last_cut) >= min_gap:\n+                cuts.append(idx)\n+                last_cut = idx\n+        # Emit sparse updates (every 50 frames) instead of on every cut\n+        if progress and (idx % emit_interval == 0 or idx == target_frames - 1):\n+            frames_done = idx if idx >= 0 else 0\n+            if target_frames:\n+                frames_done = min(target_frames, frames_done)\n+            progress.emit(\n+                frames_done,\n+                phase=\"scene_detect:cut\",\n+                summary={\"count\": len(cuts)},\n+                extra=_non_video_phase_meta(),\n+            )\n+        prev_hist = hist\n+        idx += 1\n+    cap.release()\n+    if progress:\n+        final_frames = target_frames or idx\n+        progress.emit(\n+            final_frames,\n+            phase=\"scene_detect:cut\",\n+            summary={\"cuts\": len(cuts)},\n+            force=True,\n+            extra=_non_video_phase_meta(\"done\"),\n+        )\n+    return cuts\n+\n+\n+def _run_full_pipeline(\n+    args: argparse.Namespace,\n+    video_dest: Path,\n+    *,\n+    source_fps: float,\n+    progress: ProgressEmitter | None = None,\n+    target_fps: float | None = None,\n+    frame_exporter: FrameExporter | None = None,\n+    total_frames: int | None = None,\n+    video_fps: float | None = None,\n+) -> Tuple[int, int, int, str, str, float | None, Dict[str, Any], Dict[str, Any]]:\n+    import cv2  # type: ignore\n+\n+    analyzed_fps = target_fps or source_fps\n+    if not analyzed_fps or analyzed_fps <= 0:\n+        analyzed_fps = _detect_fps(video_dest)\n+    frame_stride = _effective_stride(args.stride, target_fps or analyzed_fps, source_fps)\n+    ts_fps = analyzed_fps if analyzed_fps and analyzed_fps > 0 else max(args.fps or 30.0, 1.0)\n+    frames_goal = None\n+    if total_frames and total_frames > 0:\n+        frames_goal = int(total_frames)\n+    elif progress and progress.target_frames:\n+        frames_goal = progress.target_frames\n+    video_clock_fps = video_fps if video_fps and video_fps > 0 else (source_fps if source_fps > 0 else None)\n+\n+    def _progress_value(frame_index: int, *, include_current: bool = False, step: str | None = None) -> tuple[int, Dict[str, Any]]:\n+        base = frame_index + (1 if include_current else 0)\n+        if base < 0:\n+            base = 0\n+        total = frames_goal or base\n+        value = base\n+        if frames_goal:\n+            value = min(frames_goal, base)\n+        meta = _video_phase_meta(value, total if total > 0 else None, video_clock_fps, step=step)\n+        return value, meta\n+    device = pick_device(args.device)\n+    detector_choice = _normalize_detector_choice(getattr(args, \"detector\", None))\n+    tracker_choice = _normalize_tracker_choice(getattr(args, \"tracker\", None))\n+    tracker_choice = _normalize_tracker_choice(getattr(args, \"tracker\", None))","path":"tools/episode_run.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"This assignment to 'tracker_choice' is unnecessary as it is [redefined](1) before this value is used.","created_at":"2025-11-11T20:00:04Z","updated_at":"2025-11-11T20:00:21Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565101","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565101"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565101"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565101/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":1922,"original_line":1922,"side":"RIGHT","author_association":"NONE","original_position":1922,"position":1922,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565108","pull_request_review_id":3449784939,"id":2515565108,"node_id":"PRRC_kwDOQRcQs86V8Ho0","diff_hunk":"@@ -0,0 +1,76 @@\n+from __future__ import annotations\n+\n+import json\n+import sys\n+from pathlib import Path\n+\n+import requests\n+import streamlit as st\n+\n+PAGE_PATH = Path(__file__).resolve()\n+WORKSPACE_DIR = PAGE_PATH.parents[1]\n+if str(WORKSPACE_DIR) not in sys.path:\n+    sys.path.append(str(WORKSPACE_DIR))\n+\n+import ui_helpers as helpers  # noqa: E402\n+\n+cfg = helpers.init_page(\"Screentime\")\n+st.title(\"Screentime\")\n+\n+\n+def _require_episode() -> str:","path":"apps/workspace-ui/pages/4_Screentime.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Mixing implicit and explicit returns may indicate an error, as implicit returns always return None.","created_at":"2025-11-11T20:00:04Z","updated_at":"2025-11-11T20:00:21Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565108","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565108"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565108"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565108/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":21,"original_line":21,"side":"RIGHT","author_association":"NONE","original_position":21,"position":21,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565116","pull_request_review_id":3449784939,"id":2515565116,"node_id":"PRRC_kwDOQRcQs86V8Ho8","diff_hunk":"@@ -0,0 +1,61 @@\n+from __future__ import annotations\n+\n+from fastapi.testclient import TestClient\n+\n+from apps.api.main import app\n+from apps.api.routers import episodes as episodes_router\n+\n+\n+class _MockStorage:\n+    bucket = \"screenalytics\"\n+\n+    def __init__(self) -> None:\n+        self.mirror_calls = []\n+\n+    def video_object_key_v2(self, show_slug: str, season: int, episode: int) -> str:\n+        return f\"raw/videos/{show_slug}/s{season:02d}/e{episode:02d}/episode.mp4\"\n+\n+    def video_object_key_v1(self, ep_id: str) -> str:\n+        return f\"raw/videos/{ep_id}/episode.mp4\"\n+\n+    def object_exists(self, key: str) -> bool:\n+        return key.endswith(\"legacy/episode.mp4\")\n+\n+    def presign_episode_video(self, *_, **__):\n+        raise NotImplementedError\n+\n+    def ensure_local_mirror(self, ep_id: str, **kwargs):\n+        self.mirror_calls.append((ep_id, kwargs))\n+        return {\n+            \"local_video_path\": f\"/tmp/{ep_id}.mp4\",\n+            \"bytes\": 100,\n+            \"etag\": \"etag\",\n+            \"used_key_version\": \"v1\",\n+        }\n+\n+\n+def test_mirror_falls_back_to_v1(monkeypatch, tmp_path):\n+    monkeypatch.setenv(\"SCREENALYTICS_DATA_ROOT\", str(tmp_path / \"data\"))\n+    client = TestClient(app)\n+\n+    create_resp = client.post(\n+        \"/episodes\",\n+        json={\n+            \"show_slug_or_id\": \"legacy\",\n+            \"season_number\": 1,\n+            \"episode_number\": 1,\n+        },\n+    )\n+    ep_id = create_resp.json()[\"ep_id\"]\n+\n+    mock_storage = _MockStorage()\n+    monkeypatch.setattr(episodes_router, \"STORAGE\", mock_storage)\n+\n+    resp = client.post(f\"/episodes/{ep_id}/mirror\")\n+    assert resp.status_code == 200\n+    data = resp.json()\n+    assert data[\"used_key_version\"] == \"v1\"\n+    assert mock_storage.mirror_calls\n+    _, kwargs = mock_storage.mirror_calls[0]\n+    assert kwargs[\"show_ref\"] == \"legacy\"\n+*** End Patch","path":"tests/api/test_mirror_v1_v2_fallback.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Syntax Error (in Python 3).","created_at":"2025-11-11T20:00:04Z","updated_at":"2025-11-11T20:00:21Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565116","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565116"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565116"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565116/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":61,"original_line":61,"side":"RIGHT","author_association":"NONE","original_position":61,"position":61,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565125","pull_request_review_id":3449784939,"id":2515565125,"node_id":"PRRC_kwDOQRcQs86V8HpF","diff_hunk":"@@ -0,0 +1,463 @@\n+from __future__ import annotations\n+\n+import sys\n+import time","path":"apps/workspace-ui/pages/2_Episode_Detail.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Import of 'time' is not used.","created_at":"2025-11-11T20:00:05Z","updated_at":"2025-11-11T20:00:21Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565125","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565125"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565125"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565125/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":4,"original_line":4,"side":"RIGHT","author_association":"NONE","original_position":4,"position":4,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565135","pull_request_review_id":3449784939,"id":2515565135,"node_id":"PRRC_kwDOQRcQs86V8HpP","diff_hunk":"@@ -0,0 +1,27 @@\n+\"\"\"Runtime bridge to packages/py-screenalytics/artifacts.py.\"\"\"\n+\n+from __future__ import annotations\n+\n+import importlib.util\n+from pathlib import Path\n+from types import ModuleType","path":"py_screenalytics/artifacts.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Import of 'ModuleType' is not used.","created_at":"2025-11-11T20:00:05Z","updated_at":"2025-11-11T20:00:21Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565135","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565135"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565135"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565135/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":7,"original_line":7,"side":"RIGHT","author_association":"NONE","original_position":7,"position":7,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565151","pull_request_review_id":3449784939,"id":2515565151,"node_id":"PRRC_kwDOQRcQs86V8Hpf","diff_hunk":"@@ -0,0 +1,27 @@\n+\"\"\"Runtime bridge to packages/py-screenalytics/artifacts.py.\"\"\"\n+\n+from __future__ import annotations\n+\n+import importlib.util\n+from pathlib import Path\n+from types import ModuleType\n+from typing import Any","path":"py_screenalytics/artifacts.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Import of 'Any' is not used.","created_at":"2025-11-11T20:00:05Z","updated_at":"2025-11-11T20:00:21Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565151","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565151"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565151"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565151/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":8,"original_line":8,"side":"RIGHT","author_association":"NONE","original_position":8,"position":8,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565161","pull_request_review_id":3449784939,"id":2515565161,"node_id":"PRRC_kwDOQRcQs86V8Hpp","diff_hunk":"@@ -1,69 +1,253 @@\n-\"\"\"Tracking scaffolding for ByteTrack stage.\"\"\"\n+\"\"\"ByteTrack-inspired tracker that consumes det_v1 JSONL records.\n+\n+The implementation favors deterministic behaviour suitable for CI:\n+\n+* Detections are filtered by `track_thresh`.\n+* Tracks are matched via IoU with a configurable `match_thresh`.\n+* Tracks remain active for `track_buffer` frames of inactivity.\n+* Output adheres to `track_v1` (DATA_SCHEMA.md) and adds basic stats.\n+\"\"\"\n \n from __future__ import annotations\n \n import argparse\n import json\n+from dataclasses import dataclass, field","path":"FEATURES/tracking/src/bytetrack_runner.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Import of 'field' is not used.\n```suggestion\nfrom dataclasses import dataclass\n```","created_at":"2025-11-11T20:00:06Z","updated_at":"2025-11-11T20:00:21Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565161","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565161"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565161"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565161/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":15,"original_line":15,"side":"RIGHT","author_association":"NONE","original_position":16,"position":16,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565191","pull_request_review_id":3449784939,"id":2515565191,"node_id":"PRRC_kwDOQRcQs86V8HqH","diff_hunk":"@@ -0,0 +1,178 @@\n+\"\"\"Cast management service for show/season cast members.\"\"\"\n+\n+from __future__ import annotations\n+\n+import json\n+import uuid\n+from datetime import datetime\n+from enum import Enum\n+from pathlib import Path\n+from typing import Any, Dict, List, Optional\n+\n+import numpy as np","path":"apps/api/services/cast.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Import of 'np' is not used.","created_at":"2025-11-11T20:00:06Z","updated_at":"2025-11-11T20:00:21Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565191","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565191"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565191"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565191/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":12,"original_line":12,"side":"RIGHT","author_association":"NONE","original_position":12,"position":12,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565209","pull_request_review_id":3449784939,"id":2515565209,"node_id":"PRRC_kwDOQRcQs86V8HqZ","diff_hunk":"@@ -0,0 +1,3034 @@\n+#!/usr/bin/env python\n+\"\"\"Dev-only CLI to run detection → tracking for a single episode.\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import math\n+import os\n+import shutil\n+import sys\n+import time\n+from collections import Counter, defaultdict\n+from dataclasses import dataclass, field\n+from datetime import datetime\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, Iterator, List, Optional, Tuple, Callable","path":"tools/episode_run.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Import of 'Callable' is not used.","created_at":"2025-11-11T20:00:06Z","updated_at":"2025-11-11T20:00:21Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565209","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565209"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565209"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565209/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":17,"original_line":17,"side":"RIGHT","author_association":"NONE","original_position":17,"position":17,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565216","pull_request_review_id":3449784939,"id":2515565216,"node_id":"PRRC_kwDOQRcQs86V8Hqg","diff_hunk":"@@ -0,0 +1,3034 @@\n+#!/usr/bin/env python\n+\"\"\"Dev-only CLI to run detection → tracking for a single episode.\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import math\n+import os\n+import shutil\n+import sys\n+import time\n+from collections import Counter, defaultdict\n+from dataclasses import dataclass, field\n+from datetime import datetime\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, Iterator, List, Optional, Tuple, Callable\n+\n+import logging\n+from functools import lru_cache","path":"tools/episode_run.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Import of 'lru_cache' is not used.","created_at":"2025-11-11T20:00:07Z","updated_at":"2025-11-11T20:00:21Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565216","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565216"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565216"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565216/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":20,"original_line":20,"side":"RIGHT","author_association":"NONE","original_position":20,"position":20,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565231","pull_request_review_id":3449784939,"id":2515565231,"node_id":"PRRC_kwDOQRcQs86V8Hqv","diff_hunk":"@@ -0,0 +1,3034 @@\n+#!/usr/bin/env python\n+\"\"\"Dev-only CLI to run detection → tracking for a single episode.\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import math\n+import os\n+import shutil\n+import sys\n+import time\n+from collections import Counter, defaultdict\n+from dataclasses import dataclass, field\n+from datetime import datetime\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, Iterator, List, Optional, Tuple, Callable\n+\n+import logging\n+from functools import lru_cache\n+\n+import numpy as np\n+\n+REPO_ROOT = Path(__file__).resolve().parents[1]\n+if str(REPO_ROOT) not in sys.path:\n+    sys.path.insert(0, str(REPO_ROOT))\n+\n+from apps.api.services.storage import (\n+    EpisodeContext,\n+    StorageService,\n+    artifact_prefixes,\n+    episode_context_from_id,\n+)\n+from py_screenalytics.artifacts import ensure_dirs, get_path\n+from tools._img_utils import clip_bbox, safe_crop, safe_imwrite, to_u8_bgr","path":"tools/episode_run.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Import of 'clip_bbox' is not used.","created_at":"2025-11-11T20:00:07Z","updated_at":"2025-11-11T20:00:21Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565231","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565231"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565231"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565231/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":35,"original_line":35,"side":"RIGHT","author_association":"NONE","original_position":35,"position":35,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565242","pull_request_review_id":3449784939,"id":2515565242,"node_id":"PRRC_kwDOQRcQs86V8Hq6","diff_hunk":"@@ -0,0 +1,433 @@\n+\"\"\"Cluster grouping service for within-episode and across-episode person matching.\"\"\"\n+\n+from __future__ import annotations\n+\n+import json\n+import os\n+from datetime import datetime\n+from pathlib import Path\n+from typing import Any, Dict, List, Optional, Tuple","path":"apps/api/services/grouping.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Import of 'Tuple' is not used.","created_at":"2025-11-11T20:00:07Z","updated_at":"2025-11-11T20:00:21Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565242","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565242"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565242"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565242/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":9,"original_line":9,"side":"RIGHT","author_association":"NONE","original_position":9,"position":9,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565257","pull_request_review_id":3449784939,"id":2515565257,"node_id":"PRRC_kwDOQRcQs86V8HrJ","diff_hunk":"@@ -0,0 +1,219 @@\n+\"\"\"People management service for show-level person entities.\"\"\"\n+\n+from __future__ import annotations\n+\n+import json\n+import uuid","path":"apps/api/services/people.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Import of 'uuid' is not used.","created_at":"2025-11-11T20:00:08Z","updated_at":"2025-11-11T20:00:21Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565257","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565257"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565257"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565257/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":6,"original_line":6,"side":"RIGHT","author_association":"NONE","original_position":6,"position":6,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565269","pull_request_review_id":3449784939,"id":2515565269,"node_id":"PRRC_kwDOQRcQs86V8HrV","diff_hunk":"@@ -0,0 +1,91 @@\n+\"\"\"People management endpoints for show-level person entities.\"\"\"\n+\n+from __future__ import annotations\n+\n+import sys\n+from pathlib import Path\n+from typing import List, Optional\n+\n+from fastapi import APIRouter, HTTPException\n+from pydantic import BaseModel, Field","path":"apps/api/routers/people.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Import of 'Field' is not used.","created_at":"2025-11-11T20:00:08Z","updated_at":"2025-11-11T20:00:21Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565269","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565269"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565269"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565269/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":10,"original_line":10,"side":"RIGHT","author_association":"NONE","original_position":10,"position":10,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565286","pull_request_review_id":3449784939,"id":2515565286,"node_id":"PRRC_kwDOQRcQs86V8Hrm","diff_hunk":"@@ -0,0 +1,69 @@\n+from __future__ import annotations\n+\n+import json\n+import shutil\n+\n+import pytest","path":"tests/api/test_episode_status.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Import of 'pytest' is not used.","created_at":"2025-11-11T20:00:08Z","updated_at":"2025-11-11T20:00:22Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565286","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565286"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565286"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565286/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":6,"original_line":6,"side":"RIGHT","author_association":"NONE","original_position":6,"position":6,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565300","pull_request_review_id":3449784939,"id":2515565300,"node_id":"PRRC_kwDOQRcQs86V8Hr0","diff_hunk":"@@ -0,0 +1,214 @@\n+import json\n+import os","path":"tests/api/test_frames_reassign.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Import of 'os' is not used.","created_at":"2025-11-11T20:00:09Z","updated_at":"2025-11-11T20:00:22Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565300","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565300"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565300"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565300/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":2,"original_line":2,"side":"RIGHT","author_association":"NONE","original_position":2,"position":2,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565311","pull_request_review_id":3449784939,"id":2515565311,"node_id":"PRRC_kwDOQRcQs86V8Hr_","diff_hunk":"@@ -0,0 +1,13 @@\n+from __future__ import annotations\n+\n+from pathlib import Path\n+\n+import pytest","path":"tests/ui/test_helpers_compile.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Import of 'pytest' is not used.","created_at":"2025-11-11T20:00:09Z","updated_at":"2025-11-11T20:00:22Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565311","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565311"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565311"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565311/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":5,"original_line":5,"side":"RIGHT","author_association":"NONE","original_position":5,"position":5,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565330","pull_request_review_id":3449784939,"id":2515565330,"node_id":"PRRC_kwDOQRcQs86V8HsS","diff_hunk":"@@ -0,0 +1,25 @@\n+from __future__ import annotations\n+\n+import sys\n+import types\n+\n+import pytest\n+\n+try:  # pragma: no cover - optional ML dependency\n+    import numpy  # noqa: F401","path":"tests/ml/test_retinaface_init_missing.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Import of 'numpy' is not used.","created_at":"2025-11-11T20:00:09Z","updated_at":"2025-11-11T20:00:22Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565330","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565330"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565330"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565330/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":9,"original_line":9,"side":"RIGHT","author_association":"NONE","original_position":9,"position":9,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565353","pull_request_review_id":3449784939,"id":2515565353,"node_id":"PRRC_kwDOQRcQs86V8Hsp","diff_hunk":"@@ -0,0 +1,68 @@\n+import json\n+import os","path":"tests/api/test_roster_endpoints.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Import of 'os' is not used.","created_at":"2025-11-11T20:00:10Z","updated_at":"2025-11-11T20:00:22Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565353","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565353"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565353"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565353/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":2,"original_line":2,"side":"RIGHT","author_association":"NONE","original_position":2,"position":2,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565376","pull_request_review_id":3449784939,"id":2515565376,"node_id":"PRRC_kwDOQRcQs86V8HtA","diff_hunk":"@@ -0,0 +1,44 @@\n+from __future__ import annotations\n+\n+import os","path":"tests/api/test_video_meta_fps.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Import of 'os' is not used.","created_at":"2025-11-11T20:00:10Z","updated_at":"2025-11-11T20:00:22Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565376","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565376"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565376"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565376/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":3,"original_line":3,"side":"RIGHT","author_association":"NONE","original_position":3,"position":3,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565401","pull_request_review_id":3449784939,"id":2515565401,"node_id":"PRRC_kwDOQRcQs86V8HtZ","diff_hunk":"@@ -6,24 +6,40 @@\n SRC_DIR = REPO_ROOT / \"FEATURES\" / \"tracking\" / \"src\"\n sys.path.append(str(SRC_DIR))\n \n-from bytetrack_runner import build_tracks, main  # noqa: E402\n+from bytetrack_runner import build_tracks, load_config, main  # noqa: E402","path":"FEATURES/tracking/tests/test_tracking_io.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Import of 'load_config' is not used.\n```suggestion\nfrom bytetrack_runner import build_tracks, main  # noqa: E402\n```","created_at":"2025-11-11T20:00:10Z","updated_at":"2025-11-11T20:00:22Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565401","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565401"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565401"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565401/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":9,"original_line":9,"side":"RIGHT","author_association":"NONE","original_position":5,"position":5,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565427","pull_request_review_id":3449784939,"id":2515565427,"node_id":"PRRC_kwDOQRcQs86V8Htz","diff_hunk":"@@ -0,0 +1,899 @@\n+\"\"\"Object storage helpers for presigned uploads and artifact sync.\"\"\"\n+\n+from __future__ import annotations\n+\n+import json\n+import logging\n+import os\n+import re\n+import shutil\n+from dataclasses import dataclass\n+from mimetypes import guess_type\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, List, Optional\n+\n+from py_screenalytics.artifacts import get_path\n+\n+DEFAULT_BUCKET = \"screenalytics\"\n+DEFAULT_REGION = \"us-east-1\"\n+DEFAULT_EXPIRY = 900  # 15 minutes\n+LOCAL_UPLOAD_BASE = \"http://localhost/_local-storage\"\n+ARTIFACT_ROOT = \"artifacts\"\n+_V2_KEY_RE = re.compile(\n+    r\"raw/videos/(?P<show>[^/]+)/s(?P<season>\\d{2})/e(?P<episode>\\d{2})/episode\\.mp4\"\n+)\n+_V1_KEY_RE = re.compile(r\"raw/videos/(?P<ep_id>[^/]+)/episode\\.mp4\")\n+_EP_ID_REGEX = re.compile(r\"^(?P<show>.+)-s(?P<season>\\d{2})e(?P<episode>\\d{2})$\", re.IGNORECASE)\n+_FRAME_NAME_RE = re.compile(r\"frame_(\\d{6})\\.jpg$\", re.IGNORECASE)\n+_CURSOR_SEP = \"|\"\n+LOGGER = logging.getLogger(__name__)\n+\n+\n+def _frame_idx_from_key(key: str) -> int | None:\n+    match = _FRAME_NAME_RE.search(key)\n+    if not match:\n+        return None\n+    try:\n+        return int(match.group(1))\n+    except ValueError:\n+        return None\n+\n+\n+def _split_cursor(raw: str | None) -> tuple[str | None, int]:\n+    if not raw:\n+        return None, 0\n+    value = raw.strip()\n+    if not value:\n+        return None, 0\n+    if _CURSOR_SEP in value:\n+        key, _, remainder = value.partition(_CURSOR_SEP)\n+        try:\n+            cycle = int(remainder)\n+        except ValueError:\n+            cycle = 0\n+        return key or None, max(cycle, 0)\n+    return value, 0\n+\n+\n+def _encode_cursor(key: str, cycle: int) -> str:\n+    cycle = max(int(cycle), 0)\n+    return f\"{key}{_CURSOR_SEP}{cycle}\" if cycle else key\n+\n+\n+def _normalize_cursor_key(track_prefix: str, raw_key: str | None) -> str | None:\n+    if not raw_key:\n+        return None\n+    candidate = raw_key.strip()\n+    if not candidate:\n+        return None\n+    candidate = candidate.replace(\"\\\\\", \"/\")\n+    marker = track_prefix\n+    idx = candidate.find(marker)\n+    if idx >= 0:\n+        candidate = candidate[idx:]\n+    if candidate.startswith(marker):\n+        return candidate\n+    leaf = candidate.split(\"/\")[-1]\n+    if not leaf:\n+        return None\n+    return f\"{marker}{leaf}\"\n+\n+\n+def _boto3():\n+    try:\n+        import boto3  # type: ignore\n+        return boto3\n+    except ImportError as exc:  # pragma: no cover - only triggered in misconfig\n+        raise RuntimeError(\n+            \"boto3 is required when STORAGE_BACKEND is 's3' or 'minio'\"\n+        ) from exc\n+\n+\n+@dataclass(frozen=True)\n+class EpisodeContext:\n+    ep_id: str\n+    show_slug: str\n+    season_number: int\n+    episode_number: int\n+\n+\n+@dataclass\n+class PresignedUpload:\n+    ep_id: str\n+    bucket: str\n+    object_key: str\n+    upload_url: str | None\n+    expires_in: int | None\n+    headers: Dict[str, str]\n+    method: str\n+    path: str | None = None\n+    backend: str = \"s3\"\n+\n+\n+def episode_context_from_id(ep_id: str) -> EpisodeContext:\n+    match = _EP_ID_REGEX.match(ep_id)\n+    if not match:\n+        raise ValueError(f\"Unable to parse episode id '{ep_id}' into show/season/episode\")\n+    show = match.group(\"show\")\n+    season = int(match.group(\"season\"))\n+    episode = int(match.group(\"episode\"))\n+    return EpisodeContext(ep_id=ep_id, show_slug=show, season_number=season, episode_number=episode)\n+\n+\n+def artifact_prefixes(ep_ctx: EpisodeContext) -> Dict[str, str]:\n+    \"\"\"Return v2 S3 prefixes for frames/crops/manifests under single bucket.\"\"\"\n+\n+    show = ep_ctx.show_slug\n+    season = ep_ctx.season_number\n+    episode = ep_ctx.episode_number\n+    return {\n+        \"frames\": f\"{ARTIFACT_ROOT}/frames/{show}/s{season:02d}/e{episode:02d}/frames/\",\n+        \"crops\": f\"{ARTIFACT_ROOT}/crops/{show}/s{season:02d}/e{episode:02d}/tracks/\",\n+        \"manifests\": f\"{ARTIFACT_ROOT}/manifests/{show}/s{season:02d}/e{episode:02d}/\",\n+        \"thumbs_tracks\": f\"{ARTIFACT_ROOT}/thumbs/{show}/s{season:02d}/e{episode:02d}/tracks/\",\n+        \"thumbs_identities\": f\"{ARTIFACT_ROOT}/thumbs/{show}/s{season:02d}/e{episode:02d}/identities/\",\n+    }\n+\n+\n+def parse_v2_episode_key(key: str) -> Dict[str, object] | None:\n+    \"\"\"Parse a v2 episode key (raw/videos/{show}/s{ss}/e{ee}/episode.mp4).\"\"\"\n+\n+    match = _V2_KEY_RE.search(key)\n+    if not match:\n+        return None\n+    show = match.group(\"show\")\n+    season = int(match.group(\"season\"))\n+    episode = int(match.group(\"episode\"))\n+    ep_id = f\"{show.lower()}-s{season:02d}e{episode:02d}\"\n+    return {\n+        \"ep_id\": ep_id,\n+        \"show\": show,\n+        \"show_slug\": show,\n+        \"season\": season,\n+        \"episode\": episode,\n+        \"key_version\": \"v2\",\n+    }\n+\n+\n+class StorageService:\n+    \"\"\"Lightweight S3/MinIO helper that only presigns uploads.\"\"\"\n+\n+    def __init__(self) -> None:\n+        self.backend = os.environ.get(\"STORAGE_BACKEND\", \"s3\").lower()\n+        self.region = os.environ.get(\"AWS_DEFAULT_REGION\", DEFAULT_REGION)\n+        self.prefix = os.environ.get(\"AWS_S3_PREFIX\", \"raw/\")\n+        if self.prefix and not self.prefix.endswith(\"/\"):\n+            self.prefix += \"/\"\n+        auto_create = os.environ.get(\"S3_AUTO_CREATE\", \"0\")\n+        self.auto_create = auto_create.lower() in {\"1\", \"true\", \"yes\"}\n+        self.bucket = DEFAULT_BUCKET\n+        self._client = None\n+        self._client_error_cls = None\n+        self.write_enabled = True\n+\n+        if self.backend == \"s3\":\n+            boto3_mod = _boto3()\n+            from botocore.exceptions import ClientError  # type: ignore\n+\n+            client_kwargs: Dict[str, object] = {\"region_name\": self.region}\n+            custom_endpoint = os.environ.get(\"SCREENALYTICS_OBJECT_STORE_ENDPOINT\")\n+            if custom_endpoint:\n+                client_kwargs[\"endpoint_url\"] = custom_endpoint\n+            self._client = boto3_mod.client(\"s3\", **client_kwargs)\n+            configured_bucket = os.environ.get(\"SCREENALYTICS_S3_BUCKET\") or os.environ.get(\"AWS_S3_BUCKET\")\n+            self.bucket = configured_bucket or DEFAULT_BUCKET\n+            self._client_error_cls = ClientError\n+            self._ensure_s3_bucket(ClientError)\n+        elif self.backend == \"minio\":\n+            boto3_mod = _boto3()\n+            from botocore.client import Config  # type: ignore\n+            from botocore.exceptions import ClientError  # type: ignore\n+\n+            endpoint = os.environ.get(\"SCREENALYTICS_OBJECT_STORE_ENDPOINT\", \"http://localhost:9000\")\n+            access_key = os.environ.get(\"SCREENALYTICS_OBJECT_STORE_ACCESS_KEY\", \"minio\")\n+            secret_key = os.environ.get(\"SCREENALYTICS_OBJECT_STORE_SECRET_KEY\", \"miniosecret\")\n+            signature_version = os.environ.get(\"SCREENALYTICS_OBJECT_STORE_SIGNATURE\", \"s3v4\")\n+            minio_region = os.environ.get(\"SCREENALYTICS_OBJECT_STORE_REGION\", DEFAULT_REGION)\n+            self.bucket = os.environ.get(\"SCREENALYTICS_OBJECT_STORE_BUCKET\", DEFAULT_BUCKET)\n+            self._client = boto3_mod.client(\n+                \"s3\",\n+                endpoint_url=endpoint,\n+                region_name=minio_region,\n+                aws_access_key_id=access_key,\n+                aws_secret_access_key=secret_key,\n+                config=Config(signature_version=signature_version),\n+            )\n+            self._client_error_cls = ClientError\n+        elif self.backend == \"local\":\n+            self.bucket = \"local\"\n+        else:\n+            raise ValueError(f\"Unsupported STORAGE_BACKEND '{self.backend}'\")\n+\n+        flag = os.environ.get(\"S3_WRITE\")\n+        default_enabled = self.backend in {\"s3\", \"minio\"} and self._client is not None\n+        if flag is not None:\n+            default_enabled = flag.lower() in {\"1\", \"true\", \"yes\"}\n+        self.write_enabled = default_enabled and self.backend in {\"s3\", \"minio\"} and self._client is not None\n+\n+    def s3_enabled(self) -> bool:\n+        return self.backend in {\"s3\", \"minio\"} and self._client is not None\n+\n+    def presign_episode_video(\n+        self,\n+        ep_id: str,\n+        *,\n+        object_key: str,\n+        content_type: str = \"video/mp4\",\n+        expires_in: int = DEFAULT_EXPIRY,\n+    ) -> PresignedUpload:\n+        headers = {\"Content-Type\": content_type}\n+\n+        if self.backend == \"local\":\n+            upload_url = f\"{LOCAL_UPLOAD_BASE}/{object_key}\"\n+            method = \"FILE\"\n+            path = object_key\n+        else:\n+            assert self._client is not None  # for mypy\n+            params = {\"Bucket\": self.bucket, \"Key\": object_key, \"ContentType\": content_type}\n+            upload_url = self._client.generate_presigned_url(\n+                \"put_object\",\n+                Params=params,\n+                ExpiresIn=expires_in,\n+                HttpMethod=\"PUT\",\n+            )\n+            method = \"PUT\"\n+            path = None\n+\n+        return PresignedUpload(\n+            ep_id=ep_id,\n+            bucket=self.bucket,\n+            object_key=object_key,\n+            upload_url=upload_url,\n+            expires_in=expires_in,\n+            headers=headers,\n+            method=method,\n+            path=path,\n+            backend=self.backend,\n+        )\n+\n+    def presign_get(self, key: str, expires_in: int = 3600) -> str | None:\n+        if self.backend not in {\"s3\", \"minio\"} or self._client is None:\n+            return None\n+        params = {\"Bucket\": self.bucket, \"Key\": key}\n+        return self._client.generate_presigned_url(  # type: ignore[union-attr]\n+            \"get_object\",\n+            Params=params,\n+            ExpiresIn=expires_in,\n+        )\n+\n+    def ensure_local_mirror(\n+        self,\n+        ep_id: str,\n+        *,\n+        show_ref: str | None = None,\n+        season_number: int | None = None,\n+        episode_number: int | None = None,\n+    ) -> Dict[str, Optional[object]]:\n+        local_path = get_path(ep_id, \"video\")\n+        local_path.parent.mkdir(parents=True, exist_ok=True)\n+        info: Dict[str, Optional[object]] = {\n+            \"local_video_path\": str(local_path),\n+            \"bytes\": local_path.stat().st_size if local_path.exists() else None,\n+            \"etag\": None,\n+            \"used_key_version\": None,\n+        }\n+        if self.backend == \"local\":\n+            return info\n+        if self.backend not in {\"s3\", \"minio\"} or self._client is None or self._client_error_cls is None:\n+            return info\n+        keys_to_try: List[tuple[str, str]] = []\n+        if show_ref is not None and season_number is not None and episode_number is not None:\n+            keys_to_try.append((\"v2\", self.video_object_key_v2(show_ref, season_number, episode_number)))\n+        keys_to_try.append((\"v1\", self.video_object_key_v1(ep_id)))\n+\n+        for version, key in keys_to_try:\n+            try:\n+                head = self._client.head_object(Bucket=self.bucket, Key=key)\n+                etag = head.get(\"ETag\")\n+                info[\"etag\"] = etag.strip('\"') if isinstance(etag, str) else etag\n+                info[\"bytes\"] = head.get(\"ContentLength\")\n+                if not local_path.exists():\n+                    self._client.download_file(self.bucket, key, str(local_path))\n+                    info[\"bytes\"] = local_path.stat().st_size\n+                info[\"used_key_version\"] = version\n+                return info\n+            except self._client_error_cls as exc:  # type: ignore[misc]\n+                error_code = exc.response.get(\"Error\", {}).get(\"Code\") if hasattr(exc, \"response\") else None\n+                if error_code in {\"404\", \"NoSuchKey\", \"NotFound\"}:\n+                    continue\n+                raise\n+        raise RuntimeError(\"Episode video not found in S3 (checked v2 then v1)\")\n+        return info\n+\n+    def object_exists(self, key: str) -> bool:\n+        if self.backend not in {\"s3\", \"minio\"} or self._client is None or self._client_error_cls is None:\n+            return False\n+        try:\n+            self._client.head_object(Bucket=self.bucket, Key=key)\n+            return True\n+        except self._client_error_cls as exc:  # type: ignore[misc]","path":"apps/api/services/storage.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Non-exception [instance of 'NoneType'](1) in exception handler which will never match raised exception.","created_at":"2025-11-11T20:00:11Z","updated_at":"2025-11-11T20:00:22Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565427","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565427"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565427"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565427/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":319,"original_line":319,"side":"RIGHT","author_association":"NONE","original_position":319,"position":319,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565445","pull_request_review_id":3449784939,"id":2515565445,"node_id":"PRRC_kwDOQRcQs86V8HuF","diff_hunk":"@@ -0,0 +1,111 @@\n+from __future__ import annotations\n+\n+import logging\n+from pathlib import Path\n+from typing import Iterable\n+\n+import cv2  # type: ignore\n+import numpy as np\n+\n+LOGGER = logging.getLogger(__name__)\n+\n+\n+def clip_bbox(x1: float, y1: float, x2: float, y2: float, *, W: int, H: int) -> tuple[int, int, int, int] | None:\n+    \"\"\"Clamp an XYXY box to integer pixel coordinates.\"\"\"\n+    if W <= 1 or H <= 1:\n+        return None\n+    try:\n+        x1 = max(0, min(int(x1), W - 1))\n+        x2 = max(0, min(int(x2), W))\n+        y1 = max(0, min(int(y1), H - 1))\n+        y2 = max(0, min(int(y2), H))\n+    except (TypeError, ValueError):\n+        return None\n+    if x2 <= x1 or y2 <= y1:\n+        return None\n+    return x1, y1, x2, y2\n+\n+\n+def to_u8_bgr(image: np.ndarray) -> np.ndarray:\n+    \"\"\"Return a contiguous uint8 BGR image.\"\"\"\n+    if image is None:\n+        return image\n+    arr = np.asarray(image)\n+    if arr.size == 0:\n+        return arr\n+    if arr.dtype != np.uint8:\n+        arr = np.clip(arr, 0, 255)\n+        max_val = float(arr.max()) if arr.size else 0.0\n+        if max_val <= 1.0:\n+            arr = arr * 255.0\n+        arr = arr.astype(np.uint8, copy=False)\n+    if arr.ndim == 2:\n+        arr = cv2.cvtColor(arr, cv2.COLOR_GRAY2BGR)\n+    elif arr.ndim == 3 and arr.shape[2] >= 3:\n+        arr = arr[:, :, :3]\n+    elif arr.ndim == 3 and arr.shape[2] == 1:\n+        arr = np.repeat(arr, 3, axis=2)\n+    else:\n+        arr = np.broadcast_to(arr[..., None], arr.shape + (3,))\n+    return np.ascontiguousarray(arr)\n+\n+\n+def safe_crop(frame_bgr, bbox: Iterable[float]) -> tuple[np.ndarray | None, tuple[int, int, int, int] | None, str | None]:\n+    \"\"\"Crop using clip_bbox + dtype normalization.\"\"\"\n+    if frame_bgr is None:\n+        return None, None, \"frame_missing\"\n+    arr = np.asarray(frame_bgr)\n+    if arr.ndim < 2:\n+        return None, None, \"invalid_frame\"\n+    H, W = arr.shape[:2]\n+    try:\n+        x1, y1, x2, y2 = bbox\n+    except Exception:\n+        return None, None, \"invalid_bbox\"\n+    clipped = clip_bbox(x1, y1, x2, y2, W=W, H=H)\n+    if clipped is None:\n+        return None, None, \"degenerate_bbox\"\n+    rx1, ry1, rx2, ry2 = clipped\n+    crop = arr[ry1:ry2, rx1:rx2]\n+    if crop.size == 0:\n+        return None, clipped, \"empty_slice\"\n+    return to_u8_bgr(crop), clipped, None\n+\n+\n+def safe_imwrite(path: str | Path, image, jpg_q: int = 85) -> tuple[bool, str | None]:\n+    \"\"\"Write JPEGs with variance + size guards.\"\"\"\n+    if image is None:\n+        return False, \"image_missing\"\n+    img = to_u8_bgr(np.asarray(image))\n+    out_path = Path(path)\n+    out_path.parent.mkdir(parents=True, exist_ok=True)\n+    jpeg_q = max(1, min(int(jpg_q or 85), 100))\n+    variance = float(np.std(img)) if img.size else 0.0\n+    range_val = (\n+        float(np.nanmax(img)) - float(np.nanmin(img))\n+        if img.size\n+        else 0.0\n+    )\n+    ok = cv2.imwrite(str(out_path), img, [cv2.IMWRITE_JPEG_QUALITY, jpeg_q])\n+    if not ok:\n+        return False, \"imwrite_failed\"\n+    try:\n+        size_bytes = out_path.stat().st_size\n+    except OSError:\n+        size_bytes = 0\n+    if size_bytes < 1024:\n+        try:\n+            out_path.unlink()\n+        except OSError:","path":"tools/_img_utils.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:11Z","updated_at":"2025-11-11T20:00:22Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565445","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565445"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565445"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565445/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":99,"original_line":99,"side":"RIGHT","author_association":"NONE","original_position":99,"position":99,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565463","pull_request_review_id":3449784939,"id":2515565463,"node_id":"PRRC_kwDOQRcQs86V8HuX","diff_hunk":"@@ -0,0 +1,111 @@\n+from __future__ import annotations\n+\n+import logging\n+from pathlib import Path\n+from typing import Iterable\n+\n+import cv2  # type: ignore\n+import numpy as np\n+\n+LOGGER = logging.getLogger(__name__)\n+\n+\n+def clip_bbox(x1: float, y1: float, x2: float, y2: float, *, W: int, H: int) -> tuple[int, int, int, int] | None:\n+    \"\"\"Clamp an XYXY box to integer pixel coordinates.\"\"\"\n+    if W <= 1 or H <= 1:\n+        return None\n+    try:\n+        x1 = max(0, min(int(x1), W - 1))\n+        x2 = max(0, min(int(x2), W))\n+        y1 = max(0, min(int(y1), H - 1))\n+        y2 = max(0, min(int(y2), H))\n+    except (TypeError, ValueError):\n+        return None\n+    if x2 <= x1 or y2 <= y1:\n+        return None\n+    return x1, y1, x2, y2\n+\n+\n+def to_u8_bgr(image: np.ndarray) -> np.ndarray:\n+    \"\"\"Return a contiguous uint8 BGR image.\"\"\"\n+    if image is None:\n+        return image\n+    arr = np.asarray(image)\n+    if arr.size == 0:\n+        return arr\n+    if arr.dtype != np.uint8:\n+        arr = np.clip(arr, 0, 255)\n+        max_val = float(arr.max()) if arr.size else 0.0\n+        if max_val <= 1.0:\n+            arr = arr * 255.0\n+        arr = arr.astype(np.uint8, copy=False)\n+    if arr.ndim == 2:\n+        arr = cv2.cvtColor(arr, cv2.COLOR_GRAY2BGR)\n+    elif arr.ndim == 3 and arr.shape[2] >= 3:\n+        arr = arr[:, :, :3]\n+    elif arr.ndim == 3 and arr.shape[2] == 1:\n+        arr = np.repeat(arr, 3, axis=2)\n+    else:\n+        arr = np.broadcast_to(arr[..., None], arr.shape + (3,))\n+    return np.ascontiguousarray(arr)\n+\n+\n+def safe_crop(frame_bgr, bbox: Iterable[float]) -> tuple[np.ndarray | None, tuple[int, int, int, int] | None, str | None]:\n+    \"\"\"Crop using clip_bbox + dtype normalization.\"\"\"\n+    if frame_bgr is None:\n+        return None, None, \"frame_missing\"\n+    arr = np.asarray(frame_bgr)\n+    if arr.ndim < 2:\n+        return None, None, \"invalid_frame\"\n+    H, W = arr.shape[:2]\n+    try:\n+        x1, y1, x2, y2 = bbox\n+    except Exception:\n+        return None, None, \"invalid_bbox\"\n+    clipped = clip_bbox(x1, y1, x2, y2, W=W, H=H)\n+    if clipped is None:\n+        return None, None, \"degenerate_bbox\"\n+    rx1, ry1, rx2, ry2 = clipped\n+    crop = arr[ry1:ry2, rx1:rx2]\n+    if crop.size == 0:\n+        return None, clipped, \"empty_slice\"\n+    return to_u8_bgr(crop), clipped, None\n+\n+\n+def safe_imwrite(path: str | Path, image, jpg_q: int = 85) -> tuple[bool, str | None]:\n+    \"\"\"Write JPEGs with variance + size guards.\"\"\"\n+    if image is None:\n+        return False, \"image_missing\"\n+    img = to_u8_bgr(np.asarray(image))\n+    out_path = Path(path)\n+    out_path.parent.mkdir(parents=True, exist_ok=True)\n+    jpeg_q = max(1, min(int(jpg_q or 85), 100))\n+    variance = float(np.std(img)) if img.size else 0.0\n+    range_val = (\n+        float(np.nanmax(img)) - float(np.nanmin(img))\n+        if img.size\n+        else 0.0\n+    )\n+    ok = cv2.imwrite(str(out_path), img, [cv2.IMWRITE_JPEG_QUALITY, jpeg_q])\n+    if not ok:\n+        return False, \"imwrite_failed\"\n+    try:\n+        size_bytes = out_path.stat().st_size\n+    except OSError:\n+        size_bytes = 0\n+    if size_bytes < 1024:\n+        try:\n+            out_path.unlink()\n+        except OSError:\n+            pass\n+        return False, \"tiny_file\"\n+    if variance <= 0.05 and range_val <= 1.0:\n+        try:\n+            out_path.unlink()\n+        except OSError:","path":"tools/_img_utils.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:11Z","updated_at":"2025-11-11T20:00:22Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565463","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565463"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565463"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565463/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":105,"original_line":105,"side":"RIGHT","author_association":"NONE","original_position":105,"position":105,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565489","pull_request_review_id":3449784939,"id":2515565489,"node_id":"PRRC_kwDOQRcQs86V8Hux","diff_hunk":"@@ -0,0 +1,98 @@\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import os\n+from collections import Counter\n+from pathlib import Path\n+from typing import Any, Dict, Iterable\n+\n+TRUTHY = {\"1\", \"true\", \"yes\", \"on\"}\n+\n+\n+class JsonlLogger:\n+    \"\"\"Line-buffered logger for crop diagnostics.\"\"\"\n+\n+    def __init__(self, path: str | Path) -> None:\n+        self.path = Path(path)\n+        self.path.parent.mkdir(parents=True, exist_ok=True)\n+        # line buffering for tail -f\n+        self._handle = self.path.open(\"a\", encoding=\"utf-8\", buffering=1)\n+\n+    def __call__(self, payload: Dict[str, Any]) -> None:\n+        self._handle.write(json.dumps(payload, ensure_ascii=False) + \"\\n\")\n+\n+    def close(self) -> None:\n+        try:\n+            self._handle.close()\n+        except Exception:","path":"tools/debug_thumbs.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:12Z","updated_at":"2025-11-11T20:00:22Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565489","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565489"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565489"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565489/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":28,"original_line":28,"side":"RIGHT","author_association":"NONE","original_position":28,"position":28,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565517","pull_request_review_id":3449784939,"id":2515565517,"node_id":"PRRC_kwDOQRcQs86V8HvN","diff_hunk":"@@ -0,0 +1,1454 @@\n+from __future__ import annotations\n+\n+import json\n+import logging\n+import os\n+import re\n+import sys\n+from collections import defaultdict\n+from datetime import date, datetime\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, List\n+\n+from fastapi import APIRouter, Body, HTTPException, Query\n+from pydantic import BaseModel, Field\n+\n+PROJECT_ROOT = Path(__file__).resolve().parents[3]\n+if str(PROJECT_ROOT) not in sys.path:\n+    sys.path.append(str(PROJECT_ROOT))\n+\n+from py_screenalytics.artifacts import ensure_dirs, get_path\n+\n+from apps.api.services import roster as roster_service\n+from apps.api.services import identities as identity_service\n+from apps.api.services.episodes import EpisodeStore\n+from apps.api.services.storage import (\n+    StorageService,\n+    artifact_prefixes,\n+    delete_local_tree,\n+    delete_s3_prefix,\n+    episode_context_from_id,\n+    v2_artifact_prefixes,\n+)\n+\n+router = APIRouter()\n+EPISODE_STORE = EpisodeStore()\n+STORAGE = StorageService()\n+LOGGER = logging.getLogger(__name__)\n+\n+\n+def _manifests_dir(ep_id: str) -> Path:\n+    return get_path(ep_id, \"detections\").parent\n+\n+\n+def _faces_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"faces.jsonl\"\n+\n+\n+def _faces_ops_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"faces_ops.jsonl\"\n+\n+\n+def _append_face_ops(ep_id: str, entries: Iterable[Dict[str, Any]]) -> None:\n+    entries = list(entries)\n+    if not entries:\n+        return\n+    path = _faces_ops_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    timestamp = datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n+    with path.open(\"a\", encoding=\"utf-8\") as handle:\n+        for entry in entries:\n+            payload = dict(entry)\n+            payload.setdefault(\"ts\", timestamp)\n+            handle.write(json.dumps(payload) + \"\\n\")\n+\n+\n+def _identities_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"identities.json\"\n+\n+\n+def _runs_dir(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"runs\"\n+\n+\n+def _tracks_path(ep_id: str) -> Path:\n+    return get_path(ep_id, \"tracks\")\n+\n+\n+def _thumbs_root(ep_id: str) -> Path:\n+    return get_path(ep_id, \"frames_root\") / \"thumbs\"\n+\n+\n+def _remove_face_assets(ep_id: str, rows: Iterable[Dict[str, Any]]) -> None:\n+    frames_root = get_path(ep_id, \"frames_root\")\n+    thumbs_root = _thumbs_root(ep_id)\n+    for row in rows:\n+        thumb_rel = row.get(\"thumb_rel_path\")\n+        if isinstance(thumb_rel, str):\n+            thumb_file = thumbs_root / thumb_rel\n+            try:\n+                thumb_file.unlink()\n+            except FileNotFoundError:","path":"apps/api/routers/episodes.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:12Z","updated_at":"2025-11-11T20:00:22Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565517","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565517"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565517"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565517/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":91,"original_line":91,"side":"RIGHT","author_association":"NONE","original_position":91,"position":91,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565539","pull_request_review_id":3449784939,"id":2515565539,"node_id":"PRRC_kwDOQRcQs86V8Hvj","diff_hunk":"@@ -0,0 +1,1454 @@\n+from __future__ import annotations\n+\n+import json\n+import logging\n+import os\n+import re\n+import sys\n+from collections import defaultdict\n+from datetime import date, datetime\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, List\n+\n+from fastapi import APIRouter, Body, HTTPException, Query\n+from pydantic import BaseModel, Field\n+\n+PROJECT_ROOT = Path(__file__).resolve().parents[3]\n+if str(PROJECT_ROOT) not in sys.path:\n+    sys.path.append(str(PROJECT_ROOT))\n+\n+from py_screenalytics.artifacts import ensure_dirs, get_path\n+\n+from apps.api.services import roster as roster_service\n+from apps.api.services import identities as identity_service\n+from apps.api.services.episodes import EpisodeStore\n+from apps.api.services.storage import (\n+    StorageService,\n+    artifact_prefixes,\n+    delete_local_tree,\n+    delete_s3_prefix,\n+    episode_context_from_id,\n+    v2_artifact_prefixes,\n+)\n+\n+router = APIRouter()\n+EPISODE_STORE = EpisodeStore()\n+STORAGE = StorageService()\n+LOGGER = logging.getLogger(__name__)\n+\n+\n+def _manifests_dir(ep_id: str) -> Path:\n+    return get_path(ep_id, \"detections\").parent\n+\n+\n+def _faces_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"faces.jsonl\"\n+\n+\n+def _faces_ops_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"faces_ops.jsonl\"\n+\n+\n+def _append_face_ops(ep_id: str, entries: Iterable[Dict[str, Any]]) -> None:\n+    entries = list(entries)\n+    if not entries:\n+        return\n+    path = _faces_ops_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    timestamp = datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n+    with path.open(\"a\", encoding=\"utf-8\") as handle:\n+        for entry in entries:\n+            payload = dict(entry)\n+            payload.setdefault(\"ts\", timestamp)\n+            handle.write(json.dumps(payload) + \"\\n\")\n+\n+\n+def _identities_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"identities.json\"\n+\n+\n+def _runs_dir(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"runs\"\n+\n+\n+def _tracks_path(ep_id: str) -> Path:\n+    return get_path(ep_id, \"tracks\")\n+\n+\n+def _thumbs_root(ep_id: str) -> Path:\n+    return get_path(ep_id, \"frames_root\") / \"thumbs\"\n+\n+\n+def _remove_face_assets(ep_id: str, rows: Iterable[Dict[str, Any]]) -> None:\n+    frames_root = get_path(ep_id, \"frames_root\")\n+    thumbs_root = _thumbs_root(ep_id)\n+    for row in rows:\n+        thumb_rel = row.get(\"thumb_rel_path\")\n+        if isinstance(thumb_rel, str):\n+            thumb_file = thumbs_root / thumb_rel\n+            try:\n+                thumb_file.unlink()\n+            except FileNotFoundError:\n+                pass\n+        crop_rel = row.get(\"crop_rel_path\")\n+        if isinstance(crop_rel, str):\n+            crop_file = frames_root / crop_rel\n+            try:\n+                crop_file.unlink()\n+            except FileNotFoundError:\n+                pass","path":"apps/api/routers/episodes.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.\n```suggestion\n                # It's fine if the thumbnail file does not exist.\n        crop_rel = row.get(\"crop_rel_path\")\n        if isinstance(crop_rel, str):\n            crop_file = frames_root / crop_rel\n            try:\n                crop_file.unlink()\n            except FileNotFoundError:\n                # It's fine if the crop file does not exist.\n```","created_at":"2025-11-11T20:00:12Z","updated_at":"2025-11-11T20:00:22Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565539","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565539"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565539"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565539/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":92,"original_start_line":92,"start_side":"RIGHT","line":99,"original_line":99,"side":"RIGHT","author_association":"NONE","original_position":99,"position":99,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565563","pull_request_review_id":3449784939,"id":2515565563,"node_id":"PRRC_kwDOQRcQs86V8Hv7","diff_hunk":"@@ -0,0 +1,3034 @@\n+#!/usr/bin/env python\n+\"\"\"Dev-only CLI to run detection → tracking for a single episode.\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import math\n+import os\n+import shutil\n+import sys\n+import time\n+from collections import Counter, defaultdict\n+from dataclasses import dataclass, field\n+from datetime import datetime\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, Iterator, List, Optional, Tuple, Callable\n+\n+import logging\n+from functools import lru_cache\n+\n+import numpy as np\n+\n+REPO_ROOT = Path(__file__).resolve().parents[1]\n+if str(REPO_ROOT) not in sys.path:\n+    sys.path.insert(0, str(REPO_ROOT))\n+\n+from apps.api.services.storage import (\n+    EpisodeContext,\n+    StorageService,\n+    artifact_prefixes,\n+    episode_context_from_id,\n+)\n+from py_screenalytics.artifacts import ensure_dirs, get_path\n+from tools._img_utils import clip_bbox, safe_crop, safe_imwrite, to_u8_bgr\n+from tools.debug_thumbs import init_debug_logger, debug_thumbs_enabled, NullLogger, JsonlLogger\n+\n+PIPELINE_VERSION = os.environ.get(\"SCREENALYTICS_PIPELINE_VERSION\", \"2025-11-11\")\n+APP_VERSION = os.environ.get(\"SCREENALYTICS_APP_VERSION\", PIPELINE_VERSION)\n+YOLO_MODEL_NAME = os.environ.get(\"SCREENALYTICS_YOLO_MODEL\", \"yolov8n.pt\")\n+TRACKER_CONFIG = os.environ.get(\"SCREENALYTICS_TRACKER_CONFIG\", \"bytetrack.yaml\")\n+TRACKER_NAME = Path(TRACKER_CONFIG).stem if TRACKER_CONFIG else \"bytetrack\"\n+YOLO_IMAGE_SIZE = int(os.environ.get(\"SCREENALYTICS_YOLO_IMGSZ\", 640))\n+YOLO_CONF_THRESHOLD = float(os.environ.get(\"SCREENALYTICS_YOLO_CONF\", 0.25))\n+YOLO_IOU_THRESHOLD = float(os.environ.get(\"SCREENALYTICS_YOLO_IOU\", 0.45))\n+PROGRESS_FRAME_STEP = int(os.environ.get(\"SCREENALYTICS_PROGRESS_FRAME_STEP\", 25))\n+LOGGER = logging.getLogger(\"episode_run\")\n+DATA_ROOT = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+DETECTOR_CHOICES = (\"retinaface\", \"yolov8face\")\n+DEFAULT_DETECTOR = DETECTOR_CHOICES[0]\n+TRACKER_CHOICES = (\"bytetrack\", \"strongsort\")\n+DEFAULT_TRACKER = TRACKER_CHOICES[0]\n+ARC_FACE_MODEL_NAME = os.environ.get(\"ARCFACE_MODEL\", \"arcface_r100_v1\")\n+RETINAFACE_MODEL_NAME = os.environ.get(\"RETINAFACE_MODEL\", \"retinaface_r50_v1\")\n+FACE_CLASS_LABEL = \"face\"\n+MIN_FACE_AREA = 20.0\n+FACE_RATIO_BOUNDS = (0.5, 2.0)\n+RETINAFACE_SCORE_THRESHOLD = 0.5\n+RETINAFACE_NMS = 0.45\n+\n+RUN_MARKERS_SUBDIR = \"runs\"\n+def _parse_retinaface_det_size(value: str | None) -> tuple[int, int] | None:\n+    if not value:\n+        return 640, 640\n+    tokens: list[str] = []\n+    buf = value.replace(\"x\", \",\").replace(\"X\", \",\")\n+    for part in buf.split(\",\"):\n+        part = part.strip()\n+        if part:\n+            tokens.append(part)\n+    if len(tokens) != 2:\n+        return 640, 640\n+    try:\n+        width = max(int(float(tokens[0])), 1)\n+        height = max(int(float(tokens[1])), 1)\n+        return width, height\n+    except ValueError:\n+        return 640, 640\n+\n+\n+RETINAFACE_DET_SIZE = _parse_retinaface_det_size(os.environ.get(\"RETINAFACE_DET_SIZE\"))\n+\n+\n+def _normalize_det_thresh(value: float | str | None) -> float:\n+    try:\n+        numeric = float(value) if value is not None else RETINAFACE_SCORE_THRESHOLD\n+    except (TypeError, ValueError):\n+        numeric = RETINAFACE_SCORE_THRESHOLD\n+    return min(max(numeric, 0.0), 1.0)\n+YOLO_FACE_CONF = 0.5\n+BYTE_TRACK_MIN_BOX_AREA = 20.0\n+DEFAULT_GMC_METHOD = os.environ.get(\"SCREENALYTICS_GMC_METHOD\", \"sparseOptFlow\")\n+DEFAULT_REID_MODEL = os.environ.get(\"SCREENALYTICS_REID_MODEL\", \"yolov8n-cls.pt\")\n+DEFAULT_REID_ENABLED = os.environ.get(\"SCREENALYTICS_REID_ENABLED\", \"1\").lower() in {\"1\", \"true\", \"yes\"}\n+RETINAFACE_HELP = \"RetinaFace weights missing or could not initialize. See README 'Models' or run scripts/fetch_models.py.\"\n+\n+\n+def _env_flag(name: str, default: bool) -> bool:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    value = raw.strip().lower()\n+    if value in {\"1\", \"true\", \"yes\", \"on\"}:\n+        return True\n+    if value in {\"0\", \"false\", \"off\", \"no\"}:\n+        return False\n+    return default\n+\n+\n+def _env_float(name: str, default: float) -> float:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    try:\n+        return float(raw)\n+    except (TypeError, ValueError):\n+        return default\n+\n+\n+def _env_int(name: str, default: int) -> int:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    try:\n+        return int(raw)\n+    except (TypeError, ValueError):\n+        return default\n+\n+\n+def _resolve_track_sample_limit(value: str | int | None) -> int | None:\n+    if value is None:\n+        return None\n+    if isinstance(value, str):\n+        text = value.strip().lower()\n+        if text in {\"\", \"none\", \"unlimited\", \"all\", \"off\", \"disable\"}:\n+            return None\n+        try:\n+            numeric = int(float(text))\n+        except ValueError:\n+            return None\n+    else:\n+        numeric = int(value)\n+    return numeric if numeric > 0 else None\n+\n+\n+def _set_track_sample_limit(value: int | None) -> None:\n+    global TRACK_SAMPLE_LIMIT\n+    TRACK_SAMPLE_LIMIT = _resolve_track_sample_limit(value)\n+\n+\n+TRACK_SAMPLE_LIMIT = _resolve_track_sample_limit(os.environ.get(\"SCREENALYTICS_TRACK_SAMPLE_LIMIT\"))\n+\n+\n+SCENE_DETECT_DEFAULT = _env_flag(\"SCENE_DETECT\", True)\n+SCENE_THRESHOLD_DEFAULT = max(min(_env_float(\"SCENE_THRESHOLD\", 0.30), 2.0), 0.0)\n+SCENE_MIN_LEN_DEFAULT = max(_env_int(\"SCENE_MIN_LEN\", 12), 1)\n+SCENE_WARMUP_DETS_DEFAULT = max(_env_int(\"SCENE_WARMUP_DETS\", 3), 0)\n+\n+\n+def _normalize_device_label(device: str | None) -> str:\n+    normalized = (device or \"cpu\").lower()\n+    if normalized in {\"0\", \"cuda\", \"gpu\"}:\n+        return \"cuda\"\n+    return normalized\n+\n+\n+def _onnx_providers_for(device: str | None) -> tuple[list[str], str]:\n+    normalized = (device or \"auto\").lower()\n+    providers: list[str] = [\"CPUExecutionProvider\"]\n+    resolved = \"cpu\"\n+    if normalized in {\"cuda\", \"0\", \"gpu\", \"auto\"}:\n+        try:\n+            import onnxruntime as ort  # type: ignore\n+\n+            available = ort.get_available_providers()\n+        except Exception:\n+            available = []\n+        if \"CUDAExecutionProvider\" in available:\n+            providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n+            resolved = \"cuda\"\n+            return providers, resolved\n+        if normalized in {\"cuda\", \"0\", \"gpu\"}:\n+            LOGGER.warning(\"CUDA requested for RetinaFace/ArcFace but CUDAExecutionProvider unavailable; falling back to CPU\")\n+    if normalized in {\"mps\", \"metal\", \"apple\"}:\n+        return [\"CPUExecutionProvider\"], \"cpu\"\n+    return providers, resolved\n+\n+\n+def _init_retinaface(model_name: str, device: str, score_thresh: float = RETINAFACE_SCORE_THRESHOLD) -> tuple[Any, str]:\n+    try:\n+        from insightface.model_zoo import get_model  # type: ignore\n+    except ImportError as exc:  # pragma: no cover - runtime guard\n+        raise RuntimeError(\"insightface is required for RetinaFace detection\") from exc\n+\n+    providers, resolved = _onnx_providers_for(device)\n+    model = get_model(model_name)\n+    if model is None:\n+        raise RuntimeError(\n+            f\"RetinaFace weights '{model_name}' not found. Install insightface models or run scripts/fetch_models.py.\"\n+        )\n+    ctx_id = 0 if resolved == \"cuda\" else -1\n+    # InsightFace 0.7.x configures detection threshold at prepare-time\n+    # (detect() no longer accepts a `threshold` kwarg).\n+    prepare_kwargs = {\n+        \"ctx_id\": ctx_id,\n+        \"providers\": providers,\n+        \"nms\": RETINAFACE_NMS,\n+        \"det_thresh\": float(score_thresh),\n+    }\n+    if RETINAFACE_DET_SIZE:\n+        prepare_kwargs[\"input_size\"] = RETINAFACE_DET_SIZE\n+    try:\n+        model.prepare(**prepare_kwargs)\n+    except TypeError:\n+        prepare_kwargs.pop(\"input_size\", None)\n+        model.prepare(**prepare_kwargs)\n+    return model, resolved\n+\n+\n+def _init_arcface(model_name: str, device: str):\n+    try:\n+        from insightface.model_zoo import get_model  # type: ignore\n+    except ImportError as exc:  # pragma: no cover\n+        raise RuntimeError(\"insightface is required for ArcFace embeddings\") from exc\n+\n+    providers, resolved = _onnx_providers_for(device)\n+    model = get_model(model_name)\n+    if model is None:\n+        raise RuntimeError(\n+            f\"ArcFace weights '{model_name}' not found. Install insightface models or run scripts/fetch_models.py.\"\n+        )\n+    ctx_id = 0 if resolved == \"cuda\" else -1\n+    model.prepare(ctx_id=ctx_id, providers=providers)\n+    return model, resolved\n+\n+\n+def ensure_retinaface_ready(device: str, det_thresh: float | None = None) -> tuple[bool, Optional[str], Optional[str]]:\n+    \"\"\"Lightweight readiness probe for API preflight checks.\"\"\"\n+\n+    model = None\n+    try:\n+        model, resolved = _init_retinaface(\n+            RETINAFACE_MODEL_NAME,\n+            device,\n+            det_thresh if det_thresh is not None else RETINAFACE_SCORE_THRESHOLD,\n+        )\n+    except Exception as exc:  # pragma: no cover - surfaced via API tests\n+        return False, str(exc), None\n+    finally:\n+        if model is not None:\n+            del model\n+    return True, None, resolved\n+\n+\n+def pick_device(explicit: str | None = None) -> str:\n+    \"\"\"Return the safest device available.\n+\n+    Order of preference: explicit override → CUDA → MPS → CPU.\n+    Values returned are what Ultralytics expects (\"cpu\", \"mps\", \"cuda\"/\"0\").\n+    \"\"\"\n+\n+    if explicit and explicit not in {\"auto\", \"\"}:\n+        return explicit\n+\n+    try:\n+        import torch  # type: ignore\n+\n+        if torch.cuda.is_available():  # pragma: no cover - depends on env\n+            return \"0\"\n+        mps_available = getattr(torch.backends, \"mps\", None)\n+        if mps_available is not None and mps_available.is_available():  # pragma: no cover - mac only\n+            return \"mps\"\n+    except Exception:  # pragma: no cover - torch import/runtime guard\n+        pass\n+\n+    return \"cpu\"\n+\n+\n+def _normalize_detector_choice(detector: str | None) -> str:\n+    if detector:\n+        value = detector.strip().lower()\n+        if value in DETECTOR_CHOICES:\n+            return value\n+    return DEFAULT_DETECTOR\n+\n+\n+def _normalize_tracker_choice(tracker: str | None) -> str:\n+    if tracker:\n+        value = tracker.strip().lower()\n+        if value in TRACKER_CHOICES:\n+            return value\n+    return DEFAULT_TRACKER\n+\n+\n+def _valid_face_box(bbox: np.ndarray, score: float, *, min_score: float, min_area: float) -> bool:\n+    width = bbox[2] - bbox[0]\n+    height = bbox[3] - bbox[1]\n+    area = max(width, 0.0) * max(height, 0.0)\n+    if score < min_score:\n+        return False\n+    if area < min_area:\n+        return False\n+    ratio = width / max(height, 1e-6)\n+    return FACE_RATIO_BOUNDS[0] <= ratio <= FACE_RATIO_BOUNDS[1]\n+\n+\n+def _nms_detections(\n+    detections: list[tuple[np.ndarray, float, np.ndarray | None]],\n+    thresh: float,\n+) -> list[tuple[np.ndarray, float, np.ndarray | None]]:\n+    ordered = sorted(range(len(detections)), key=lambda idx: detections[idx][1], reverse=True)\n+    keep: list[tuple[np.ndarray, float, np.ndarray | None]] = []\n+    while ordered:\n+        current_idx = ordered.pop(0)\n+        current = detections[current_idx]\n+        keep.append(current)\n+        remaining: list[int] = []\n+        for idx in ordered:\n+            iou = _bbox_iou(current[0].tolist(), detections[idx][0].tolist())\n+            if iou < thresh:\n+                remaining.append(idx)\n+        ordered = remaining\n+    return keep\n+\n+\n+@dataclass\n+class TrackAccumulator:\n+    track_id: int\n+    class_id: int | str\n+    first_ts: float\n+    last_ts: float\n+    frame_count: int = 0\n+    samples: List[dict] = field(default_factory=list)\n+\n+    def add(self, ts: float, frame_idx: int, bbox_xyxy: List[float], landmarks: List[float] | None = None) -> None:\n+        self.frame_count += 1\n+        self.last_ts = ts\n+        limit = TRACK_SAMPLE_LIMIT\n+        if limit is None or len(self.samples) < limit:\n+            self.samples.append(\n+                {\n+                    \"frame_idx\": frame_idx,\n+                    \"ts\": round(float(ts), 4),\n+                    \"bbox_xyxy\": [round(float(coord), 4) for coord in bbox_xyxy],\n+                    **({\"landmarks\": [round(float(val), 4) for val in landmarks]} if landmarks else {}),\n+                }\n+            )\n+\n+    def to_row(self) -> dict:\n+        row = {\n+            \"track_id\": self.track_id,\n+            \"class\": self.class_id,\n+            \"first_ts\": round(float(self.first_ts), 4),\n+            \"last_ts\": round(float(self.last_ts), 4),\n+            \"frame_count\": self.frame_count,\n+            \"pipeline_ver\": PIPELINE_VERSION,\n+        }\n+        if self.samples:\n+            row[\"bboxes_sampled\"] = self.samples\n+        return row\n+\n+\n+@dataclass\n+class DetectionSample:\n+    bbox: np.ndarray\n+    conf: float\n+    class_idx: int\n+    class_label: str\n+    landmarks: np.ndarray | None = None\n+    embedding: np.ndarray | None = None\n+\n+\n+@dataclass\n+class TrackedObject:\n+    track_id: int\n+    bbox: np.ndarray\n+    conf: float\n+    class_idx: int\n+    class_label: str\n+    det_index: int | None = None\n+    landmarks: np.ndarray | None = None\n+\n+\n+class _TrackerDetections:\n+    \"\"\"Lightweight structure that mimics ultralytics' Boxes for BYTETracker inputs.\"\"\"\n+\n+    def __init__(self, boxes: np.ndarray, scores: np.ndarray, classes: np.ndarray) -> None:\n+        self.xyxy = boxes.astype(np.float32)\n+        self.conf = scores.astype(np.float32)\n+        self.cls = classes.astype(np.float32)\n+        self._xywh: np.ndarray | None = None\n+\n+    @property\n+    def xywh(self) -> np.ndarray:\n+        if self._xywh is None:\n+            self._xywh = self.xyxy.copy()\n+            self._xywh[:, 2] = self._xywh[:, 2] - self._xywh[:, 0]\n+            self._xywh[:, 3] = self._xywh[:, 3] - self._xywh[:, 1]\n+            self._xywh[:, 0] = self._xywh[:, 0] + self._xywh[:, 2] / 2\n+            self._xywh[:, 1] = self._xywh[:, 1] + self._xywh[:, 3] / 2\n+        return self._xywh\n+\n+    @property\n+    def xywhr(self) -> np.ndarray:\n+        return self.xywh\n+\n+\n+def _tracker_inputs_from_samples(detections: list[DetectionSample]) -> _TrackerDetections:\n+    if detections:\n+        boxes = np.vstack([sample.bbox for sample in detections]).astype(np.float32)\n+        scores = np.asarray([sample.conf for sample in detections], dtype=np.float32)\n+        classes = np.asarray([sample.class_idx for sample in detections], dtype=np.float32)\n+        return _TrackerDetections(boxes, scores, classes)\n+    return _TrackerDetections(\n+        np.zeros((0, 4), dtype=np.float32),\n+        np.zeros(0, dtype=np.float32),\n+        np.zeros(0, dtype=np.float32),\n+    )\n+\n+\n+class ByteTrackAdapter:\n+    \"\"\"Wrapper around ultralytics BYTETracker for direct invocation.\"\"\"\n+\n+    def __init__(self, frame_rate: float = 30.0) -> None:\n+        self.frame_rate = max(frame_rate, 1)\n+        self._tracker = self._build_tracker()\n+\n+    def _build_tracker(self):\n+        from types import SimpleNamespace\n+\n+        from ultralytics.trackers.byte_tracker import BYTETracker\n+\n+        cfg = SimpleNamespace(\n+            tracker_type=\"bytetrack\",\n+            track_high_thresh=0.6,\n+            track_low_thresh=0.1,\n+            new_track_thresh=0.6,\n+            track_buffer=30,\n+            match_thresh=0.8,\n+            min_box_area=BYTE_TRACK_MIN_BOX_AREA,\n+        )\n+        return BYTETracker(cfg, frame_rate=self.frame_rate)\n+\n+    def update(self, detections: list[DetectionSample], frame_idx: int, image) -> list[TrackedObject]:\n+        det_struct = _tracker_inputs_from_samples(detections)\n+        tracks = self._tracker.update(det_struct, image)\n+        tracked: list[TrackedObject] = []\n+        if tracks.size == 0:\n+            return tracked\n+        for row in tracks:\n+            bbox = np.asarray(row[:4], dtype=np.float32)\n+            track_id = int(row[4])\n+            score = float(row[5])\n+            class_idx = int(row[6]) if len(row) > 6 else 0\n+            det_index = int(row[7]) if len(row) > 7 else None\n+            label = \"\"\n+            landmarks = None\n+            if det_index is not None and 0 <= det_index < len(detections):\n+                det = detections[det_index]\n+                label = det.class_label\n+                class_idx = det.class_idx\n+                landmarks = det.landmarks\n+            tracked.append(\n+                TrackedObject(\n+                    track_id=track_id,\n+                    bbox=bbox,\n+                    conf=score,\n+                    class_idx=class_idx,\n+                    class_label=label,\n+                    det_index=det_index,\n+                    landmarks=landmarks,\n+                )\n+            )\n+        return tracked\n+\n+    def reset(self) -> None:\n+        self._tracker = self._build_tracker()\n+\n+\n+class StrongSortAdapter:\n+    \"\"\"Adapter around Ultralytics BOT-SORT tracker (used as a StrongSORT-style ReID tracker).\"\"\"\n+\n+    def __init__(self, frame_rate: float = 30.0) -> None:\n+        self.frame_rate = max(frame_rate, 1)\n+        self._tracker = self._build_tracker()\n+\n+    def _build_tracker(self):\n+        from types import SimpleNamespace\n+\n+        try:\n+            from ultralytics.trackers.bot_sort import BOTSORT\n+        except ImportError as exc:  # pragma: no cover - dependency missing\n+            raise RuntimeError(\n+                \"StrongSORT tracker unavailable; ensure ultralytics>=8.2.70 is installed.\"\n+            ) from exc\n+\n+        cfg = SimpleNamespace(\n+            tracker_type=\"strongsort\",\n+            track_high_thresh=0.6,\n+            track_low_thresh=0.1,\n+            new_track_thresh=0.6,\n+            track_buffer=30,\n+            match_thresh=0.8,\n+            min_box_area=BYTE_TRACK_MIN_BOX_AREA,\n+            gmc_method=os.environ.get(\"SCREENALYTICS_GMC_METHOD\", DEFAULT_GMC_METHOD),\n+            proximity_thresh=float(os.environ.get(\"SCREENALYTICS_REID_PROXIMITY\", \"0.6\")),\n+            appearance_thresh=float(os.environ.get(\"SCREENALYTICS_REID_APPEARANCE\", \"0.7\")),\n+            with_reid=_env_flag(\"SCREENALYTICS_REID_ENABLED\", DEFAULT_REID_ENABLED),\n+            model=os.environ.get(\"SCREENALYTICS_REID_MODEL\", DEFAULT_REID_MODEL) or \"auto\",\n+            fuse_score=_env_flag(\"SCREENALYTICS_REID_FUSE_SCORE\", False),\n+        )\n+        return BOTSORT(cfg, frame_rate=self.frame_rate)\n+\n+    def update(self, detections: list[DetectionSample], frame_idx: int, image) -> list[TrackedObject]:\n+        det_struct = _tracker_inputs_from_samples(detections)\n+        tracks = self._tracker.update(det_struct, image)\n+        tracked: list[TrackedObject] = []\n+        if tracks.size == 0:\n+            return tracked\n+        for row in tracks:\n+            bbox = np.asarray(row[:4], dtype=np.float32)\n+            track_id = int(row[4])\n+            score = float(row[5])\n+            class_idx = int(row[6]) if len(row) > 6 else 0\n+            det_index = int(row[7]) if len(row) > 7 else None\n+            label = \"\"\n+            landmarks = None\n+            if det_index is not None and 0 <= det_index < len(detections):\n+                det = detections[det_index]\n+                label = det.class_label\n+                class_idx = det.class_idx\n+                landmarks = det.landmarks\n+            tracked.append(\n+                TrackedObject(\n+                    track_id=track_id,\n+                    bbox=bbox,\n+                    conf=score,\n+                    class_idx=class_idx,\n+                    class_label=label,\n+                    det_index=det_index,\n+                    landmarks=landmarks,\n+                )\n+            )\n+        return tracked\n+\n+    def reset(self) -> None:\n+        self._tracker = self._build_tracker()\n+\n+\n+class RetinaFaceDetectorBackend:\n+    def __init__(self, device: str, score_thresh: float = RETINAFACE_SCORE_THRESHOLD) -> None:\n+        self.device = device\n+        self.score_thresh = max(min(float(score_thresh or RETINAFACE_SCORE_THRESHOLD), 1.0), 0.0)\n+        self.min_area = MIN_FACE_AREA\n+        self._model = None\n+        self._resolved_device: Optional[str] = None\n+\n+    def _lazy_model(self):\n+        if self._model is not None:\n+            return self._model\n+        try:\n+            model, resolved = _init_retinaface(self.model_name, self.device, self.score_thresh)\n+        except Exception as exc:\n+            raise RuntimeError(f\"{RETINAFACE_HELP} ({exc})\") from exc\n+        self._resolved_device = resolved\n+        self._model = model\n+        return self._model\n+\n+    @property\n+    def model_name(self) -> str:\n+        return RETINAFACE_MODEL_NAME\n+\n+    @property\n+    def resolved_device(self) -> str:\n+        if self._resolved_device is None:\n+            self.ensure_ready()\n+        return self._resolved_device or \"cpu\"\n+\n+    def ensure_ready(self) -> None:\n+        self._lazy_model()\n+\n+    def detect(self, image) -> list[DetectionSample]:\n+        model = self._lazy_model()\n+        # Threshold + input size configured during model.prepare. Some InsightFace\n+        # RetinaFace builds still require an explicit input_size, so pass it when\n+        # available.\n+        detect_kwargs = {}\n+        input_size = getattr(model, \"input_size\", None) or RETINAFACE_DET_SIZE\n+        if input_size:\n+            detect_kwargs[\"input_size\"] = input_size\n+        bboxes, landmarks = model.detect(image, **detect_kwargs)\n+        if bboxes is None or len(bboxes) == 0:\n+            return []\n+        pending: list[tuple[np.ndarray, float, np.ndarray | None]] = []\n+        for idx in range(len(bboxes)):\n+            raw = bboxes[idx]\n+            score = float(raw[4]) if raw.shape[0] >= 5 else float(self.score_thresh)\n+            bbox = raw[:4].astype(np.float32)\n+            if not _valid_face_box(bbox, score, min_score=self.score_thresh, min_area=self.min_area):\n+                continue\n+            kps = None\n+            if landmarks is not None and idx < len(landmarks):\n+                kps = landmarks[idx].astype(np.float32).reshape(-1)\n+            pending.append((bbox, score, kps))\n+        filtered = _nms_detections(pending, RETINAFACE_NMS) if pending else []\n+        samples: list[DetectionSample] = []\n+        for bbox, score, kps in filtered:\n+            samples.append(\n+                DetectionSample(\n+                    bbox=bbox.astype(np.float32),\n+                    conf=score,\n+                    class_idx=0,\n+                    class_label=FACE_CLASS_LABEL,\n+                    landmarks=kps.copy() if isinstance(kps, np.ndarray) else None,\n+                )\n+            )\n+        return samples\n+\n+\n+class YoloFaceDetectorBackend:\n+    def __init__(self, device: str) -> None:\n+        from ultralytics import YOLO\n+\n+        self.device = device\n+        self.model_path = os.environ.get(\"SCREENALYTICS_YOLO_FACE_MODEL\", \"yolov8n-face.pt\")\n+        self._model = YOLO(self.model_path)\n+        self._resolved_device = _normalize_device_label(device)\n+\n+    @property\n+    def model_name(self) -> str:\n+        return self.model_path\n+\n+    @property\n+    def resolved_device(self) -> str:\n+        return self._resolved_device\n+\n+    def ensure_ready(self) -> None:\n+        _ = self._model\n+\n+    def detect(self, image) -> list[DetectionSample]:\n+        results = self._model.predict(\n+            source=image,\n+            imgsz=YOLO_IMAGE_SIZE,\n+            conf=YOLO_FACE_CONF,\n+            device=self.device,\n+            verbose=False,\n+        )\n+        samples: list[DetectionSample] = []\n+        if not results:\n+            return samples\n+        boxes = results[0].boxes\n+        if boxes is None or boxes.data is None or len(boxes) == 0:\n+            return samples\n+        xyxy = boxes.xyxy.cpu().numpy()\n+        scores = boxes.conf.cpu().numpy()\n+        for idx in range(len(xyxy)):\n+            bbox = xyxy[idx].astype(np.float32)\n+            score = float(scores[idx])\n+            if not _valid_face_box(bbox, score, min_score=YOLO_FACE_CONF, min_area=MIN_FACE_AREA):\n+                continue\n+            samples.append(\n+                DetectionSample(\n+                    bbox=bbox,\n+                    conf=score,\n+                    class_idx=0,\n+                    class_label=FACE_CLASS_LABEL,\n+                )\n+            )\n+        return samples\n+\n+\n+def _build_face_detector(detector: str, device: str, score_thresh: float = RETINAFACE_SCORE_THRESHOLD):\n+    if detector == \"yolov8face\":\n+        return YoloFaceDetectorBackend(device)\n+    return RetinaFaceDetectorBackend(device, score_thresh=score_thresh)\n+\n+\n+def _build_tracker_adapter(tracker: str, frame_rate: float) -> ByteTrackAdapter | StrongSortAdapter:\n+    if tracker == \"strongsort\":\n+        return StrongSortAdapter(frame_rate=frame_rate)\n+    return ByteTrackAdapter(frame_rate=frame_rate)\n+\n+\n+class ArcFaceEmbedder:\n+    def __init__(self, device: str) -> None:\n+        self.device = device\n+        self._model = None\n+        self._resolved_device: Optional[str] = None\n+\n+    def _lazy_model(self):\n+        if self._model is not None:\n+            return self._model\n+        try:\n+            model, resolved = _init_arcface(ARC_FACE_MODEL_NAME, self.device)\n+        except Exception as exc:\n+            raise RuntimeError(f\"ArcFace init failed: {exc}. Install insightface + models or run scripts/fetch_models.py.\") from exc\n+        self._resolved_device = resolved\n+        self._model = model\n+        return self._model\n+\n+    def ensure_ready(self) -> None:\n+        self._lazy_model()\n+\n+    @property\n+    def resolved_device(self) -> str:\n+        if self._resolved_device is None:\n+            self.ensure_ready()\n+        return self._resolved_device or \"cpu\"\n+\n+    def encode(self, crops: list[np.ndarray]) -> np.ndarray:\n+        if not crops:\n+            return np.zeros((0, 512), dtype=np.float32)\n+        model = self._lazy_model()\n+        embeddings: list[np.ndarray] = []\n+        for crop in crops:\n+            if crop is None or crop.size == 0:\n+                embeddings.append(np.zeros(512, dtype=np.float32))\n+                continue\n+            resized = _resize_for_arcface(crop)\n+            feat = model.get_feat(resized)\n+            vec = np.asarray(feat, dtype=np.float32)\n+            norm = np.linalg.norm(vec)\n+            if norm > 0:\n+                vec = vec / norm\n+            embeddings.append(vec)\n+        return np.vstack(embeddings)\n+\n+\n+def _resize_for_arcface(image):\n+    import cv2  # type: ignore\n+\n+    target = (112, 112)\n+    resized = cv2.resize(image, target)\n+    return resized\n+\n+\n+def _prepare_face_crop(\n+    image, bbox: list[float], landmarks: list[float] | None, margin: float = 0.15\n+) -> tuple[np.ndarray | None, str | None]:\n+    import numpy as _np\n+\n+    if landmarks and len(landmarks) >= 10:\n+        try:\n+            from insightface.utils import face_align  # type: ignore\n+\n+            pts = _np.asarray(landmarks, dtype=_np.float32).reshape(-1, 2)\n+            aligned = face_align.norm_crop(image, landmark=pts)\n+            return to_u8_bgr(aligned), None\n+        except Exception:\n+            pass\n+    x1, y1, x2, y2 = bbox\n+    width = max(x2 - x1, 1.0)\n+    height = max(y2 - y1, 1.0)\n+    expand_x = width * margin\n+    expand_y = height * margin\n+    expanded_box = [\n+        x1 - expand_x,\n+        y1 - expand_y,\n+        x2 + expand_x,\n+        y2 + expand_y,\n+    ]\n+    crop, _, err = safe_crop(image, expanded_box)\n+    if crop is None:\n+        return None, err or \"crop_failed\"\n+    return crop, None\n+\n+\n+def _make_skip_face_row(\n+    ep_id: str,\n+    track_id: int,\n+    frame_idx: int,\n+    ts_val: float,\n+    bbox: list[float],\n+    detector_choice: str,\n+    reason: str,\n+    *,\n+    crop_rel_path: str | None = None,\n+    crop_s3_key: str | None = None,\n+    thumb_rel_path: str | None = None,\n+    thumb_s3_key: str | None = None,\n+) -> Dict[str, Any]:\n+    row: Dict[str, Any] = {\n+        \"ep_id\": ep_id,\n+        \"face_id\": f\"face_{track_id:04d}_{frame_idx:06d}\",\n+        \"track_id\": track_id,\n+        \"frame_idx\": frame_idx,\n+        \"ts\": ts_val,\n+        \"bbox_xyxy\": bbox,\n+        \"detector\": detector_choice,\n+        \"pipeline_ver\": PIPELINE_VERSION,\n+        \"skip\": reason,\n+    }\n+    if crop_rel_path:\n+        row[\"crop_rel_path\"] = crop_rel_path\n+    if crop_s3_key:\n+        row[\"crop_s3_key\"] = crop_s3_key\n+    if thumb_rel_path:\n+        row[\"thumb_rel_path\"] = thumb_rel_path\n+    if thumb_s3_key:\n+        row[\"thumb_s3_key\"] = thumb_s3_key\n+    return row\n+\n+\n+class TrackRecorder:\n+    \"\"\"Maintains exported track ids, metrics, and sampled boxes.\"\"\"\n+\n+    def __init__(self, *, max_gap: int, remap_ids: bool) -> None:\n+        self.max_gap = max(1, int(max_gap))\n+        self.remap_ids = remap_ids\n+        self._next_export_id = 1\n+        self._mapping: dict[int, dict[str, int]] = {}\n+        self._active_exports: set[int] = set()\n+        self._accumulators: dict[int, TrackAccumulator] = {}\n+        self.metrics = {\"tracks_born\": 0, \"tracks_lost\": 0, \"id_switches\": 0}\n+\n+    def _spawn_export_id(self) -> int:\n+        export_id = self._next_export_id\n+        self._next_export_id += 1\n+        self._active_exports.add(export_id)\n+        self.metrics[\"tracks_born\"] += 1\n+        return export_id\n+\n+    def _complete_track(self, export_id: int) -> None:\n+        if export_id in self._active_exports:\n+            self._active_exports.remove(export_id)\n+            self.metrics[\"tracks_lost\"] += 1\n+\n+    def record(\n+        self,\n+        *,\n+        tracker_track_id: int,\n+        frame_idx: int,\n+        ts: float,\n+        bbox: list[float] | np.ndarray,\n+        class_label: int | str,\n+        landmarks: list[float] | None = None,\n+    ) -> int:\n+        if isinstance(bbox, np.ndarray):\n+            bbox_values = bbox.tolist()\n+        else:\n+            bbox_values = bbox\n+        export_id: int\n+        mapping = self._mapping.get(tracker_track_id)\n+        if self.remap_ids:\n+            start_new = mapping is None\n+            if mapping is not None:\n+                gap = frame_idx - mapping.get(\"last_frame\", frame_idx)\n+                if gap > self.max_gap:\n+                    self.metrics[\"id_switches\"] += 1\n+                    self._complete_track(mapping[\"export_id\"])\n+                    start_new = True\n+            if start_new:\n+                export_id = self._spawn_export_id()\n+            else:\n+                export_id = mapping[\"export_id\"]\n+            self._mapping[tracker_track_id] = {\"export_id\": export_id, \"last_frame\": frame_idx}\n+        else:\n+            if mapping is None:\n+                export_id = tracker_track_id\n+                self._active_exports.add(export_id)\n+                self._mapping[tracker_track_id] = {\"export_id\": export_id, \"last_frame\": frame_idx}\n+                self.metrics[\"tracks_born\"] += 1\n+            else:\n+                export_id = mapping[\"export_id\"]\n+                mapping[\"last_frame\"] = frame_idx\n+        track = self._accumulators.get(export_id)\n+        if track is None:\n+            track = TrackAccumulator(track_id=export_id, class_id=class_label, first_ts=ts, last_ts=ts)\n+            self._accumulators[export_id] = track\n+        track.add(ts, frame_idx, bbox_values, landmarks=landmarks)\n+        return export_id\n+\n+    def finalize(self) -> None:\n+        for export_id in list(self._active_exports):\n+            self._complete_track(export_id)\n+        self._mapping.clear()\n+\n+    def rows(self) -> list[dict]:\n+        payload: list[dict] = []\n+        for track in sorted(self._accumulators.values(), key=lambda item: item.track_id):\n+            payload.append(track.to_row())\n+        return payload\n+\n+    def top_long_tracks(self, limit: int = 5) -> list[dict]:\n+        longest = sorted(self._accumulators.values(), key=lambda item: item.frame_count, reverse=True)[:limit]\n+        return [\n+            {\"track_id\": track.track_id, \"frame_count\": track.frame_count}\n+            for track in longest\n+            if track.frame_count > 0\n+        ]\n+\n+\n+def _bbox_iou(box_a: List[float], box_b: List[float]) -> float:\n+    ax1, ay1, ax2, ay2 = box_a\n+    bx1, by1, bx2, by2 = box_b\n+    inter_x1 = max(ax1, bx1)\n+    inter_y1 = max(ay1, by1)\n+    inter_x2 = min(ax2, bx2)\n+    inter_y2 = min(ay2, by2)\n+    inter_w = max(inter_x2 - inter_x1, 0.0)\n+    inter_h = max(inter_y2 - inter_y1, 0.0)\n+    inter_area = inter_w * inter_h\n+    if inter_area <= 0:\n+        return 0.0\n+    area_a = max(ax2 - ax1, 0.0) * max(ay2 - ay1, 0.0)\n+    area_b = max(bx2 - bx1, 0.0) * max(by2 - by1, 0.0)\n+    denom = area_a + area_b - inter_area\n+    if denom <= 0:\n+        return 0.0\n+    return inter_area / denom\n+\n+\n+def _try_import(module: str):\n+    try:\n+        return __import__(module)\n+    except ImportError:\n+        return None\n+\n+\n+def sanitize_xyxy(x1: float, y1: float, x2: float, y2: float, width: int, height: int) -> tuple[int, int, int, int] | None:\n+    \"\"\"Round + clamp XYXY boxes to integer pixel coordinates, skipping empty windows.\"\"\"\n+    if width <= 0 or height <= 0:\n+        return None\n+    x1_int = int(max(0, min(round(x1), width - 1)))\n+    y1_int = int(max(0, min(round(y1), height - 1)))\n+    x2_int = int(max(0, min(round(x2), width)))\n+    y2_int = int(max(0, min(round(y2), height)))\n+    if x2_int <= x1_int or y2_int <= y1_int:\n+        return None\n+    return x1_int, y1_int, x2_int, y2_int\n+\n+\n+def _image_stats(image) -> tuple[float, float, float]:\n+    arr = np.asarray(image)\n+    if arr.size == 0:\n+        return 0.0, 0.0, 0.0\n+    return float(np.nanmin(arr)), float(np.nanmax(arr)), float(np.nanmean(arr))\n+\n+\n+def _normalize_to_uint8(image: np.ndarray) -> np.ndarray:\n+    arr = np.asarray(image)\n+    if arr.size == 0:\n+        return arr\n+    if arr.dtype == np.uint8:\n+        return arr\n+    if np.issubdtype(arr.dtype, np.floating):\n+        mn = float(np.nanmin(arr))\n+        mx = float(np.nanmax(arr))\n+        if mx <= 1.0 and mn >= 0.0:\n+            arr = (arr * 255.0).round()\n+        elif mn >= -1.0 and mx <= 1.0:\n+            arr = ((arr + 1.0) * 127.5).round()\n+        arr = np.nan_to_num(arr, nan=0.0, posinf=255.0, neginf=0.0)\n+        return np.clip(arr, 0, 255).astype(np.uint8)\n+    if np.issubdtype(arr.dtype, np.integer):\n+        arr = np.clip(arr.astype(np.int64), 0, 255)\n+        return arr.astype(np.uint8)\n+    return arr.astype(np.uint8, copy=False)\n+\n+\n+def save_jpeg(path: str | Path, image, *, quality: int = 85, color: str = \"bgr\") -> None:\n+    \"\"\"Normalize + persist an image to JPEG, ensuring non-blank uint8 BGR data.\"\"\"\n+    import cv2  # type: ignore\n+\n+    arr = np.asarray(image)\n+    if arr.size == 0:\n+        raise ValueError(f\"Cannot save empty image to {path}\")\n+    arr = np.ascontiguousarray(_normalize_to_uint8(arr))\n+    if arr.ndim == 2:\n+        arr = cv2.cvtColor(arr, cv2.COLOR_GRAY2BGR)\n+    elif arr.ndim == 3 and arr.shape[2] == 3:\n+        mode = color.lower()\n+        if mode == \"rgb\":\n+            arr = cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)\n+        elif mode not in {\"bgr\", \"rgb\"}:\n+            raise ValueError(f\"Unsupported color mode '{color}'\")\n+    else:\n+        raise ValueError(f\"Unsupported image shape for JPEG write: {arr.shape}\")\n+    arr = np.ascontiguousarray(arr)\n+    jpeg_quality = max(1, min(int(quality or 85), 100))\n+    out_path = Path(path)\n+    out_path.parent.mkdir(parents=True, exist_ok=True)\n+    ok = cv2.imwrite(str(out_path), arr, [cv2.IMWRITE_JPEG_QUALITY, jpeg_quality])\n+    if not ok:\n+        raise RuntimeError(f\"cv2.imwrite failed for {out_path}\")\n+\n+\n+class ThumbWriter:\n+    def __init__(self, ep_id: str, size: int = 256, jpeg_quality: int = 85) -> None:\n+        self.ep_id = ep_id\n+        self.size = size\n+        self.jpeg_quality = max(1, min(int(jpeg_quality or 85), 100))\n+        self.root_dir = get_path(ep_id, \"frames_root\") / \"thumbs\"\n+        self.root_dir.mkdir(parents=True, exist_ok=True)\n+        self._stat_samples = 0\n+        self._stat_limit = 10\n+        try:\n+            import cv2  # type: ignore\n+\n+            self._cv2 = cv2\n+        except ImportError:\n+            self._cv2 = None\n+\n+    def write(self, image, bbox: List[float], track_id: int, frame_idx: int) -> tuple[str | None, Path | None]:\n+        if self._cv2 is None or image is None:\n+            return None, None\n+        crop, clipped_bbox, err = safe_crop(image, bbox)\n+        if crop is None:\n+            LOGGER.debug(\"Skipping thumb track=%s frame=%s reason=%s\", track_id, frame_idx, err)\n+            return None, None\n+        thumb = self._letterbox(crop)\n+        if self._stat_samples < self._stat_limit:\n+            mn, mx, mean = _image_stats(thumb)\n+            LOGGER.info(\"thumb stats track=%s frame=%s min=%.3f max=%.3f mean=%.3f\", track_id, frame_idx, mn, mx, mean)\n+            if mx - mn < 1e-6:\n+                LOGGER.warning(\"Nearly constant thumb track=%s frame=%s\", track_id, frame_idx)\n+            self._stat_samples += 1\n+        rel_path = Path(f\"track_{track_id:04d}/thumb_{frame_idx:06d}.jpg\")\n+        abs_path = self.root_dir / rel_path\n+        ok, reason = safe_imwrite(abs_path, thumb, self.jpeg_quality)\n+        if not ok:\n+            LOGGER.warning(\"Failed to write thumb %s: %s\", abs_path, reason)\n+            return None, None\n+        return rel_path.as_posix(), abs_path\n+\n+    def _letterbox(self, crop):\n+        if self._cv2 is None:\n+            return np.zeros((self.size, self.size, 3), dtype=np.uint8)\n+        if crop.size == 0:\n+            crop = np.zeros((self.size, self.size, 3), dtype=np.uint8)\n+        h, w = crop.shape[:2]\n+        scale = min(self.size / max(w, 1), self.size / max(h, 1))\n+        new_w = max(int(w * scale), 1)\n+        new_h = max(int(h * scale), 1)\n+        resized = self._cv2.resize(crop, (new_w, new_h))\n+        canvas = np.zeros((self.size, self.size, 3), dtype=np.uint8)\n+        top = (self.size - new_h) // 2\n+        left = (self.size - new_w) // 2\n+        canvas[top : top + new_h, left : left + new_w] = resized\n+        return canvas\n+\n+\n+def _faces_embed_path(ep_id: str) -> Path:\n+    embed_dir = DATA_ROOT / \"embeds\" / ep_id\n+    embed_dir.mkdir(parents=True, exist_ok=True)\n+    return embed_dir / \"faces.npy\"\n+class ProgressEmitter:\n+    \"\"\"Emit structured progress to stdout + optional file for SSE/polling.\"\"\"\n+\n+    VERSION = 2\n+\n+    def __init__(\n+        self,\n+        ep_id: str,\n+        file_path: str | Path | None,\n+        *,\n+        frames_total: int,\n+        secs_total: float | None,\n+        stride: int,\n+        fps_detected: float | None,\n+        fps_requested: float | None,\n+        frame_interval: int | None = None,\n+        run_id: str | None = None,\n+    ) -> None:\n+        import uuid\n+        self.ep_id = ep_id\n+        self.run_id = run_id or str(uuid.uuid4())\n+        self.path = Path(file_path).expanduser() if file_path else None\n+        if self.path:\n+            self.path.parent.mkdir(parents=True, exist_ok=True)\n+        self.frames_total = max(int(frames_total or 0), 0)\n+        self.secs_total = float(secs_total) if secs_total else None\n+        self.stride = max(int(stride), 1)\n+        self.fps_detected = float(fps_detected) if fps_detected else None\n+        self.fps_requested = float(fps_requested) if fps_requested else None\n+        default_interval = PROGRESS_FRAME_STEP\n+        chosen_interval = frame_interval if frame_interval is not None else default_interval\n+        self._frame_interval = max(int(chosen_interval), 1)\n+        self._start_ts = time.time()\n+        self._last_frames = 0\n+        self._last_phase: str | None = None\n+        self._last_step: str | None = None\n+        self._device: str | None = None\n+        self._detector: str | None = None\n+        self._tracker: str | None = None\n+        self._resolved_device: str | None = None\n+        self._closed = False\n+\n+    def _now(self) -> str:\n+        return datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n+\n+    def _should_emit(self, frames_done: int, phase: str, step: str | None, force: bool) -> bool:\n+        if force:\n+            return True\n+        if phase != self._last_phase:\n+            return True\n+        if step != self._last_step:\n+            return True\n+        return (frames_done - self._last_frames) >= self._frame_interval\n+\n+    def _compose_payload(\n+        self,\n+        frames_done: int,\n+        phase: str,\n+        device: str | None,\n+        summary: Dict[str, object] | None,\n+        error: str | None,\n+        detector: str | None,\n+        tracker: str | None,\n+        resolved_device: str | None,\n+        extra: Dict[str, Any] | None = None,\n+    ) -> Dict[str, object]:\n+        secs_done = time.time() - self._start_ts\n+        fps_infer = None\n+        if secs_done > 0 and frames_done >= 0:\n+            fps_infer = frames_done / secs_done\n+        payload: Dict[str, object] = {\n+            \"progress_version\": self.VERSION,\n+            \"ep_id\": self.ep_id,\n+            \"run_id\": self.run_id,\n+            \"phase\": phase,\n+            \"frames_done\": frames_done,\n+            \"frames_total\": self.frames_total,\n+            \"secs_done\": round(float(secs_done), 3),\n+            \"secs_total\": round(float(self.secs_total), 3) if self.secs_total else None,\n+            \"device\": device or self._device,\n+            \"fps_infer\": round(float(fps_infer), 3) if fps_infer else None,\n+            \"fps_detected\": round(float(self.fps_detected), 3) if self.fps_detected else None,\n+            \"fps_requested\": round(float(self.fps_requested), 3) if self.fps_requested else None,\n+            \"stride\": self.stride,\n+            \"updated_at\": self._now(),\n+            \"detector\": detector or self._detector,\n+            \"tracker\": tracker or self._tracker,\n+            \"resolved_device\": resolved_device or self._resolved_device,\n+        }\n+        if summary:\n+            payload[\"summary\"] = summary\n+        if error:\n+            payload[\"error\"] = error\n+        if extra:\n+            payload.update(extra)\n+        return payload\n+\n+    def _write_payload(self, payload: Dict[str, object]) -> None:\n+        line = json.dumps(payload, sort_keys=True)\n+        print(line, flush=True)\n+\n+        # Structured logging for episode-wide grep\n+        phase = payload.get(\"phase\", \"\")\n+        step = payload.get(\"step\", \"\")\n+        frames = payload.get(\"frames_done\", 0)\n+        total = payload.get(\"frames_total\", 0)\n+        vt = payload.get(\"video_time\")\n+        vtotal = payload.get(\"video_total\")\n+        fps = payload.get(\"fps_infer\")\n+        run_id_short = self.run_id[:8] if self.run_id else \"unknown\"\n+\n+        if vt is not None and vtotal is not None:\n+            LOGGER.info(\n+                \"[job=%s run=%s phase=%s step=%s frames=%s/%s vt=%.1f/%.1f fps=%.2f]\",\n+                self.ep_id, run_id_short, phase, step, frames, total, vt, vtotal, fps or 0.0\n+            )\n+        else:\n+            LOGGER.info(\n+                \"[job=%s run=%s phase=%s step=%s frames=%s/%s fps=%.2f]\",\n+                self.ep_id, run_id_short, phase, step, frames, total, fps or 0.0\n+            )\n+\n+        if self.path:\n+            tmp_path = self.path.with_suffix(\".tmp\")\n+            tmp_path.write_text(line, encoding=\"utf-8\")\n+            tmp_path.replace(self.path)\n+\n+    def emit(\n+        self,\n+        frames_done: int,\n+        *,\n+        phase: str,\n+        device: str | None = None,\n+        summary: Dict[str, object] | None = None,\n+        error: str | None = None,\n+        force: bool = False,\n+        detector: str | None = None,\n+        tracker: str | None = None,\n+        resolved_device: str | None = None,\n+        extra: Dict[str, Any] | None = None,\n+        **fields: Any,\n+    ) -> None:\n+        if self._closed:\n+            return\n+        frames_done = max(int(frames_done), 0)\n+        if self.frames_total and frames_done > self.frames_total:\n+            frames_done = self.frames_total\n+\n+        # Extract step from extra dict if present\n+        step = None\n+        if extra and \"step\" in extra:\n+            step = extra.get(\"step\")\n+\n+        if not self._should_emit(frames_done, phase, step, force):\n+            return\n+        if device is not None:\n+            self._device = device\n+        if detector is not None:\n+            self._detector = detector\n+        if tracker is not None:\n+            self._tracker = tracker\n+        if resolved_device is not None:\n+            self._resolved_device = resolved_device\n+        combined_extra: Dict[str, Any] = {} if extra is None else dict(extra)\n+        if fields:\n+            combined_extra.update(fields)\n+        payload = self._compose_payload(\n+            frames_done,\n+            phase,\n+            device,\n+            summary,\n+            error,\n+            detector,\n+            tracker,\n+            resolved_device,\n+            combined_extra or None,\n+        )\n+        self._write_payload(payload)\n+        self._last_frames = frames_done\n+        self._last_phase = phase\n+        self._last_step = step\n+\n+    def complete(\n+        self,\n+        summary: Dict[str, object],\n+        device: str | None = None,\n+        detector: str | None = None,\n+        tracker: str | None = None,\n+        resolved_device: str | None = None,\n+        *,\n+        step: str | None = None,\n+        extra: Dict[str, Any] | None = None,\n+    ) -> None:\n+        final_frames = self.frames_total or summary.get(\"frames_sampled\") or self._last_frames\n+        final_frames = int(final_frames or 0)\n+        completion_extra: Dict[str, Any] = {} if extra is None else dict(extra)\n+        if step:\n+            completion_extra[\"step\"] = step\n+        self.emit(\n+            final_frames,\n+            phase=\"done\",\n+            device=device,\n+            summary=summary,\n+            force=True,\n+            detector=detector,\n+            tracker=tracker,\n+            resolved_device=resolved_device,\n+            extra=completion_extra or None,\n+        )\n+\n+    def fail(self, error: str) -> None:\n+        self.emit(self._last_frames, phase=\"error\", error=error, force=True, tracker=self._tracker)\n+\n+    @property\n+    def target_frames(self) -> int:\n+        return self.frames_total or 0\n+\n+    def close(self) -> None:\n+        self._closed = True\n+\n+\n+def _non_video_phase_meta(step: str | None = None) -> Dict[str, Any]:\n+    meta: Dict[str, Any] = {\"video_time\": None, \"video_total\": None}\n+    if step:\n+        meta[\"step\"] = step\n+    return meta\n+\n+\n+def _video_phase_meta(frames_done: int, frames_total: int | None, fps: float | None, step: str | None = None) -> Dict[str, Any]:\n+    meta: Dict[str, Any] = {}\n+    if fps and fps > 0 and frames_total and frames_total > 0:\n+        video_total = frames_total / fps\n+        video_time = min(frames_done / fps, video_total)\n+        meta[\"video_total\"] = round(video_total, 3)\n+        meta[\"video_time\"] = round(video_time, 3)\n+    else:\n+        meta[\"video_time\"] = None\n+        meta[\"video_total\"] = None\n+    if step:\n+        meta[\"step\"] = step\n+    return meta\n+\n+\n+def _utcnow_iso() -> str:\n+    return datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n+\n+\n+def _write_run_marker(ep_id: str, phase: str, payload: Dict[str, Any]) -> None:\n+    manifests_dir = get_path(ep_id, \"detections\").parent\n+    run_dir = manifests_dir / RUN_MARKERS_SUBDIR\n+    run_dir.mkdir(parents=True, exist_ok=True)\n+    marker_path = run_dir / f\"{phase}.json\"\n+    marker_path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n+\n+\n+class FrameExporter:\n+    \"\"\"Handles optional frame + crop JPEG exports for S3 sync.\"\"\"\n+\n+    def __init__(\n+        self,\n+        ep_id: str,\n+        *,\n+        save_frames: bool,\n+        save_crops: bool,\n+        jpeg_quality: int,\n+        debug_logger: JsonlLogger | NullLogger | None = None,\n+    ) -> None:\n+        self.ep_id = ep_id\n+        self.save_frames = save_frames\n+        self.save_crops = save_crops\n+        self.jpeg_quality = max(1, min(int(jpeg_quality or 85), 100))\n+        self.root_dir = get_path(ep_id, \"frames_root\")\n+        self.frames_dir = self.root_dir / \"frames\"\n+        self.crops_dir = self.root_dir / \"crops\"\n+        if self.save_frames:\n+            self.frames_dir.mkdir(parents=True, exist_ok=True)\n+        if self.save_crops:\n+            self.crops_dir.mkdir(parents=True, exist_ok=True)\n+        self.frames_written = 0\n+        self.crops_written = 0\n+        self._track_indexes: Dict[int, Dict[str, Dict[str, Any]]] = defaultdict(dict)\n+        self._stat_samples = 0\n+        self._stat_limit = 10\n+        self._crop_attempts = 0\n+        self._crop_error_counts: Counter[str] = Counter()\n+        self._fail_fast_threshold = 0.10\n+        self._fail_fast_min_attempts = 10\n+        self._fail_fast_reasons = {\"near_uniform_gray\", \"tiny_file\"}\n+        self.debug_logger = debug_logger\n+\n+    def _log_image_stats(self, kind: str, path: Path, image) -> None:\n+        if self._stat_samples >= self._stat_limit:\n+            return\n+        mn, mx, mean = _image_stats(image)\n+        LOGGER.info(\"%s stats %s min=%.3f max=%.3f mean=%.3f\", kind, path, mn, mx, mean)\n+        if mx - mn < 1e-6:\n+            LOGGER.warning(\"Nearly constant %s %s mn=%.6f mx=%.6f mean=%.6f\", kind, path, mn, mx, mean)\n+        self._stat_samples += 1\n+\n+    def export(self, frame_idx: int, image, crops: List[Tuple[int, List[float]]], ts: float | None = None) -> None:\n+        if not (self.save_frames or self.save_crops):\n+            return\n+        if self.save_frames:\n+            frame_path = self.frames_dir / f\"frame_{frame_idx:06d}.jpg\"\n+            try:\n+                self._log_image_stats(\"frame\", frame_path, image)\n+                save_jpeg(frame_path, image, quality=self.jpeg_quality, color=\"bgr\")\n+                self.frames_written += 1\n+            except Exception as exc:  # pragma: no cover - best effort\n+                LOGGER.warning(\"Failed to save frame %s: %s\", frame_path, exc)\n+        if self.save_crops and crops:\n+            for track_id, bbox in crops:\n+                if track_id is None:\n+                    continue\n+                crop_path = self.crop_abs_path(track_id, frame_idx)\n+                try:\n+                    saved = self._write_crop(image, bbox, crop_path, track_id, frame_idx)\n+                except Exception as exc:  # pragma: no cover - best effort\n+                    LOGGER.warning(\"Failed to save crop %s: %s\", crop_path, exc)\n+                    self._register_crop_attempt(\"exception\")\n+                    saved = False\n+                if saved:\n+                    self.crops_written += 1\n+                    self._record_crop_index(track_id, frame_idx, ts)\n+\n+    def crop_component(self, track_id: int, frame_idx: int) -> str:\n+        return f\"track_{track_id:04d}/frame_{frame_idx:06d}.jpg\"\n+\n+    def crop_rel_path(self, track_id: int, frame_idx: int) -> str:\n+        return f\"crops/{self.crop_component(track_id, frame_idx)}\"\n+\n+    def crop_abs_path(self, track_id: int, frame_idx: int) -> Path:\n+        return self.crops_dir / self.crop_component(track_id, frame_idx)\n+\n+    def _record_crop_index(self, track_id: int, frame_idx: int, ts: float | None) -> None:\n+        if not self.save_crops:\n+            return\n+        key = self.crop_component(track_id, frame_idx)\n+        entry = {\n+            \"key\": key,\n+            \"frame_idx\": int(frame_idx),\n+            \"ts\": round(float(ts), 4) if ts is not None else None,\n+        }\n+        self._track_indexes.setdefault(track_id, {})[key] = entry\n+\n+    def write_indexes(self) -> None:\n+        if not self.save_crops or not self._track_indexes:\n+            return\n+        for track_id, entries in self._track_indexes.items():\n+            if not entries:\n+                continue\n+            track_dir = self.crops_dir / f\"track_{track_id:04d}\"\n+            if not track_dir.exists():\n+                continue\n+            ordered = sorted(entries.values(), key=lambda item: item[\"frame_idx\"])\n+            index_path = track_dir / \"index.json\"\n+            try:\n+                index_path.write_text(json.dumps(ordered, indent=2), encoding=\"utf-8\")\n+            except Exception as exc:  # pragma: no cover - best effort\n+                LOGGER.warning(\"Failed to write crop index %s: %s\", index_path, exc)\n+\n+    def _register_crop_attempt(self, reason: str | None) -> None:\n+        self._crop_attempts += 1\n+        if reason:\n+            self._crop_error_counts[reason] += 1\n+        if reason in self._fail_fast_reasons:\n+            self._maybe_fail_fast()\n+\n+    def _maybe_fail_fast(self) -> None:\n+        if self._crop_attempts < self._fail_fast_min_attempts:\n+            return\n+        bad = sum(self._crop_error_counts.get(reason, 0) for reason in self._fail_fast_reasons)\n+        ratio = bad / max(self._crop_attempts, 1)\n+        if ratio >= self._fail_fast_threshold:\n+            raise RuntimeError(\n+                f\"Too many invalid crops ({bad}/{self._crop_attempts}, {ratio:.1%}); aborting export\"\n+            )\n+\n+    def _write_crop(\n+        self,\n+        image,\n+        bbox: List[float],\n+        crop_path: Path,\n+        track_id: int,\n+        frame_idx: int,\n+    ) -> bool:\n+        start = time.time()\n+        bbox_vals = [float(val) for val in bbox]\n+        crop, clipped_bbox, crop_err = safe_crop(image, bbox_vals)\n+        debug_payload: Dict[str, Any] | None = None\n+        if self.debug_logger:\n+            debug_payload = {\n+                \"track_id\": track_id,\n+                \"frame_idx\": frame_idx,\n+                \"out\": str(crop_path),\n+                \"bbox\": bbox_vals,\n+                \"clipped_bbox\": list(clipped_bbox) if clipped_bbox else None,\n+                \"err_before_save\": crop_err,\n+            }\n+        if crop is None:\n+            self._register_crop_attempt(crop_err or \"no_crop\")\n+            if debug_payload is not None:\n+                debug_payload.update(\n+                    {\n+                        \"save_ok\": False,\n+                        \"save_err\": crop_err or \"no_crop\",\n+                        \"ms\": int((time.time() - start) * 1000),\n+                    }\n+                )\n+                self._emit_debug(debug_payload)\n+            return False\n+\n+        ok, save_err = safe_imwrite(crop_path, crop, self.jpeg_quality)\n+        reason = save_err if not ok else None\n+        self._register_crop_attempt(reason)\n+\n+        if debug_payload is not None:\n+            mn, mx, mean = _image_stats(crop)\n+            debug_payload.update(\n+                {\n+                    \"shape\": tuple(int(x) for x in crop.shape),\n+                    \"dtype\": str(crop.dtype),\n+                    \"min\": mn,\n+                    \"max\": mx,\n+                    \"mean\": mean,\n+                    \"save_ok\": bool(ok),\n+                    \"save_err\": save_err,\n+                    \"file_size\": crop_path.stat().st_size if ok and crop_path.exists() else None,\n+                    \"ms\": int((time.time() - start) * 1000),\n+                }\n+            )\n+            self._emit_debug(debug_payload)\n+\n+        if not ok and save_err:\n+            LOGGER.warning(\"Failed to save crop %s: %s\", crop_path, save_err)\n+        return bool(ok)\n+\n+    def _emit_debug(self, payload: Dict[str, Any]) -> None:\n+        if not self.debug_logger:\n+            return\n+        try:\n+            self.debug_logger(payload)\n+        except Exception:  # pragma: no cover - best effort diagnostics\n+            pass\n+\n+\n+class FrameDecoder:\n+    \"\"\"Random-access video frame reader.\"\"\"\n+\n+    def __init__(self, video_path: Path) -> None:\n+        import cv2  # type: ignore\n+\n+        self._cv2 = cv2\n+        self._cap = cv2.VideoCapture(str(video_path))\n+        if not self._cap.isOpened():\n+            raise FileNotFoundError(f\"Unable to open video {video_path}\")\n+\n+    def read(self, frame_idx: int):\n+        self._cap.set(self._cv2.CAP_PROP_POS_FRAMES, max(int(frame_idx), 0))\n+        ok, frame = self._cap.read()\n+        if not ok:\n+            raise RuntimeError(f\"Failed to decode frame {frame_idx}\")\n+        return frame\n+\n+    def close(self) -> None:\n+        if self._cap is not None:\n+            self._cap.release()\n+            self._cap = None\n+\n+    def __del__(self) -> None:  # pragma: no cover - defensive\n+        try:\n+            self.close()\n+        except Exception:","path":"tools/episode_run.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:13Z","updated_at":"2025-11-11T20:00:22Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565563","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565563"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565563"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565563/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":1519,"original_line":1519,"side":"RIGHT","author_association":"NONE","original_position":1519,"position":1519,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565580","pull_request_review_id":3449784939,"id":2515565580,"node_id":"PRRC_kwDOQRcQs86V8HwM","diff_hunk":"@@ -0,0 +1,1454 @@\n+from __future__ import annotations\n+\n+import json\n+import logging\n+import os\n+import re\n+import sys\n+from collections import defaultdict\n+from datetime import date, datetime\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, List\n+\n+from fastapi import APIRouter, Body, HTTPException, Query\n+from pydantic import BaseModel, Field\n+\n+PROJECT_ROOT = Path(__file__).resolve().parents[3]\n+if str(PROJECT_ROOT) not in sys.path:\n+    sys.path.append(str(PROJECT_ROOT))\n+\n+from py_screenalytics.artifacts import ensure_dirs, get_path\n+\n+from apps.api.services import roster as roster_service\n+from apps.api.services import identities as identity_service\n+from apps.api.services.episodes import EpisodeStore\n+from apps.api.services.storage import (\n+    StorageService,\n+    artifact_prefixes,\n+    delete_local_tree,\n+    delete_s3_prefix,\n+    episode_context_from_id,\n+    v2_artifact_prefixes,\n+)\n+\n+router = APIRouter()\n+EPISODE_STORE = EpisodeStore()\n+STORAGE = StorageService()\n+LOGGER = logging.getLogger(__name__)\n+\n+\n+def _manifests_dir(ep_id: str) -> Path:\n+    return get_path(ep_id, \"detections\").parent\n+\n+\n+def _faces_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"faces.jsonl\"\n+\n+\n+def _faces_ops_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"faces_ops.jsonl\"\n+\n+\n+def _append_face_ops(ep_id: str, entries: Iterable[Dict[str, Any]]) -> None:\n+    entries = list(entries)\n+    if not entries:\n+        return\n+    path = _faces_ops_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    timestamp = datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n+    with path.open(\"a\", encoding=\"utf-8\") as handle:\n+        for entry in entries:\n+            payload = dict(entry)\n+            payload.setdefault(\"ts\", timestamp)\n+            handle.write(json.dumps(payload) + \"\\n\")\n+\n+\n+def _identities_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"identities.json\"\n+\n+\n+def _runs_dir(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"runs\"\n+\n+\n+def _tracks_path(ep_id: str) -> Path:\n+    return get_path(ep_id, \"tracks\")\n+\n+\n+def _thumbs_root(ep_id: str) -> Path:\n+    return get_path(ep_id, \"frames_root\") / \"thumbs\"\n+\n+\n+def _remove_face_assets(ep_id: str, rows: Iterable[Dict[str, Any]]) -> None:\n+    frames_root = get_path(ep_id, \"frames_root\")\n+    thumbs_root = _thumbs_root(ep_id)\n+    for row in rows:\n+        thumb_rel = row.get(\"thumb_rel_path\")\n+        if isinstance(thumb_rel, str):\n+            thumb_file = thumbs_root / thumb_rel\n+            try:\n+                thumb_file.unlink()\n+            except FileNotFoundError:\n+                pass\n+        crop_rel = row.get(\"crop_rel_path\")\n+        if isinstance(crop_rel, str):\n+            crop_file = frames_root / crop_rel\n+            try:\n+                crop_file.unlink()\n+            except FileNotFoundError:\n+                pass\n+\n+\n+def _analytics_root(ep_id: str) -> Path:\n+    data_root = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+    return data_root / \"analytics\" / ep_id\n+\n+\n+def _episode_local_dirs(ep_id: str) -> List[Path]:\n+    dirs = [\n+        get_path(ep_id, \"video\").parent,\n+        get_path(ep_id, \"frames_root\"),\n+        _manifests_dir(ep_id),\n+        _analytics_root(ep_id),\n+    ]\n+    unique: List[Path] = []\n+    seen = set()\n+    for path in dirs:\n+        resolved = path.resolve()\n+        if resolved in seen:\n+            continue\n+        seen.add(resolved)\n+        unique.append(path)\n+    return unique\n+\n+\n+def _safe_int(value) -> int | None:\n+    try:\n+        return int(value)\n+    except (TypeError, ValueError):\n+        return None\n+\n+\n+def _count_nonempty_lines(path: Path) -> int:\n+    if not path.exists():\n+        return 0\n+    try:\n+        with path.open(\"r\", encoding=\"utf-8\") as handle:\n+            return sum(1 for line in handle if line.strip())\n+    except OSError:\n+        return 0\n+\n+\n+def _load_run_marker(ep_id: str, phase: str) -> Dict[str, Any] | None:\n+    marker_path = _runs_dir(ep_id) / f\"{phase}.json\"\n+    if not marker_path.exists():\n+        return None\n+    try:\n+        payload = json.loads(marker_path.read_text(encoding=\"utf-8\"))\n+    except (OSError, json.JSONDecodeError):\n+        return None\n+    return payload if isinstance(payload, dict) else None\n+\n+\n+def _phase_status_from_marker(phase: str, marker: Dict[str, Any]) -> Dict[str, Any]:\n+    status_value = str(marker.get(\"status\") or \"unknown\").lower()\n+    return {\n+        \"phase\": phase,\n+        \"status\": status_value,\n+        \"faces\": _safe_int(marker.get(\"faces\")),\n+        \"identities\": _safe_int(marker.get(\"identities\")),\n+        \"started_at\": marker.get(\"started_at\"),\n+        \"finished_at\": marker.get(\"finished_at\"),\n+        \"version\": marker.get(\"version\"),\n+        \"source\": \"marker\",\n+    }\n+\n+\n+def _faces_phase_status(ep_id: str) -> Dict[str, Any]:\n+    marker = _load_run_marker(ep_id, \"faces_embed\")\n+    if marker:\n+        return _phase_status_from_marker(\"faces_embed\", marker)\n+    faces_path = _faces_path(ep_id)\n+    faces_count = _count_nonempty_lines(faces_path)\n+    status_value = \"success\" if faces_count > 0 else \"missing\"\n+    source = \"output\" if faces_path.exists() else \"absent\"\n+    return {\n+        \"phase\": \"faces_embed\",\n+        \"status\": status_value,\n+        \"faces\": faces_count,\n+        \"identities\": None,\n+        \"started_at\": None,\n+        \"finished_at\": None,\n+        \"version\": None,\n+        \"source\": source,\n+    }\n+\n+\n+def _cluster_phase_status(ep_id: str) -> Dict[str, Any]:\n+    marker = _load_run_marker(ep_id, \"cluster\")\n+    if marker:\n+        return _phase_status_from_marker(\"cluster\", marker)\n+    identities_path = _identities_path(ep_id)\n+    faces_total = 0\n+    identities_count = 0\n+    if identities_path.exists():\n+        try:\n+            payload = json.loads(identities_path.read_text(encoding=\"utf-8\"))\n+        except (OSError, json.JSONDecodeError):\n+            payload = {}\n+        identities = payload.get(\"identities\") if isinstance(payload, dict) else None\n+        if isinstance(identities, list):\n+            identities_count = len(identities)\n+        stats_block = payload.get(\"stats\") if isinstance(payload, dict) else None\n+        if isinstance(stats_block, dict):\n+            faces_total = _safe_int(stats_block.get(\"faces\")) or 0\n+    status_value = \"success\" if identities_count > 0 else \"missing\"\n+    source = \"output\" if identities_path.exists() else \"absent\"\n+    return {\n+        \"phase\": \"cluster\",\n+        \"status\": status_value,\n+        \"faces\": faces_total,\n+        \"identities\": identities_count,\n+        \"started_at\": None,\n+        \"finished_at\": None,\n+        \"version\": None,\n+        \"source\": source,\n+    }\n+\n+\n+def _delete_episode_assets(ep_id: str, options) -> Dict[str, Any]:\n+    record = EPISODE_STORE.get(ep_id)\n+    if not record:\n+        raise HTTPException(status_code=404, detail=\"Episode not found\")\n+    try:\n+        ep_ctx = episode_context_from_id(ep_id)\n+    except ValueError as exc:\n+        raise HTTPException(status_code=400, detail=\"Invalid episode id\") from exc\n+    local_deleted = 0\n+    delete_local = getattr(options, \"delete_local\", True)\n+    if delete_local:\n+        for path in _episode_local_dirs(ep_id):\n+            if not path.exists():\n+                continue\n+            try:\n+                delete_local_tree(path)\n+                local_deleted += 1\n+            except Exception as exc:  # pragma: no cover - best effort cleanup\n+                LOGGER.warning(\"Failed to delete %s: %s\", path, exc)\n+    s3_deleted = 0\n+    include_s3 = bool(getattr(options, \"include_s3\", False) or getattr(options, \"delete_artifacts\", False))\n+    delete_raw = bool(getattr(options, \"delete_raw\", False))\n+    prefixes: Dict[str, str] | None = None\n+    if include_s3 or delete_raw:\n+        prefixes = v2_artifact_prefixes(ep_ctx)\n+    if include_s3 and prefixes:\n+        for key in (\"frames\", \"crops\", \"manifests\", \"analytics\", \"thumbs_tracks\", \"thumbs_identities\"):\n+            prefix = prefixes.get(key)\n+            if prefix:\n+                s3_deleted += delete_s3_prefix(STORAGE.bucket, prefix, storage=STORAGE)\n+    if delete_raw and prefixes:\n+        for key in (\"raw_v2\", \"raw_v1\"):\n+            prefix = prefixes.get(key)\n+            if prefix:\n+                s3_deleted += delete_s3_prefix(STORAGE.bucket, prefix, storage=STORAGE)\n+    removed = EPISODE_STORE.delete(ep_id)\n+    return {\n+        \"ep_id\": ep_id,\n+        \"deleted\": {\"local_dirs\": local_deleted, \"s3_objects\": s3_deleted},\n+        \"removed_from_store\": removed,\n+    }\n+\n+\n+def _delete_all_records(options) -> Dict[str, Any]:\n+    records = EPISODE_STORE.list()\n+    deleted: List[str] = []\n+    totals = {\"local_dirs\": 0, \"s3_objects\": 0}\n+    for record in records:\n+        result = _delete_episode_assets(record.ep_id, options)\n+        deleted.append(result[\"ep_id\"])\n+        totals[\"local_dirs\"] += result[\"deleted\"][\"local_dirs\"]\n+        totals[\"s3_objects\"] += result[\"deleted\"][\"s3_objects\"]\n+    return {\"deleted\": totals, \"episodes\": deleted, \"count\": len(deleted)}\n+\n+\n+FRAME_IDX_RE = re.compile(r\"frame_(\\d+)\\.jpg$\", re.IGNORECASE)\n+TRACK_LIST_MAX_LIMIT = 500\n+\n+\n+def _load_faces(ep_id: str, *, include_skipped: bool = True) -> List[Dict[str, Any]]:\n+    path = _faces_path(ep_id)\n+    if not path.exists():\n+        return []\n+    rows: List[Dict[str, Any]] = []\n+    with path.open(\"r\", encoding=\"utf-8\") as handle:\n+        for line in handle:\n+            line = line.strip()\n+            if not line:\n+                continue\n+            try:\n+                obj = json.loads(line)\n+            except json.JSONDecodeError:\n+                continue\n+            if isinstance(obj, dict):\n+                if not include_skipped and obj.get(\"skip\"):\n+                    continue\n+                rows.append(obj)\n+    return rows\n+\n+\n+def _write_faces(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    path = _faces_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    with path.open(\"w\", encoding=\"utf-8\") as handle:\n+        for row in rows:\n+            handle.write(json.dumps(row) + \"\\n\")\n+    return path\n+\n+\n+def _load_tracks(ep_id: str) -> List[Dict[str, Any]]:\n+    path = _tracks_path(ep_id)\n+    if not path.exists():\n+        return []\n+    rows: List[Dict[str, Any]] = []\n+    with path.open(\"r\", encoding=\"utf-8\") as handle:\n+        for line in handle:\n+            line = line.strip()\n+            if not line:\n+                continue\n+            try:\n+                obj = json.loads(line)\n+            except json.JSONDecodeError:\n+                continue\n+            if isinstance(obj, dict):\n+                rows.append(obj)\n+    return rows\n+\n+\n+def _write_tracks(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    path = _tracks_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    with path.open(\"w\", encoding=\"utf-8\") as handle:\n+        for row in rows:\n+            handle.write(json.dumps(row) + \"\\n\")\n+    return path\n+\n+\n+def _load_identities(ep_id: str) -> Dict[str, Any]:\n+    path = _identities_path(ep_id)\n+    if not path.exists():\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+    try:\n+        return json.loads(path.read_text(encoding=\"utf-8\"))\n+    except json.JSONDecodeError:\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+\n+\n+def _write_identities(ep_id: str, payload: Dict[str, Any]) -> Path:\n+    path = _identities_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n+    return path\n+\n+\n+def _sync_manifests(ep_id: str, *paths: Path) -> None:\n+    try:\n+        ep_ctx = episode_context_from_id(ep_id)\n+    except ValueError:\n+        return\n+    for path in paths:\n+        if path and path.exists():\n+            try:\n+                STORAGE.put_artifact(ep_ctx, \"manifests\", path, path.name)\n+            except Exception:\n+                continue\n+\n+\n+def _identity_lookup(data: Dict[str, Any]) -> Dict[int, str]:\n+    lookup: Dict[int, str] = {}\n+    for identity in data.get(\"identities\", []):\n+        identity_id = identity.get(\"identity_id\")\n+        if not identity_id:\n+            continue\n+        for track_id in identity.get(\"track_ids\", []) or []:\n+            try:\n+                lookup[int(track_id)] = identity_id\n+            except (TypeError, ValueError):\n+                continue\n+    return lookup\n+\n+\n+def _resolve_thumb_url(ep_id: str, rel_path: str | None, s3_key: str | None) -> str | None:\n+    if s3_key:\n+        url = STORAGE.presign_get(s3_key)\n+        if url:\n+            return url\n+    if not rel_path:\n+        return None\n+    local = _thumbs_root(ep_id) / rel_path\n+    if local.exists():\n+        return str(local)\n+    return None\n+\n+\n+def _recount_track_faces(ep_id: str) -> None:\n+    faces = _load_faces(ep_id, include_skipped=False)\n+    counts: Dict[int, int] = defaultdict(int)\n+    for face in faces:\n+        try:\n+            counts[int(face.get(\"track_id\", -1))] += 1\n+        except (TypeError, ValueError):\n+            continue\n+    track_rows = _load_tracks(ep_id)\n+    if not track_rows:\n+        return\n+    for row in track_rows:\n+        try:\n+            track_id = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            track_id = -1\n+        row[\"faces_count\"] = counts.get(track_id, 0)\n+    path = _write_tracks(ep_id, track_rows)\n+    _sync_manifests(ep_id, path)\n+\n+\n+def _update_identity_stats(ep_id: str, payload: Dict[str, Any]) -> None:\n+    faces_count = len(_load_faces(ep_id, include_skipped=False))\n+    payload.setdefault(\"stats\", {})\n+    payload[\"stats\"][\"faces\"] = faces_count\n+    payload[\"stats\"][\"clusters\"] = len(payload.get(\"identities\", []))\n+\n+\n+def _frame_idx_from_name(name: str) -> int | None:\n+    match = FRAME_IDX_RE.search(name)\n+    if not match:\n+        return None\n+    try:\n+        return int(match.group(1))\n+    except ValueError:\n+        return None\n+\n+\n+def _track_face_rows(ep_id: str, track_id: int) -> Dict[int, Dict[str, Any]]:\n+    faces = _load_faces(ep_id, include_skipped=False)\n+    rows: Dict[int, Dict[str, Any]] = {}\n+    for row in faces:\n+        try:\n+            tid = int(row.get(\"track_id\", -1))\n+            frame_idx = int(row.get(\"frame_idx\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        if tid != track_id:\n+            continue\n+        rows.setdefault(frame_idx, row)\n+    return rows\n+\n+\n+def _max_candidate_count(limit: int, offset: int, sample: int) -> int:\n+    limit = max(1, limit)\n+    offset = max(0, offset)\n+    sample = max(1, sample)\n+    return max(1, (offset + limit) * sample)\n+\n+\n+def _apply_sampling(entries: List[Dict[str, Any]], sample: int, offset: int, limit: int) -> List[Dict[str, Any]]:\n+    sample = max(1, sample)\n+    offset = max(0, offset)\n+    limit = max(1, limit)\n+    downsampled = [item for idx, item in enumerate(entries) if idx % sample == 0]\n+    if offset >= len(downsampled):\n+        return []\n+    end = offset + limit if limit else None\n+    return downsampled[offset:end]\n+\n+\n+def _require_episode_context(ep_id: str):\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+    except ValueError as exc:  # pragma: no cover - invalid ids rejected upstream\n+        raise HTTPException(status_code=400, detail=\"Invalid episode id\") from exc\n+    return ctx, artifact_prefixes(ctx)\n+\n+\n+def _media_entry(track_id: int, frame_idx: int, key: str, url: str, meta: Dict[str, Any]) -> Dict[str, Any]:\n+    return {\n+        \"track_id\": track_id,\n+        \"frame_idx\": frame_idx,\n+        \"key\": key,\n+        \"url\": url,\n+        \"w\": meta.get(\"crop_width\") or meta.get(\"width\"),\n+        \"h\": meta.get(\"crop_height\") or meta.get(\"height\"),\n+        \"ts\": meta.get(\"ts\"),\n+    }\n+\n+\n+\n+def _list_track_frame_media(ep_id: str, track_id: int, sample: int, limit: int, offset: int) -> List[Dict[str, Any]]:\n+    sample = max(1, sample)\n+    limit = max(1, min(limit, TRACK_LIST_MAX_LIMIT))\n+    offset = max(0, offset)\n+    face_rows = _track_face_rows(ep_id, track_id)\n+    if not face_rows:\n+        return []\n+    frame_indices = sorted(face_rows.keys())\n+    max_candidates = _max_candidate_count(limit, offset, sample)\n+    frame_indices = frame_indices[:max_candidates]\n+    entries: List[Dict[str, Any]] = []\n+    if STORAGE.backend == \"local\":\n+        frames_dir = get_path(ep_id, \"frames_root\") / \"frames\"\n+        for idx in frame_indices:\n+            path = frames_dir / f\"frame_{idx:06d}.jpg\"\n+            if not path.exists():\n+                continue\n+            path_str = path.as_posix()\n+            meta = face_rows.get(idx, {})\n+            entries.append(_media_entry(track_id, idx, path_str, path_str, meta))\n+    else:\n+        _, prefixes = _require_episode_context(ep_id)\n+        base_prefix = prefixes[\"frames\"]\n+        for idx in frame_indices:\n+            key = f\"{base_prefix}frame_{idx:06d}.jpg\"\n+            url = STORAGE.presign_get(key)\n+            if not url:\n+                continue\n+            meta = face_rows.get(idx, {})\n+            entries.append(_media_entry(track_id, idx, key, url, meta))\n+    entries.sort(key=lambda item: item[\"frame_idx\"])\n+    return _apply_sampling(entries, sample, offset, limit)\n+\n+\n+class EpisodeCreateRequest(BaseModel):\n+    show_slug_or_id: str = Field(..., min_length=1, description=\"Show slug or identifier\")\n+    season_number: int = Field(..., ge=0, le=999, description=\"Season number\")\n+    episode_number: int = Field(..., ge=0, le=999, description=\"Episode number within the season\")\n+    title: str | None = Field(None, max_length=200)\n+    air_date: date | None = None\n+\n+\n+class EpisodeCreateResponse(BaseModel):\n+    ep_id: str\n+\n+\n+class EpisodeSummary(BaseModel):\n+    ep_id: str\n+    show_slug: str\n+    season_number: int\n+    episode_number: int\n+    title: str | None\n+    air_date: str | None\n+\n+\n+class EpisodeListResponse(BaseModel):\n+    episodes: List[EpisodeSummary]\n+\n+\n+class EpisodeUpsert(BaseModel):\n+    ep_id: str = Field(..., min_length=3, description=\"Deterministic ep_id (slug-sXXeYY)\")\n+    show_slug: str = Field(..., min_length=1)\n+    season: int = Field(..., ge=0, le=999)\n+    episode: int = Field(..., ge=0, le=999)\n+    title: str | None = Field(None, max_length=200)\n+    air_date: date | None = None\n+\n+\n+class FaceMoveRequest(BaseModel):\n+    from_track_id: int = Field(..., ge=0)\n+    face_ids: List[str] = Field(..., min_length=1, description=\"Face identifiers to move\")\n+    target_identity_id: str | None = Field(None, description=\"Existing identity to receive frames\")\n+    new_identity_name: str | None = Field(None, description=\"Create a new identity with this name\")\n+    show_id: str | None = Field(None, description=\"Optional show slug for roster updates\")\n+\n+\n+class TrackFrameMoveRequest(BaseModel):\n+    frame_ids: List[int] = Field(..., min_length=1, description=\"Track frame indices to move\")\n+    target_identity_id: str | None = Field(None, description=\"Existing identity target\")\n+    new_identity_name: str | None = Field(None, description=\"Optional new identity name\")\n+    show_id: str | None = Field(None, description=\"Optional show slug override\")\n+\n+\n+class TrackFrameDeleteRequest(BaseModel):\n+    frame_ids: List[int] = Field(..., min_length=1, description=\"Track frame indices to delete\")\n+    delete_assets: bool = True\n+\n+\n+class IdentityRenameRequest(BaseModel):\n+    label: str | None = Field(None, max_length=120)\n+\n+\n+class IdentityNameRequest(BaseModel):\n+    name: str = Field(..., min_length=1, max_length=200)\n+    show: str | None = Field(None, description=\"Optional show slug override\")\n+\n+\n+class IdentityMergeRequest(BaseModel):\n+    source_id: str\n+    target_id: str\n+\n+\n+class TrackMoveRequest(BaseModel):\n+    target_identity_id: str | None = None\n+\n+\n+class TrackDeleteRequest(BaseModel):\n+    delete_faces: bool = True\n+\n+\n+class FrameDeleteRequest(BaseModel):\n+    track_id: int\n+    frame_idx: int\n+    delete_assets: bool = False\n+\n+\n+\n+\n+class DeleteEpisodeIn(BaseModel):\n+    include_s3: bool = False\n+\n+\n+class DeleteAllIn(BaseModel):\n+    confirm: str\n+    include_s3: bool = False\n+\n+\n+class DeleteEpisodeLegacyIn(BaseModel):\n+    delete_artifacts: bool = True\n+    delete_raw: bool = False\n+    delete_local: bool = True\n+\n+\n+class PurgeAllLegacyIn(BaseModel):\n+    confirm: str\n+    delete_artifacts: bool = True\n+    delete_raw: bool = False\n+    delete_local: bool = True\n+\n+\n+class S3VideoItem(BaseModel):\n+    bucket: str\n+    key: str\n+    ep_id: str\n+    show: str | None = None\n+    season: int | None = None\n+    episode: int | None = None\n+    size: int | None = None\n+    last_modified: str | None = None\n+    etag: str | None = None\n+    exists_in_store: bool\n+    key_version: str | None = None\n+\n+\n+class S3VideosResponse(BaseModel):\n+    items: List[S3VideoItem]\n+    count: int\n+\n+\n+class EpisodeS3Status(BaseModel):\n+    bucket: str\n+    v2_key: str | None = None\n+    v2_exists: bool = False\n+    v1_key: str\n+    v1_exists: bool\n+\n+\n+class EpisodeLocalStatus(BaseModel):\n+    path: str\n+    exists: bool\n+\n+\n+class EpisodeDetailResponse(BaseModel):\n+    ep_id: str\n+    show_slug: str\n+    season_number: int\n+    episode_number: int\n+    title: str | None\n+    air_date: str | None\n+    s3: EpisodeS3Status\n+    local: EpisodeLocalStatus\n+\n+\n+class PhaseStatus(BaseModel):\n+    phase: str\n+    status: str\n+    faces: int | None = None\n+    identities: int | None = None\n+    started_at: str | None = None\n+    finished_at: str | None = None\n+    version: str | None = None\n+    source: str | None = None\n+\n+\n+class EpisodeStatusResponse(BaseModel):\n+    ep_id: str\n+    faces_embed: PhaseStatus\n+    cluster: PhaseStatus\n+\n+\n+class AssetUploadResponse(BaseModel):\n+    ep_id: str\n+    method: str\n+    bucket: str\n+    key: str\n+    object_key: str | None = None  # backwards compatibility\n+    upload_url: str | None\n+    expires_in: int | None\n+    headers: Dict[str, str]\n+    path: str | None = None\n+    local_video_path: str\n+    backend: str\n+\n+\n+class EpisodeMirrorResponse(BaseModel):\n+    ep_id: str\n+    local_video_path: str\n+    bytes: int | None = None\n+    etag: str | None = None\n+    used_key_version: str | None = None\n+\n+\n+class EpisodeVideoMeta(BaseModel):\n+    ep_id: str\n+    local_exists: bool\n+    local_video_path: str\n+    width: int | None = None\n+    height: int | None = None\n+    frames: int | None = None\n+    duration_sec: float | None = None\n+    fps_detected: float | None = None\n+\n+\n+class DeleteEpisodeIn(BaseModel):\n+    delete_artifacts: bool = True\n+    delete_raw: bool = False\n+    delete_local: bool = True\n+\n+\n+class PurgeAllIn(BaseModel):\n+    confirm: str\n+    delete_artifacts: bool = True\n+    delete_raw: bool = False\n+    delete_local: bool = True\n+\n+\n+@router.get(\"/episodes\", response_model=EpisodeListResponse, tags=[\"episodes\"])\n+def list_episodes() -> EpisodeListResponse:\n+    records = EPISODE_STORE.list()\n+    episodes = [\n+        EpisodeSummary(\n+            ep_id=record.ep_id,\n+            show_slug=record.show_ref,\n+            season_number=record.season_number,\n+            episode_number=record.episode_number,\n+            title=record.title,\n+            air_date=record.air_date,\n+        )\n+        for record in records\n+    ]\n+    return EpisodeListResponse(episodes=episodes)\n+\n+\n+@router.get(\"/episodes/s3_videos\", response_model=S3VideosResponse, tags=[\"episodes\"])\n+def list_s3_videos(q: str | None = Query(None), limit: int = Query(200, ge=1, le=1000)) -> S3VideosResponse:\n+    raw_items = STORAGE.list_episode_videos_s3(limit=limit)\n+    items: List[S3VideoItem] = []\n+    for obj in raw_items:\n+        ep_id = obj.get(\"ep_id\")\n+        if not isinstance(ep_id, str):\n+            continue\n+        if q and q.lower() not in ep_id.lower():\n+            continue\n+        items.append(\n+            S3VideoItem(\n+                bucket=obj.get(\"bucket\", STORAGE.bucket),\n+                key=str(obj.get(\"key\")),\n+                ep_id=ep_id,\n+                show=obj.get(\"show\"),\n+                season=obj.get(\"season\"),\n+                episode=obj.get(\"episode\"),\n+                size=obj.get(\"size\"),\n+                last_modified=str(obj.get(\"last_modified\")) if obj.get(\"last_modified\") else None,\n+                etag=obj.get(\"etag\"),\n+                exists_in_store=EPISODE_STORE.exists(ep_id),\n+                key_version=obj.get(\"key_version\"),\n+            )\n+        )\n+        if len(items) >= limit:\n+            break\n+    return S3VideosResponse(items=items, count=len(items))\n+\n+\n+@router.post(\"/episodes/{ep_id}/delete\")\n+def delete_episode_new(ep_id: str, body: DeleteEpisodeIn = Body(default=DeleteEpisodeIn())) -> Dict[str, Any]:\n+    return _delete_episode_assets(ep_id, body)\n+\n+\n+@router.post(\"/episodes/delete_all\")\n+def delete_all(body: DeleteAllIn) -> Dict[str, Any]:\n+    if body.confirm.strip() != \"DELETE ALL\":\n+        raise HTTPException(status_code=400, detail=\"Confirmation text mismatch.\")\n+    delete_opts = DeleteEpisodeIn(include_s3=body.include_s3)\n+    return _delete_all_records(delete_opts)\n+\n+\n+@router.delete(\"/episodes/{ep_id}\")\n+def delete_episode(ep_id: str, body: DeleteEpisodeLegacyIn = Body(default=DeleteEpisodeLegacyIn())) -> Dict[str, Any]:\n+    return _delete_episode_assets(ep_id, body)\n+\n+\n+@router.post(\"/episodes/purge_all\")\n+def purge_all(body: PurgeAllLegacyIn) -> Dict[str, Any]:\n+    if body.confirm.strip() != \"DELETE ALL\":\n+        raise HTTPException(status_code=400, detail=\"Confirmation text mismatch.\")\n+    delete_opts = DeleteEpisodeLegacyIn(\n+        delete_artifacts=body.delete_artifacts,\n+        delete_raw=body.delete_raw,\n+        delete_local=body.delete_local,\n+    )\n+    return _delete_all_records(delete_opts)\n+\n+\n+@router.get(\"/episodes/{ep_id}\", response_model=EpisodeDetailResponse, tags=[\"episodes\"])\n+def episode_details(ep_id: str) -> EpisodeDetailResponse:\n+    record = EPISODE_STORE.get(ep_id)\n+    if not record:\n+        raise HTTPException(status_code=404, detail=\"Episode not found\")\n+\n+    local_path = get_path(ep_id, \"video\")\n+    v2_key = STORAGE.video_object_key_v2(record.show_ref, record.season_number, record.episode_number)\n+    v1_key = STORAGE.video_object_key_v1(ep_id)\n+    v2_exists = STORAGE.object_exists(v2_key)\n+    v1_exists = STORAGE.object_exists(v1_key)\n+\n+    return EpisodeDetailResponse(\n+        ep_id=record.ep_id,\n+        show_slug=record.show_ref,\n+        season_number=record.season_number,\n+        episode_number=record.episode_number,\n+        title=record.title,\n+        air_date=record.air_date,\n+        s3=EpisodeS3Status(\n+            bucket=STORAGE.bucket,\n+            v2_key=v2_key,\n+            v2_exists=v2_exists,\n+            v1_key=v1_key,\n+            v1_exists=v1_exists,\n+        ),\n+        local=EpisodeLocalStatus(path=str(local_path), exists=local_path.exists()),\n+    )\n+\n+\n+@router.get(\"/episodes/{ep_id}/progress\", tags=[\"episodes\"])\n+def episode_progress(ep_id: str) -> dict:\n+    progress_path = get_path(ep_id, \"detections\").parent / \"progress.json\"\n+    if not progress_path.exists():\n+        raise HTTPException(status_code=404, detail=\"Progress not available\")\n+    try:\n+        payload = json.loads(progress_path.read_text(encoding=\"utf-8\"))\n+    except json.JSONDecodeError as exc:\n+        raise HTTPException(status_code=503, detail=\"Progress file corrupt\") from exc\n+    return {\"ep_id\": ep_id, \"progress\": payload}\n+\n+\n+@router.get(\"/episodes/{ep_id}/status\", response_model=EpisodeStatusResponse, tags=[\"episodes\"])\n+def episode_run_status(ep_id: str) -> EpisodeStatusResponse:\n+    faces_status = PhaseStatus(**_faces_phase_status(ep_id))\n+    cluster_status = PhaseStatus(**_cluster_phase_status(ep_id))\n+    return EpisodeStatusResponse(ep_id=ep_id, faces_embed=faces_status, cluster=cluster_status)\n+\n+\n+@router.post(\"/episodes\", response_model=EpisodeCreateResponse, tags=[\"episodes\"])\n+def create_episode(payload: EpisodeCreateRequest) -> EpisodeCreateResponse:\n+    record = EPISODE_STORE.upsert(\n+        show_ref=payload.show_slug_or_id,\n+        season_number=payload.season_number,\n+        episode_number=payload.episode_number,\n+        title=payload.title,\n+        air_date=payload.air_date,\n+    )\n+    return EpisodeCreateResponse(ep_id=record.ep_id)\n+\n+\n+@router.post(\"/episodes/upsert_by_id\", tags=[\"episodes\"])\n+def upsert_by_id(payload: EpisodeUpsert) -> dict:\n+    try:\n+        record, created = EPISODE_STORE.upsert_ep_id(\n+            ep_id=payload.ep_id,\n+            show_slug=payload.show_slug,\n+            season=payload.season,\n+            episode=payload.episode,\n+            title=payload.title,\n+            air_date=payload.air_date,\n+        )\n+    except ValueError as exc:\n+        raise HTTPException(status_code=400, detail=str(exc)) from exc\n+    return {\n+        \"ep_id\": record.ep_id,\n+        \"created\": created,\n+        \"show_slug\": record.show_ref,\n+        \"season\": record.season_number,\n+        \"episode\": record.episode_number,\n+        \"title\": record.title,\n+        \"air_date\": record.air_date,\n+    }\n+\n+\n+@router.post(\"/episodes/{ep_id}/assets\", response_model=AssetUploadResponse, tags=[\"episodes\"])\n+def presign_episode_assets(ep_id: str) -> AssetUploadResponse:\n+    record = EPISODE_STORE.get(ep_id)\n+    if not record:\n+        raise HTTPException(status_code=404, detail=\"Episode not found\")\n+\n+    ensure_dirs(ep_id)\n+    v2_key = STORAGE.video_object_key_v2(record.show_ref, record.season_number, record.episode_number)\n+    presigned = STORAGE.presign_episode_video(ep_id, object_key=v2_key)\n+    local_video_path = get_path(ep_id, \"video\")\n+    path = presigned.path or (str(local_video_path) if presigned.method == \"FILE\" else None)\n+\n+    return AssetUploadResponse(\n+        ep_id=presigned.ep_id,\n+        method=presigned.method,\n+        bucket=presigned.bucket,\n+        key=presigned.object_key,\n+        object_key=presigned.object_key,\n+        upload_url=presigned.upload_url,\n+        expires_in=presigned.expires_in,\n+        headers=presigned.headers,\n+        path=path,\n+        local_video_path=str(local_video_path),\n+        backend=presigned.backend,\n+    )\n+\n+\n+@router.post(\n+    \"/episodes/{ep_id}/mirror\",\n+    response_model=EpisodeMirrorResponse,\n+    tags=[\"episodes\"],\n+)\n+def mirror_episode_video(ep_id: str) -> EpisodeMirrorResponse:\n+    record = EPISODE_STORE.get(ep_id)\n+    if not record:\n+        raise HTTPException(status_code=404, detail=\"Episode not found\")\n+\n+    ensure_dirs(ep_id)\n+    try:\n+        result = STORAGE.ensure_local_mirror(\n+            ep_id,\n+            show_ref=record.show_ref,\n+            season_number=record.season_number,\n+            episode_number=record.episode_number,\n+        )\n+    except RuntimeError as exc:\n+        raise HTTPException(status_code=404, detail=str(exc)) from exc\n+    return EpisodeMirrorResponse(\n+        ep_id=ep_id,\n+        local_video_path=str(result.get(\"local_video_path\")),\n+        bytes=result.get(\"bytes\"),\n+        etag=result.get(\"etag\"),\n+        used_key_version=result.get(\"used_key_version\"),\n+    )\n+\n+\n+@router.post(\n+    \"/episodes/{ep_id}/hydrate\",\n+    response_model=EpisodeMirrorResponse,\n+    tags=[\"episodes\"],\n+)\n+def hydrate_episode_video(ep_id: str) -> EpisodeMirrorResponse:\n+    return mirror_episode_video(ep_id)\n+\n+\n+@router.get(\n+    \"/episodes/{ep_id}/video_meta\",\n+    response_model=EpisodeVideoMeta,\n+    tags=[\"episodes\"],\n+)\n+def episode_video_meta(ep_id: str) -> EpisodeVideoMeta:\n+    video_path = get_path(ep_id, \"video\")\n+    if not video_path.exists():\n+        raise HTTPException(status_code=404, detail=\"Local video not found\")\n+\n+    width: int | None = None\n+    height: int | None = None\n+    frames: int | None = None\n+    fps_detected: float | None = None\n+    duration_sec: float | None = None\n+\n+    try:\n+        import cv2  # type: ignore\n+\n+        cap = cv2.VideoCapture(str(video_path))\n+        if cap.isOpened():\n+            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 0) or None\n+            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 0) or None\n+            frames_val = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n+            frames = int(frames_val) if frames_val and frames_val > 0 else None\n+            fps_val = cap.get(cv2.CAP_PROP_FPS)\n+            fps_detected = float(fps_val) if fps_val and fps_val > 0 else None\n+            if frames and fps_detected:\n+                duration_sec = frames / fps_detected\n+        cap.release()\n+    except Exception as exc:  # pragma: no cover - best effort\n+        raise HTTPException(status_code=500, detail=f\"Failed to analyze video: {exc}\") from exc\n+\n+    return EpisodeVideoMeta(\n+        ep_id=ep_id,\n+        local_exists=True,\n+        local_video_path=str(video_path),\n+        width=width,\n+        height=height,\n+        frames=frames,\n+        duration_sec=duration_sec,\n+        fps_detected=fps_detected,\n+    )\n+\n+\n+@router.get(\"/episodes/{ep_id}/identities\")\n+def list_identities(ep_id: str) -> dict:\n+    payload = _load_identities(ep_id)\n+    track_lookup = {int(row.get(\"track_id\", -1)): row for row in _load_tracks(ep_id)}\n+    identities = []\n+    for identity in payload.get(\"identities\", []):\n+        track_ids = []\n+        for raw_tid in identity.get(\"track_ids\", []) or []:\n+            try:\n+                track_ids.append(int(raw_tid))\n+            except (TypeError, ValueError):\n+                continue\n+        faces_total = identity.get(\"size\")\n+        if faces_total is None:\n+            faces_total = sum(int(track_lookup.get(tid, {}).get(\"faces_count\", 0)) for tid in track_ids)\n+        identities.append(\n+            {\n+                \"identity_id\": identity.get(\"identity_id\"),\n+                \"label\": identity.get(\"label\"),\n+                \"name\": identity.get(\"name\"),\n+                \"track_ids\": track_ids,\n+                \"faces\": faces_total,\n+                \"rep_thumbnail_url\": _resolve_thumb_url(\n+                    ep_id,\n+                    identity.get(\"rep_thumb_rel_path\"),\n+                    identity.get(\"rep_thumb_s3_key\"),\n+                ),\n+            }\n+        )\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+        show_slug = ctx.show_slug\n+    except ValueError:\n+        show_slug = None\n+    if show_slug:\n+        for entry in payload.get(\"identities\", []):\n+            name = entry.get(\"name\")\n+            if isinstance(name, str) and name.strip():\n+                try:\n+                    roster_service.add_if_missing(show_slug, name)\n+                except ValueError:","path":"apps/api/routers/episodes.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.\n```suggestion\n                except ValueError:\n                    # Ignore ValueError if the identity already exists in the roster\n```","created_at":"2025-11-11T20:00:13Z","updated_at":"2025-11-11T20:00:22Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565580","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565580"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565580"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565580/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":1043,"original_line":1043,"side":"RIGHT","author_association":"NONE","original_position":1043,"position":1043,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565596","pull_request_review_id":3449784939,"id":2515565596,"node_id":"PRRC_kwDOQRcQs86V8Hwc","diff_hunk":"@@ -0,0 +1,1454 @@\n+from __future__ import annotations\n+\n+import json\n+import logging\n+import os\n+import re\n+import sys\n+from collections import defaultdict\n+from datetime import date, datetime\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, List\n+\n+from fastapi import APIRouter, Body, HTTPException, Query\n+from pydantic import BaseModel, Field\n+\n+PROJECT_ROOT = Path(__file__).resolve().parents[3]\n+if str(PROJECT_ROOT) not in sys.path:\n+    sys.path.append(str(PROJECT_ROOT))\n+\n+from py_screenalytics.artifacts import ensure_dirs, get_path\n+\n+from apps.api.services import roster as roster_service\n+from apps.api.services import identities as identity_service\n+from apps.api.services.episodes import EpisodeStore\n+from apps.api.services.storage import (\n+    StorageService,\n+    artifact_prefixes,\n+    delete_local_tree,\n+    delete_s3_prefix,\n+    episode_context_from_id,\n+    v2_artifact_prefixes,\n+)\n+\n+router = APIRouter()\n+EPISODE_STORE = EpisodeStore()\n+STORAGE = StorageService()\n+LOGGER = logging.getLogger(__name__)\n+\n+\n+def _manifests_dir(ep_id: str) -> Path:\n+    return get_path(ep_id, \"detections\").parent\n+\n+\n+def _faces_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"faces.jsonl\"\n+\n+\n+def _faces_ops_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"faces_ops.jsonl\"\n+\n+\n+def _append_face_ops(ep_id: str, entries: Iterable[Dict[str, Any]]) -> None:\n+    entries = list(entries)\n+    if not entries:\n+        return\n+    path = _faces_ops_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    timestamp = datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n+    with path.open(\"a\", encoding=\"utf-8\") as handle:\n+        for entry in entries:\n+            payload = dict(entry)\n+            payload.setdefault(\"ts\", timestamp)\n+            handle.write(json.dumps(payload) + \"\\n\")\n+\n+\n+def _identities_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"identities.json\"\n+\n+\n+def _runs_dir(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"runs\"\n+\n+\n+def _tracks_path(ep_id: str) -> Path:\n+    return get_path(ep_id, \"tracks\")\n+\n+\n+def _thumbs_root(ep_id: str) -> Path:\n+    return get_path(ep_id, \"frames_root\") / \"thumbs\"\n+\n+\n+def _remove_face_assets(ep_id: str, rows: Iterable[Dict[str, Any]]) -> None:\n+    frames_root = get_path(ep_id, \"frames_root\")\n+    thumbs_root = _thumbs_root(ep_id)\n+    for row in rows:\n+        thumb_rel = row.get(\"thumb_rel_path\")\n+        if isinstance(thumb_rel, str):\n+            thumb_file = thumbs_root / thumb_rel\n+            try:\n+                thumb_file.unlink()\n+            except FileNotFoundError:\n+                pass\n+        crop_rel = row.get(\"crop_rel_path\")\n+        if isinstance(crop_rel, str):\n+            crop_file = frames_root / crop_rel\n+            try:\n+                crop_file.unlink()\n+            except FileNotFoundError:\n+                pass\n+\n+\n+def _analytics_root(ep_id: str) -> Path:\n+    data_root = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+    return data_root / \"analytics\" / ep_id\n+\n+\n+def _episode_local_dirs(ep_id: str) -> List[Path]:\n+    dirs = [\n+        get_path(ep_id, \"video\").parent,\n+        get_path(ep_id, \"frames_root\"),\n+        _manifests_dir(ep_id),\n+        _analytics_root(ep_id),\n+    ]\n+    unique: List[Path] = []\n+    seen = set()\n+    for path in dirs:\n+        resolved = path.resolve()\n+        if resolved in seen:\n+            continue\n+        seen.add(resolved)\n+        unique.append(path)\n+    return unique\n+\n+\n+def _safe_int(value) -> int | None:\n+    try:\n+        return int(value)\n+    except (TypeError, ValueError):\n+        return None\n+\n+\n+def _count_nonempty_lines(path: Path) -> int:\n+    if not path.exists():\n+        return 0\n+    try:\n+        with path.open(\"r\", encoding=\"utf-8\") as handle:\n+            return sum(1 for line in handle if line.strip())\n+    except OSError:\n+        return 0\n+\n+\n+def _load_run_marker(ep_id: str, phase: str) -> Dict[str, Any] | None:\n+    marker_path = _runs_dir(ep_id) / f\"{phase}.json\"\n+    if not marker_path.exists():\n+        return None\n+    try:\n+        payload = json.loads(marker_path.read_text(encoding=\"utf-8\"))\n+    except (OSError, json.JSONDecodeError):\n+        return None\n+    return payload if isinstance(payload, dict) else None\n+\n+\n+def _phase_status_from_marker(phase: str, marker: Dict[str, Any]) -> Dict[str, Any]:\n+    status_value = str(marker.get(\"status\") or \"unknown\").lower()\n+    return {\n+        \"phase\": phase,\n+        \"status\": status_value,\n+        \"faces\": _safe_int(marker.get(\"faces\")),\n+        \"identities\": _safe_int(marker.get(\"identities\")),\n+        \"started_at\": marker.get(\"started_at\"),\n+        \"finished_at\": marker.get(\"finished_at\"),\n+        \"version\": marker.get(\"version\"),\n+        \"source\": \"marker\",\n+    }\n+\n+\n+def _faces_phase_status(ep_id: str) -> Dict[str, Any]:\n+    marker = _load_run_marker(ep_id, \"faces_embed\")\n+    if marker:\n+        return _phase_status_from_marker(\"faces_embed\", marker)\n+    faces_path = _faces_path(ep_id)\n+    faces_count = _count_nonempty_lines(faces_path)\n+    status_value = \"success\" if faces_count > 0 else \"missing\"\n+    source = \"output\" if faces_path.exists() else \"absent\"\n+    return {\n+        \"phase\": \"faces_embed\",\n+        \"status\": status_value,\n+        \"faces\": faces_count,\n+        \"identities\": None,\n+        \"started_at\": None,\n+        \"finished_at\": None,\n+        \"version\": None,\n+        \"source\": source,\n+    }\n+\n+\n+def _cluster_phase_status(ep_id: str) -> Dict[str, Any]:\n+    marker = _load_run_marker(ep_id, \"cluster\")\n+    if marker:\n+        return _phase_status_from_marker(\"cluster\", marker)\n+    identities_path = _identities_path(ep_id)\n+    faces_total = 0\n+    identities_count = 0\n+    if identities_path.exists():\n+        try:\n+            payload = json.loads(identities_path.read_text(encoding=\"utf-8\"))\n+        except (OSError, json.JSONDecodeError):\n+            payload = {}\n+        identities = payload.get(\"identities\") if isinstance(payload, dict) else None\n+        if isinstance(identities, list):\n+            identities_count = len(identities)\n+        stats_block = payload.get(\"stats\") if isinstance(payload, dict) else None\n+        if isinstance(stats_block, dict):\n+            faces_total = _safe_int(stats_block.get(\"faces\")) or 0\n+    status_value = \"success\" if identities_count > 0 else \"missing\"\n+    source = \"output\" if identities_path.exists() else \"absent\"\n+    return {\n+        \"phase\": \"cluster\",\n+        \"status\": status_value,\n+        \"faces\": faces_total,\n+        \"identities\": identities_count,\n+        \"started_at\": None,\n+        \"finished_at\": None,\n+        \"version\": None,\n+        \"source\": source,\n+    }\n+\n+\n+def _delete_episode_assets(ep_id: str, options) -> Dict[str, Any]:\n+    record = EPISODE_STORE.get(ep_id)\n+    if not record:\n+        raise HTTPException(status_code=404, detail=\"Episode not found\")\n+    try:\n+        ep_ctx = episode_context_from_id(ep_id)\n+    except ValueError as exc:\n+        raise HTTPException(status_code=400, detail=\"Invalid episode id\") from exc\n+    local_deleted = 0\n+    delete_local = getattr(options, \"delete_local\", True)\n+    if delete_local:\n+        for path in _episode_local_dirs(ep_id):\n+            if not path.exists():\n+                continue\n+            try:\n+                delete_local_tree(path)\n+                local_deleted += 1\n+            except Exception as exc:  # pragma: no cover - best effort cleanup\n+                LOGGER.warning(\"Failed to delete %s: %s\", path, exc)\n+    s3_deleted = 0\n+    include_s3 = bool(getattr(options, \"include_s3\", False) or getattr(options, \"delete_artifacts\", False))\n+    delete_raw = bool(getattr(options, \"delete_raw\", False))\n+    prefixes: Dict[str, str] | None = None\n+    if include_s3 or delete_raw:\n+        prefixes = v2_artifact_prefixes(ep_ctx)\n+    if include_s3 and prefixes:\n+        for key in (\"frames\", \"crops\", \"manifests\", \"analytics\", \"thumbs_tracks\", \"thumbs_identities\"):\n+            prefix = prefixes.get(key)\n+            if prefix:\n+                s3_deleted += delete_s3_prefix(STORAGE.bucket, prefix, storage=STORAGE)\n+    if delete_raw and prefixes:\n+        for key in (\"raw_v2\", \"raw_v1\"):\n+            prefix = prefixes.get(key)\n+            if prefix:\n+                s3_deleted += delete_s3_prefix(STORAGE.bucket, prefix, storage=STORAGE)\n+    removed = EPISODE_STORE.delete(ep_id)\n+    return {\n+        \"ep_id\": ep_id,\n+        \"deleted\": {\"local_dirs\": local_deleted, \"s3_objects\": s3_deleted},\n+        \"removed_from_store\": removed,\n+    }\n+\n+\n+def _delete_all_records(options) -> Dict[str, Any]:\n+    records = EPISODE_STORE.list()\n+    deleted: List[str] = []\n+    totals = {\"local_dirs\": 0, \"s3_objects\": 0}\n+    for record in records:\n+        result = _delete_episode_assets(record.ep_id, options)\n+        deleted.append(result[\"ep_id\"])\n+        totals[\"local_dirs\"] += result[\"deleted\"][\"local_dirs\"]\n+        totals[\"s3_objects\"] += result[\"deleted\"][\"s3_objects\"]\n+    return {\"deleted\": totals, \"episodes\": deleted, \"count\": len(deleted)}\n+\n+\n+FRAME_IDX_RE = re.compile(r\"frame_(\\d+)\\.jpg$\", re.IGNORECASE)\n+TRACK_LIST_MAX_LIMIT = 500\n+\n+\n+def _load_faces(ep_id: str, *, include_skipped: bool = True) -> List[Dict[str, Any]]:\n+    path = _faces_path(ep_id)\n+    if not path.exists():\n+        return []\n+    rows: List[Dict[str, Any]] = []\n+    with path.open(\"r\", encoding=\"utf-8\") as handle:\n+        for line in handle:\n+            line = line.strip()\n+            if not line:\n+                continue\n+            try:\n+                obj = json.loads(line)\n+            except json.JSONDecodeError:\n+                continue\n+            if isinstance(obj, dict):\n+                if not include_skipped and obj.get(\"skip\"):\n+                    continue\n+                rows.append(obj)\n+    return rows\n+\n+\n+def _write_faces(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    path = _faces_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    with path.open(\"w\", encoding=\"utf-8\") as handle:\n+        for row in rows:\n+            handle.write(json.dumps(row) + \"\\n\")\n+    return path\n+\n+\n+def _load_tracks(ep_id: str) -> List[Dict[str, Any]]:\n+    path = _tracks_path(ep_id)\n+    if not path.exists():\n+        return []\n+    rows: List[Dict[str, Any]] = []\n+    with path.open(\"r\", encoding=\"utf-8\") as handle:\n+        for line in handle:\n+            line = line.strip()\n+            if not line:\n+                continue\n+            try:\n+                obj = json.loads(line)\n+            except json.JSONDecodeError:\n+                continue\n+            if isinstance(obj, dict):\n+                rows.append(obj)\n+    return rows\n+\n+\n+def _write_tracks(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    path = _tracks_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    with path.open(\"w\", encoding=\"utf-8\") as handle:\n+        for row in rows:\n+            handle.write(json.dumps(row) + \"\\n\")\n+    return path\n+\n+\n+def _load_identities(ep_id: str) -> Dict[str, Any]:\n+    path = _identities_path(ep_id)\n+    if not path.exists():\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+    try:\n+        return json.loads(path.read_text(encoding=\"utf-8\"))\n+    except json.JSONDecodeError:\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+\n+\n+def _write_identities(ep_id: str, payload: Dict[str, Any]) -> Path:\n+    path = _identities_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n+    return path\n+\n+\n+def _sync_manifests(ep_id: str, *paths: Path) -> None:\n+    try:\n+        ep_ctx = episode_context_from_id(ep_id)\n+    except ValueError:\n+        return\n+    for path in paths:\n+        if path and path.exists():\n+            try:\n+                STORAGE.put_artifact(ep_ctx, \"manifests\", path, path.name)\n+            except Exception:\n+                continue\n+\n+\n+def _identity_lookup(data: Dict[str, Any]) -> Dict[int, str]:\n+    lookup: Dict[int, str] = {}\n+    for identity in data.get(\"identities\", []):\n+        identity_id = identity.get(\"identity_id\")\n+        if not identity_id:\n+            continue\n+        for track_id in identity.get(\"track_ids\", []) or []:\n+            try:\n+                lookup[int(track_id)] = identity_id\n+            except (TypeError, ValueError):\n+                continue\n+    return lookup\n+\n+\n+def _resolve_thumb_url(ep_id: str, rel_path: str | None, s3_key: str | None) -> str | None:\n+    if s3_key:\n+        url = STORAGE.presign_get(s3_key)\n+        if url:\n+            return url\n+    if not rel_path:\n+        return None\n+    local = _thumbs_root(ep_id) / rel_path\n+    if local.exists():\n+        return str(local)\n+    return None\n+\n+\n+def _recount_track_faces(ep_id: str) -> None:\n+    faces = _load_faces(ep_id, include_skipped=False)\n+    counts: Dict[int, int] = defaultdict(int)\n+    for face in faces:\n+        try:\n+            counts[int(face.get(\"track_id\", -1))] += 1\n+        except (TypeError, ValueError):\n+            continue\n+    track_rows = _load_tracks(ep_id)\n+    if not track_rows:\n+        return\n+    for row in track_rows:\n+        try:\n+            track_id = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            track_id = -1\n+        row[\"faces_count\"] = counts.get(track_id, 0)\n+    path = _write_tracks(ep_id, track_rows)\n+    _sync_manifests(ep_id, path)\n+\n+\n+def _update_identity_stats(ep_id: str, payload: Dict[str, Any]) -> None:\n+    faces_count = len(_load_faces(ep_id, include_skipped=False))\n+    payload.setdefault(\"stats\", {})\n+    payload[\"stats\"][\"faces\"] = faces_count\n+    payload[\"stats\"][\"clusters\"] = len(payload.get(\"identities\", []))\n+\n+\n+def _frame_idx_from_name(name: str) -> int | None:\n+    match = FRAME_IDX_RE.search(name)\n+    if not match:\n+        return None\n+    try:\n+        return int(match.group(1))\n+    except ValueError:\n+        return None\n+\n+\n+def _track_face_rows(ep_id: str, track_id: int) -> Dict[int, Dict[str, Any]]:\n+    faces = _load_faces(ep_id, include_skipped=False)\n+    rows: Dict[int, Dict[str, Any]] = {}\n+    for row in faces:\n+        try:\n+            tid = int(row.get(\"track_id\", -1))\n+            frame_idx = int(row.get(\"frame_idx\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        if tid != track_id:\n+            continue\n+        rows.setdefault(frame_idx, row)\n+    return rows\n+\n+\n+def _max_candidate_count(limit: int, offset: int, sample: int) -> int:\n+    limit = max(1, limit)\n+    offset = max(0, offset)\n+    sample = max(1, sample)\n+    return max(1, (offset + limit) * sample)\n+\n+\n+def _apply_sampling(entries: List[Dict[str, Any]], sample: int, offset: int, limit: int) -> List[Dict[str, Any]]:\n+    sample = max(1, sample)\n+    offset = max(0, offset)\n+    limit = max(1, limit)\n+    downsampled = [item for idx, item in enumerate(entries) if idx % sample == 0]\n+    if offset >= len(downsampled):\n+        return []\n+    end = offset + limit if limit else None\n+    return downsampled[offset:end]\n+\n+\n+def _require_episode_context(ep_id: str):\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+    except ValueError as exc:  # pragma: no cover - invalid ids rejected upstream\n+        raise HTTPException(status_code=400, detail=\"Invalid episode id\") from exc\n+    return ctx, artifact_prefixes(ctx)\n+\n+\n+def _media_entry(track_id: int, frame_idx: int, key: str, url: str, meta: Dict[str, Any]) -> Dict[str, Any]:\n+    return {\n+        \"track_id\": track_id,\n+        \"frame_idx\": frame_idx,\n+        \"key\": key,\n+        \"url\": url,\n+        \"w\": meta.get(\"crop_width\") or meta.get(\"width\"),\n+        \"h\": meta.get(\"crop_height\") or meta.get(\"height\"),\n+        \"ts\": meta.get(\"ts\"),\n+    }\n+\n+\n+\n+def _list_track_frame_media(ep_id: str, track_id: int, sample: int, limit: int, offset: int) -> List[Dict[str, Any]]:\n+    sample = max(1, sample)\n+    limit = max(1, min(limit, TRACK_LIST_MAX_LIMIT))\n+    offset = max(0, offset)\n+    face_rows = _track_face_rows(ep_id, track_id)\n+    if not face_rows:\n+        return []\n+    frame_indices = sorted(face_rows.keys())\n+    max_candidates = _max_candidate_count(limit, offset, sample)\n+    frame_indices = frame_indices[:max_candidates]\n+    entries: List[Dict[str, Any]] = []\n+    if STORAGE.backend == \"local\":\n+        frames_dir = get_path(ep_id, \"frames_root\") / \"frames\"\n+        for idx in frame_indices:\n+            path = frames_dir / f\"frame_{idx:06d}.jpg\"\n+            if not path.exists():\n+                continue\n+            path_str = path.as_posix()\n+            meta = face_rows.get(idx, {})\n+            entries.append(_media_entry(track_id, idx, path_str, path_str, meta))\n+    else:\n+        _, prefixes = _require_episode_context(ep_id)\n+        base_prefix = prefixes[\"frames\"]\n+        for idx in frame_indices:\n+            key = f\"{base_prefix}frame_{idx:06d}.jpg\"\n+            url = STORAGE.presign_get(key)\n+            if not url:\n+                continue\n+            meta = face_rows.get(idx, {})\n+            entries.append(_media_entry(track_id, idx, key, url, meta))\n+    entries.sort(key=lambda item: item[\"frame_idx\"])\n+    return _apply_sampling(entries, sample, offset, limit)\n+\n+\n+class EpisodeCreateRequest(BaseModel):\n+    show_slug_or_id: str = Field(..., min_length=1, description=\"Show slug or identifier\")\n+    season_number: int = Field(..., ge=0, le=999, description=\"Season number\")\n+    episode_number: int = Field(..., ge=0, le=999, description=\"Episode number within the season\")\n+    title: str | None = Field(None, max_length=200)\n+    air_date: date | None = None\n+\n+\n+class EpisodeCreateResponse(BaseModel):\n+    ep_id: str\n+\n+\n+class EpisodeSummary(BaseModel):\n+    ep_id: str\n+    show_slug: str\n+    season_number: int\n+    episode_number: int\n+    title: str | None\n+    air_date: str | None\n+\n+\n+class EpisodeListResponse(BaseModel):\n+    episodes: List[EpisodeSummary]\n+\n+\n+class EpisodeUpsert(BaseModel):\n+    ep_id: str = Field(..., min_length=3, description=\"Deterministic ep_id (slug-sXXeYY)\")\n+    show_slug: str = Field(..., min_length=1)\n+    season: int = Field(..., ge=0, le=999)\n+    episode: int = Field(..., ge=0, le=999)\n+    title: str | None = Field(None, max_length=200)\n+    air_date: date | None = None\n+\n+\n+class FaceMoveRequest(BaseModel):\n+    from_track_id: int = Field(..., ge=0)\n+    face_ids: List[str] = Field(..., min_length=1, description=\"Face identifiers to move\")\n+    target_identity_id: str | None = Field(None, description=\"Existing identity to receive frames\")\n+    new_identity_name: str | None = Field(None, description=\"Create a new identity with this name\")\n+    show_id: str | None = Field(None, description=\"Optional show slug for roster updates\")\n+\n+\n+class TrackFrameMoveRequest(BaseModel):\n+    frame_ids: List[int] = Field(..., min_length=1, description=\"Track frame indices to move\")\n+    target_identity_id: str | None = Field(None, description=\"Existing identity target\")\n+    new_identity_name: str | None = Field(None, description=\"Optional new identity name\")\n+    show_id: str | None = Field(None, description=\"Optional show slug override\")\n+\n+\n+class TrackFrameDeleteRequest(BaseModel):\n+    frame_ids: List[int] = Field(..., min_length=1, description=\"Track frame indices to delete\")\n+    delete_assets: bool = True\n+\n+\n+class IdentityRenameRequest(BaseModel):\n+    label: str | None = Field(None, max_length=120)\n+\n+\n+class IdentityNameRequest(BaseModel):\n+    name: str = Field(..., min_length=1, max_length=200)\n+    show: str | None = Field(None, description=\"Optional show slug override\")\n+\n+\n+class IdentityMergeRequest(BaseModel):\n+    source_id: str\n+    target_id: str\n+\n+\n+class TrackMoveRequest(BaseModel):\n+    target_identity_id: str | None = None\n+\n+\n+class TrackDeleteRequest(BaseModel):\n+    delete_faces: bool = True\n+\n+\n+class FrameDeleteRequest(BaseModel):\n+    track_id: int\n+    frame_idx: int\n+    delete_assets: bool = False\n+\n+\n+\n+\n+class DeleteEpisodeIn(BaseModel):\n+    include_s3: bool = False\n+\n+\n+class DeleteAllIn(BaseModel):\n+    confirm: str\n+    include_s3: bool = False\n+\n+\n+class DeleteEpisodeLegacyIn(BaseModel):\n+    delete_artifacts: bool = True\n+    delete_raw: bool = False\n+    delete_local: bool = True\n+\n+\n+class PurgeAllLegacyIn(BaseModel):\n+    confirm: str\n+    delete_artifacts: bool = True\n+    delete_raw: bool = False\n+    delete_local: bool = True\n+\n+\n+class S3VideoItem(BaseModel):\n+    bucket: str\n+    key: str\n+    ep_id: str\n+    show: str | None = None\n+    season: int | None = None\n+    episode: int | None = None\n+    size: int | None = None\n+    last_modified: str | None = None\n+    etag: str | None = None\n+    exists_in_store: bool\n+    key_version: str | None = None\n+\n+\n+class S3VideosResponse(BaseModel):\n+    items: List[S3VideoItem]\n+    count: int\n+\n+\n+class EpisodeS3Status(BaseModel):\n+    bucket: str\n+    v2_key: str | None = None\n+    v2_exists: bool = False\n+    v1_key: str\n+    v1_exists: bool\n+\n+\n+class EpisodeLocalStatus(BaseModel):\n+    path: str\n+    exists: bool\n+\n+\n+class EpisodeDetailResponse(BaseModel):\n+    ep_id: str\n+    show_slug: str\n+    season_number: int\n+    episode_number: int\n+    title: str | None\n+    air_date: str | None\n+    s3: EpisodeS3Status\n+    local: EpisodeLocalStatus\n+\n+\n+class PhaseStatus(BaseModel):\n+    phase: str\n+    status: str\n+    faces: int | None = None\n+    identities: int | None = None\n+    started_at: str | None = None\n+    finished_at: str | None = None\n+    version: str | None = None\n+    source: str | None = None\n+\n+\n+class EpisodeStatusResponse(BaseModel):\n+    ep_id: str\n+    faces_embed: PhaseStatus\n+    cluster: PhaseStatus\n+\n+\n+class AssetUploadResponse(BaseModel):\n+    ep_id: str\n+    method: str\n+    bucket: str\n+    key: str\n+    object_key: str | None = None  # backwards compatibility\n+    upload_url: str | None\n+    expires_in: int | None\n+    headers: Dict[str, str]\n+    path: str | None = None\n+    local_video_path: str\n+    backend: str\n+\n+\n+class EpisodeMirrorResponse(BaseModel):\n+    ep_id: str\n+    local_video_path: str\n+    bytes: int | None = None\n+    etag: str | None = None\n+    used_key_version: str | None = None\n+\n+\n+class EpisodeVideoMeta(BaseModel):\n+    ep_id: str\n+    local_exists: bool\n+    local_video_path: str\n+    width: int | None = None\n+    height: int | None = None\n+    frames: int | None = None\n+    duration_sec: float | None = None\n+    fps_detected: float | None = None\n+\n+\n+class DeleteEpisodeIn(BaseModel):\n+    delete_artifacts: bool = True\n+    delete_raw: bool = False\n+    delete_local: bool = True\n+\n+\n+class PurgeAllIn(BaseModel):\n+    confirm: str\n+    delete_artifacts: bool = True\n+    delete_raw: bool = False\n+    delete_local: bool = True\n+\n+\n+@router.get(\"/episodes\", response_model=EpisodeListResponse, tags=[\"episodes\"])\n+def list_episodes() -> EpisodeListResponse:\n+    records = EPISODE_STORE.list()\n+    episodes = [\n+        EpisodeSummary(\n+            ep_id=record.ep_id,\n+            show_slug=record.show_ref,\n+            season_number=record.season_number,\n+            episode_number=record.episode_number,\n+            title=record.title,\n+            air_date=record.air_date,\n+        )\n+        for record in records\n+    ]\n+    return EpisodeListResponse(episodes=episodes)\n+\n+\n+@router.get(\"/episodes/s3_videos\", response_model=S3VideosResponse, tags=[\"episodes\"])\n+def list_s3_videos(q: str | None = Query(None), limit: int = Query(200, ge=1, le=1000)) -> S3VideosResponse:\n+    raw_items = STORAGE.list_episode_videos_s3(limit=limit)\n+    items: List[S3VideoItem] = []\n+    for obj in raw_items:\n+        ep_id = obj.get(\"ep_id\")\n+        if not isinstance(ep_id, str):\n+            continue\n+        if q and q.lower() not in ep_id.lower():\n+            continue\n+        items.append(\n+            S3VideoItem(\n+                bucket=obj.get(\"bucket\", STORAGE.bucket),\n+                key=str(obj.get(\"key\")),\n+                ep_id=ep_id,\n+                show=obj.get(\"show\"),\n+                season=obj.get(\"season\"),\n+                episode=obj.get(\"episode\"),\n+                size=obj.get(\"size\"),\n+                last_modified=str(obj.get(\"last_modified\")) if obj.get(\"last_modified\") else None,\n+                etag=obj.get(\"etag\"),\n+                exists_in_store=EPISODE_STORE.exists(ep_id),\n+                key_version=obj.get(\"key_version\"),\n+            )\n+        )\n+        if len(items) >= limit:\n+            break\n+    return S3VideosResponse(items=items, count=len(items))\n+\n+\n+@router.post(\"/episodes/{ep_id}/delete\")\n+def delete_episode_new(ep_id: str, body: DeleteEpisodeIn = Body(default=DeleteEpisodeIn())) -> Dict[str, Any]:\n+    return _delete_episode_assets(ep_id, body)\n+\n+\n+@router.post(\"/episodes/delete_all\")\n+def delete_all(body: DeleteAllIn) -> Dict[str, Any]:\n+    if body.confirm.strip() != \"DELETE ALL\":\n+        raise HTTPException(status_code=400, detail=\"Confirmation text mismatch.\")\n+    delete_opts = DeleteEpisodeIn(include_s3=body.include_s3)\n+    return _delete_all_records(delete_opts)\n+\n+\n+@router.delete(\"/episodes/{ep_id}\")\n+def delete_episode(ep_id: str, body: DeleteEpisodeLegacyIn = Body(default=DeleteEpisodeLegacyIn())) -> Dict[str, Any]:\n+    return _delete_episode_assets(ep_id, body)\n+\n+\n+@router.post(\"/episodes/purge_all\")\n+def purge_all(body: PurgeAllLegacyIn) -> Dict[str, Any]:\n+    if body.confirm.strip() != \"DELETE ALL\":\n+        raise HTTPException(status_code=400, detail=\"Confirmation text mismatch.\")\n+    delete_opts = DeleteEpisodeLegacyIn(\n+        delete_artifacts=body.delete_artifacts,\n+        delete_raw=body.delete_raw,\n+        delete_local=body.delete_local,\n+    )\n+    return _delete_all_records(delete_opts)\n+\n+\n+@router.get(\"/episodes/{ep_id}\", response_model=EpisodeDetailResponse, tags=[\"episodes\"])\n+def episode_details(ep_id: str) -> EpisodeDetailResponse:\n+    record = EPISODE_STORE.get(ep_id)\n+    if not record:\n+        raise HTTPException(status_code=404, detail=\"Episode not found\")\n+\n+    local_path = get_path(ep_id, \"video\")\n+    v2_key = STORAGE.video_object_key_v2(record.show_ref, record.season_number, record.episode_number)\n+    v1_key = STORAGE.video_object_key_v1(ep_id)\n+    v2_exists = STORAGE.object_exists(v2_key)\n+    v1_exists = STORAGE.object_exists(v1_key)\n+\n+    return EpisodeDetailResponse(\n+        ep_id=record.ep_id,\n+        show_slug=record.show_ref,\n+        season_number=record.season_number,\n+        episode_number=record.episode_number,\n+        title=record.title,\n+        air_date=record.air_date,\n+        s3=EpisodeS3Status(\n+            bucket=STORAGE.bucket,\n+            v2_key=v2_key,\n+            v2_exists=v2_exists,\n+            v1_key=v1_key,\n+            v1_exists=v1_exists,\n+        ),\n+        local=EpisodeLocalStatus(path=str(local_path), exists=local_path.exists()),\n+    )\n+\n+\n+@router.get(\"/episodes/{ep_id}/progress\", tags=[\"episodes\"])\n+def episode_progress(ep_id: str) -> dict:\n+    progress_path = get_path(ep_id, \"detections\").parent / \"progress.json\"\n+    if not progress_path.exists():\n+        raise HTTPException(status_code=404, detail=\"Progress not available\")\n+    try:\n+        payload = json.loads(progress_path.read_text(encoding=\"utf-8\"))\n+    except json.JSONDecodeError as exc:\n+        raise HTTPException(status_code=503, detail=\"Progress file corrupt\") from exc\n+    return {\"ep_id\": ep_id, \"progress\": payload}\n+\n+\n+@router.get(\"/episodes/{ep_id}/status\", response_model=EpisodeStatusResponse, tags=[\"episodes\"])\n+def episode_run_status(ep_id: str) -> EpisodeStatusResponse:\n+    faces_status = PhaseStatus(**_faces_phase_status(ep_id))\n+    cluster_status = PhaseStatus(**_cluster_phase_status(ep_id))\n+    return EpisodeStatusResponse(ep_id=ep_id, faces_embed=faces_status, cluster=cluster_status)\n+\n+\n+@router.post(\"/episodes\", response_model=EpisodeCreateResponse, tags=[\"episodes\"])\n+def create_episode(payload: EpisodeCreateRequest) -> EpisodeCreateResponse:\n+    record = EPISODE_STORE.upsert(\n+        show_ref=payload.show_slug_or_id,\n+        season_number=payload.season_number,\n+        episode_number=payload.episode_number,\n+        title=payload.title,\n+        air_date=payload.air_date,\n+    )\n+    return EpisodeCreateResponse(ep_id=record.ep_id)\n+\n+\n+@router.post(\"/episodes/upsert_by_id\", tags=[\"episodes\"])\n+def upsert_by_id(payload: EpisodeUpsert) -> dict:\n+    try:\n+        record, created = EPISODE_STORE.upsert_ep_id(\n+            ep_id=payload.ep_id,\n+            show_slug=payload.show_slug,\n+            season=payload.season,\n+            episode=payload.episode,\n+            title=payload.title,\n+            air_date=payload.air_date,\n+        )\n+    except ValueError as exc:\n+        raise HTTPException(status_code=400, detail=str(exc)) from exc\n+    return {\n+        \"ep_id\": record.ep_id,\n+        \"created\": created,\n+        \"show_slug\": record.show_ref,\n+        \"season\": record.season_number,\n+        \"episode\": record.episode_number,\n+        \"title\": record.title,\n+        \"air_date\": record.air_date,\n+    }\n+\n+\n+@router.post(\"/episodes/{ep_id}/assets\", response_model=AssetUploadResponse, tags=[\"episodes\"])\n+def presign_episode_assets(ep_id: str) -> AssetUploadResponse:\n+    record = EPISODE_STORE.get(ep_id)\n+    if not record:\n+        raise HTTPException(status_code=404, detail=\"Episode not found\")\n+\n+    ensure_dirs(ep_id)\n+    v2_key = STORAGE.video_object_key_v2(record.show_ref, record.season_number, record.episode_number)\n+    presigned = STORAGE.presign_episode_video(ep_id, object_key=v2_key)\n+    local_video_path = get_path(ep_id, \"video\")\n+    path = presigned.path or (str(local_video_path) if presigned.method == \"FILE\" else None)\n+\n+    return AssetUploadResponse(\n+        ep_id=presigned.ep_id,\n+        method=presigned.method,\n+        bucket=presigned.bucket,\n+        key=presigned.object_key,\n+        object_key=presigned.object_key,\n+        upload_url=presigned.upload_url,\n+        expires_in=presigned.expires_in,\n+        headers=presigned.headers,\n+        path=path,\n+        local_video_path=str(local_video_path),\n+        backend=presigned.backend,\n+    )\n+\n+\n+@router.post(\n+    \"/episodes/{ep_id}/mirror\",\n+    response_model=EpisodeMirrorResponse,\n+    tags=[\"episodes\"],\n+)\n+def mirror_episode_video(ep_id: str) -> EpisodeMirrorResponse:\n+    record = EPISODE_STORE.get(ep_id)\n+    if not record:\n+        raise HTTPException(status_code=404, detail=\"Episode not found\")\n+\n+    ensure_dirs(ep_id)\n+    try:\n+        result = STORAGE.ensure_local_mirror(\n+            ep_id,\n+            show_ref=record.show_ref,\n+            season_number=record.season_number,\n+            episode_number=record.episode_number,\n+        )\n+    except RuntimeError as exc:\n+        raise HTTPException(status_code=404, detail=str(exc)) from exc\n+    return EpisodeMirrorResponse(\n+        ep_id=ep_id,\n+        local_video_path=str(result.get(\"local_video_path\")),\n+        bytes=result.get(\"bytes\"),\n+        etag=result.get(\"etag\"),\n+        used_key_version=result.get(\"used_key_version\"),\n+    )\n+\n+\n+@router.post(\n+    \"/episodes/{ep_id}/hydrate\",\n+    response_model=EpisodeMirrorResponse,\n+    tags=[\"episodes\"],\n+)\n+def hydrate_episode_video(ep_id: str) -> EpisodeMirrorResponse:\n+    return mirror_episode_video(ep_id)\n+\n+\n+@router.get(\n+    \"/episodes/{ep_id}/video_meta\",\n+    response_model=EpisodeVideoMeta,\n+    tags=[\"episodes\"],\n+)\n+def episode_video_meta(ep_id: str) -> EpisodeVideoMeta:\n+    video_path = get_path(ep_id, \"video\")\n+    if not video_path.exists():\n+        raise HTTPException(status_code=404, detail=\"Local video not found\")\n+\n+    width: int | None = None\n+    height: int | None = None\n+    frames: int | None = None\n+    fps_detected: float | None = None\n+    duration_sec: float | None = None\n+\n+    try:\n+        import cv2  # type: ignore\n+\n+        cap = cv2.VideoCapture(str(video_path))\n+        if cap.isOpened():\n+            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 0) or None\n+            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 0) or None\n+            frames_val = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n+            frames = int(frames_val) if frames_val and frames_val > 0 else None\n+            fps_val = cap.get(cv2.CAP_PROP_FPS)\n+            fps_detected = float(fps_val) if fps_val and fps_val > 0 else None\n+            if frames and fps_detected:\n+                duration_sec = frames / fps_detected\n+        cap.release()\n+    except Exception as exc:  # pragma: no cover - best effort\n+        raise HTTPException(status_code=500, detail=f\"Failed to analyze video: {exc}\") from exc\n+\n+    return EpisodeVideoMeta(\n+        ep_id=ep_id,\n+        local_exists=True,\n+        local_video_path=str(video_path),\n+        width=width,\n+        height=height,\n+        frames=frames,\n+        duration_sec=duration_sec,\n+        fps_detected=fps_detected,\n+    )\n+\n+\n+@router.get(\"/episodes/{ep_id}/identities\")\n+def list_identities(ep_id: str) -> dict:\n+    payload = _load_identities(ep_id)\n+    track_lookup = {int(row.get(\"track_id\", -1)): row for row in _load_tracks(ep_id)}\n+    identities = []\n+    for identity in payload.get(\"identities\", []):\n+        track_ids = []\n+        for raw_tid in identity.get(\"track_ids\", []) or []:\n+            try:\n+                track_ids.append(int(raw_tid))\n+            except (TypeError, ValueError):\n+                continue\n+        faces_total = identity.get(\"size\")\n+        if faces_total is None:\n+            faces_total = sum(int(track_lookup.get(tid, {}).get(\"faces_count\", 0)) for tid in track_ids)\n+        identities.append(\n+            {\n+                \"identity_id\": identity.get(\"identity_id\"),\n+                \"label\": identity.get(\"label\"),\n+                \"name\": identity.get(\"name\"),\n+                \"track_ids\": track_ids,\n+                \"faces\": faces_total,\n+                \"rep_thumbnail_url\": _resolve_thumb_url(\n+                    ep_id,\n+                    identity.get(\"rep_thumb_rel_path\"),\n+                    identity.get(\"rep_thumb_s3_key\"),\n+                ),\n+            }\n+        )\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+        show_slug = ctx.show_slug\n+    except ValueError:\n+        show_slug = None\n+    if show_slug:\n+        for entry in payload.get(\"identities\", []):\n+            name = entry.get(\"name\")\n+            if isinstance(name, str) and name.strip():\n+                try:\n+                    roster_service.add_if_missing(show_slug, name)\n+                except ValueError:\n+                    pass\n+    return {\"identities\": identities, \"stats\": payload.get(\"stats\", {})}\n+\n+\n+@router.get(\"/episodes/{ep_id}/cluster_tracks\")\n+def list_cluster_tracks(\n+    ep_id: str,\n+    limit_per_cluster: int | None = Query(None, ge=1, description=\"Optional max tracks per cluster\"),\n+) -> dict:\n+    try:\n+        return identity_service.cluster_track_summary(ep_id, limit_per_cluster=limit_per_cluster)\n+    except ValueError as exc:\n+        raise HTTPException(status_code=400, detail=str(exc)) from exc\n+\n+\n+@router.get(\"/episodes/{ep_id}/faces_grid\")\n+def faces_grid(ep_id: str, track_id: int | None = Query(None)) -> dict:\n+    faces = _load_faces(ep_id, include_skipped=False)\n+    identity_lookup = _identity_lookup(_load_identities(ep_id))\n+    items: List[dict] = []\n+    for row in faces:\n+        try:\n+            tid = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        if track_id is not None and tid != track_id:\n+            continue\n+        items.append(\n+            {\n+                \"face_id\": row.get(\"face_id\"),\n+                \"track_id\": tid,\n+                \"frame_idx\": row.get(\"frame_idx\"),\n+                \"ts\": row.get(\"ts\"),\n+                \"thumbnail_url\": _resolve_thumb_url(ep_id, row.get(\"thumb_rel_path\"), row.get(\"thumb_s3_key\")),\n+                \"identity_id\": identity_lookup.get(tid),\n+            }\n+        )\n+    return {\"faces\": items, \"count\": len(items)}\n+\n+\n+@router.get(\"/episodes/{ep_id}/identities/{identity_id}\")\n+def identity_detail(ep_id: str, identity_id: str) -> dict:\n+    payload = _load_identities(ep_id)\n+    identity = next((item for item in payload.get(\"identities\", []) if item.get(\"identity_id\") == identity_id), None)\n+    if not identity:\n+        raise HTTPException(status_code=404, detail=\"Identity not found\")\n+    track_lookup = {int(row.get(\"track_id\", -1)): row for row in _load_tracks(ep_id)}\n+    tracks_payload = []\n+    for raw_tid in identity.get(\"track_ids\", []) or []:\n+        try:\n+            tid = int(raw_tid)\n+        except (TypeError, ValueError):\n+            continue\n+        track_row = track_lookup.get(tid, {})\n+        tracks_payload.append(\n+            {\n+                \"track_id\": tid,\n+                \"faces_count\": track_row.get(\"faces_count\", 0),\n+                \"thumbnail_url\": _resolve_thumb_url(ep_id, track_row.get(\"thumb_rel_path\"), track_row.get(\"thumb_s3_key\")),\n+            }\n+        )\n+    return {\n+        \"identity\": {\n+            \"identity_id\": identity_id,\n+            \"label\": identity.get(\"label\"),\n+            \"name\": identity.get(\"name\"),\n+            \"track_ids\": identity.get(\"track_ids\", []),\n+            \"rep_thumbnail_url\": _resolve_thumb_url(\n+                ep_id,\n+                identity.get(\"rep_thumb_rel_path\"),\n+                identity.get(\"rep_thumb_s3_key\"),\n+            ),\n+        },\n+        \"tracks\": tracks_payload,\n+    }\n+\n+\n+@router.get(\"/episodes/{ep_id}/tracks/{track_id}\")\n+def track_detail(ep_id: str, track_id: int) -> dict:\n+    faces = [row for row in _load_faces(ep_id, include_skipped=False) if int(row.get(\"track_id\", -1)) == track_id]\n+    frames = [\n+        {\n+            \"face_id\": row.get(\"face_id\"),\n+            \"frame_idx\": row.get(\"frame_idx\"),\n+            \"ts\": row.get(\"ts\"),\n+            \"thumbnail_url\": _resolve_thumb_url(ep_id, row.get(\"thumb_rel_path\"), row.get(\"thumb_s3_key\")),\n+            \"skip\": row.get(\"skip\"),\n+        }\n+        for row in faces\n+    ]\n+    track_row = next((row for row in _load_tracks(ep_id) if int(row.get(\"track_id\", -1)) == track_id), None)\n+    return {\n+        \"track_id\": track_id,\n+        \"faces_count\": len(frames),\n+        \"thumbnail_url\": _resolve_thumb_url(\n+            ep_id,\n+            (track_row or {}).get(\"thumb_rel_path\"),\n+            (track_row or {}).get(\"thumb_s3_key\"),\n+        ),\n+        \"frames\": frames,\n+    }\n+\n+\n+@router.get(\"/episodes/{ep_id}/tracks/{track_id}/crops\")\n+def list_track_crops(\n+    ep_id: str,\n+    track_id: int,\n+    sample: int = Query(5, ge=1, le=100, description=\"Return every Nth crop\"),\n+    limit: int = Query(200, ge=1, le=TRACK_LIST_MAX_LIMIT),\n+    start_after: str | None = Query(None, description=\"Opaque cursor returned by the previous call\"),\n+) -> Dict[str, Any]:\n+    ctx, _ = _require_episode_context(ep_id)\n+    payload = STORAGE.list_track_crops(ctx, track_id, sample=sample, max_keys=limit, start_after=start_after)\n+    face_rows = _track_face_rows(ep_id, track_id)\n+    items = payload.get(\"items\", []) if isinstance(payload, dict) else []\n+    for item in items:\n+        frame_idx = item.get(\"frame_idx\")\n+        try:\n+            frame_int = int(frame_idx)\n+        except (TypeError, ValueError):\n+            continue\n+        meta = face_rows.get(frame_int, {})\n+        if meta:\n+            if \"w\" not in item and \"crop_width\" in meta:\n+                item[\"w\"] = meta.get(\"crop_width\") or meta.get(\"width\")\n+            if \"h\" not in item and \"crop_height\" in meta:\n+                item[\"h\"] = meta.get(\"crop_height\") or meta.get(\"height\")\n+            item.setdefault(\"ts\", meta.get(\"ts\"))\n+    return payload\n+\n+\n+@router.get(\"/episodes/{ep_id}/tracks/{track_id}/frames\")\n+def list_track_frames(\n+    ep_id: str,\n+    track_id: int,\n+    sample: int = Query(5, ge=1, le=100, description=\"Return every Nth frame\"),\n+    limit: int = Query(200, ge=1, le=TRACK_LIST_MAX_LIMIT),\n+    offset: int = Query(0, ge=0),\n+) -> List[Dict[str, Any]]:\n+    return _list_track_frame_media(ep_id, track_id, sample, limit, offset)\n+\n+\n+@router.post(\"/episodes/{ep_id}/tracks/{track_id}/frames/move\")\n+def move_track_frames(ep_id: str, track_id: int, body: TrackFrameMoveRequest) -> dict:\n+    frame_ids = sorted({int(idx) for idx in body.frame_ids or []})\n+    if not frame_ids:\n+        raise HTTPException(status_code=400, detail=\"frame_ids_required\")\n+    face_rows = _track_face_rows(ep_id, track_id)\n+    if not face_rows:\n+        raise HTTPException(status_code=404, detail=\"track_not_found\")\n+    selected_faces: List[str] = []\n+    ops: List[Dict[str, Any]] = []\n+    for frame_idx in frame_ids:\n+        row = face_rows.get(frame_idx)\n+        if not row:\n+            raise HTTPException(status_code=404, detail=f\"frame_not_found:{frame_idx}\")\n+        face_id = row.get(\"face_id\")\n+        if not face_id:\n+            raise HTTPException(status_code=400, detail=f\"face_id_missing:{frame_idx}\")\n+        selected_faces.append(str(face_id))\n+        ops.append({\"frame_idx\": frame_idx, \"face_id\": face_id})\n+    try:\n+        result = identity_service.move_frames(\n+            ep_id,\n+            track_id,\n+            selected_faces,\n+            target_identity_id=body.target_identity_id,\n+            new_identity_name=body.new_identity_name,\n+            show_id=body.show_id,\n+        )\n+    except ValueError as exc:\n+        raise HTTPException(status_code=400, detail=str(exc)) from exc\n+    _append_face_ops(\n+        ep_id,\n+        [\n+            {\n+                \"op\": \"move_frame\",\n+                \"frame_idx\": entry[\"frame_idx\"],\n+                \"face_id\": entry[\"face_id\"],\n+                \"source_track_id\": track_id,\n+                \"target_track_id\": result.get(\"new_track_id\"),\n+                \"target_identity_id\": result.get(\"target_identity_id\") or body.target_identity_id,\n+            }\n+            for entry in ops\n+        ],\n+    )\n+    return {\n+        \"moved\": len(selected_faces),\n+        \"frame_ids\": frame_ids,\n+        \"new_track_id\": result.get(\"new_track_id\"),\n+        \"target_identity_id\": result.get(\"target_identity_id\"),\n+        \"target_name\": result.get(\"target_name\"),\n+        \"clusters\": result.get(\"clusters\"),\n+    }\n+\n+\n+@router.delete(\"/episodes/{ep_id}/tracks/{track_id}/frames\")\n+def delete_track_frames(ep_id: str, track_id: int, body: TrackFrameDeleteRequest) -> dict:\n+    frame_ids = sorted({int(idx) for idx in body.frame_ids or []})\n+    if not frame_ids:\n+        raise HTTPException(status_code=400, detail=\"frame_ids_required\")\n+    faces = _load_faces(ep_id)\n+    removed: List[Dict[str, Any]] = []\n+    kept: List[Dict[str, Any]] = []\n+    target_set = set(frame_ids)\n+    for row in faces:\n+        try:\n+            tid = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            tid = -1\n+        frame_idx = row.get(\"frame_idx\")\n+        try:\n+            frame_val = int(frame_idx)\n+        except (TypeError, ValueError):\n+            frame_val = None\n+        if tid == track_id and frame_val in target_set:\n+            removed.append(row)\n+        else:\n+            kept.append(row)\n+    if not removed:\n+        raise HTTPException(status_code=404, detail=\"frames_not_found\")\n+    faces_path = _write_faces(ep_id, kept)\n+    if body.delete_assets:\n+        _remove_face_assets(ep_id, removed)\n+    _append_face_ops(\n+        ep_id,\n+        [\n+            {\n+                \"op\": \"delete_frame\",\n+                \"track_id\": track_id,\n+                \"frame_idx\": int(row.get(\"frame_idx\", -1)),\n+                \"face_id\": row.get(\"face_id\"),\n+            }\n+            for row in removed\n+        ],\n+    )\n+    _recount_track_faces(ep_id)\n+    identities = _load_identities(ep_id)\n+    _update_identity_stats(ep_id, identities)\n+    identities_path = _write_identities(ep_id, identities)\n+    _sync_manifests(ep_id, faces_path, identities_path)\n+    return {\n+        \"track_id\": track_id,\n+        \"deleted\": len(removed),\n+        \"remaining\": len(kept),\n+    }\n+\n+\n+@router.post(\"/episodes/{ep_id}/identities/{identity_id}/rename\")\n+def rename_identity(ep_id: str, identity_id: str, body: IdentityRenameRequest) -> dict:\n+    payload = _load_identities(ep_id)\n+    identity = next((item for item in payload.get(\"identities\", []) if item.get(\"identity_id\") == identity_id), None)\n+    if not identity:\n+        raise HTTPException(status_code=404, detail=\"Identity not found\")\n+    label = (body.label or \"\").strip()\n+    identity[\"label\"] = label or None\n+    _update_identity_stats(ep_id, payload)\n+    path = _write_identities(ep_id, payload)\n+    _sync_manifests(ep_id, path)\n+    return {\"identity_id\": identity_id, \"label\": identity[\"label\"]}\n+\n+\n+@router.post(\"/episodes/{ep_id}/identities/{identity_id}/name\")\n+def assign_identity_name(ep_id: str, identity_id: str, body: IdentityNameRequest) -> dict:\n+    try:\n+        return identity_service.assign_identity_name(ep_id, identity_id, body.name, body.show)\n+    except ValueError as exc:\n+        raise HTTPException(status_code=400, detail=str(exc)) from exc\n+\n+\n+@router.post(\"/episodes/{ep_id}/identities/merge\")\n+def merge_identities(ep_id: str, body: IdentityMergeRequest) -> dict:\n+    payload = _load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    source = next((item for item in identities if item.get(\"identity_id\") == body.source_id), None)\n+    target = next((item for item in identities if item.get(\"identity_id\") == body.target_id), None)\n+    if not source or not target:\n+        raise HTTPException(status_code=404, detail=\"Source or target identity not found\")\n+    merged_track_ids = set(target.get(\"track_ids\", []) or [])\n+    for tid in source.get(\"track_ids\", []) or []:\n+        merged_track_ids.add(tid)\n+    target[\"track_ids\"] = sorted({int(x) for x in merged_track_ids})\n+    payload[\"identities\"] = [item for item in identities if item.get(\"identity_id\") != body.source_id]\n+    _update_identity_stats(ep_id, payload)\n+    path = _write_identities(ep_id, payload)\n+    _sync_manifests(ep_id, path)\n+    return {\"target_id\": body.target_id, \"track_ids\": target[\"track_ids\"]}\n+\n+\n+@router.post(\"/episodes/{ep_id}/tracks/{track_id}/move\")\n+def move_track(ep_id: str, track_id: int, body: TrackMoveRequest) -> dict:\n+    payload = _load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    source_identity = None\n+    target_identity = None\n+    for identity in identities:\n+        if body.target_identity_id and identity.get(\"identity_id\") == body.target_identity_id:\n+            target_identity = identity\n+        if track_id in identity.get(\"track_ids\", []):\n+            source_identity = identity\n+    if body.target_identity_id and target_identity is None:\n+        raise HTTPException(status_code=404, detail=\"Target identity not found\")\n+    if source_identity and track_id in source_identity.get(\"track_ids\", []):\n+        source_identity[\"track_ids\"] = [tid for tid in source_identity[\"track_ids\"] if tid != track_id]\n+    if target_identity is not None:\n+        if track_id not in target_identity.get(\"track_ids\", []):\n+            target_identity.setdefault(\"track_ids\", []).append(track_id)\n+            target_identity[\"track_ids\"] = sorted(target_identity[\"track_ids\"])\n+    _update_identity_stats(ep_id, payload)\n+    path = _write_identities(ep_id, payload)\n+    _sync_manifests(ep_id, path)\n+    return {\n+        \"identity_id\": body.target_identity_id,\n+        \"track_ids\": target_identity[\"track_ids\"] if target_identity else [],\n+    }\n+\n+\n+@router.post(\"/episodes/{ep_id}/faces/move_frames\")\n+def move_faces(ep_id: str, body: FaceMoveRequest) -> dict:\n+    try:\n+        return identity_service.move_frames(\n+            ep_id,\n+            body.from_track_id,\n+            body.face_ids,\n+            target_identity_id=body.target_identity_id,\n+            new_identity_name=body.new_identity_name,\n+            show_id=body.show_id,\n+        )\n+    except ValueError as exc:\n+        raise HTTPException(status_code=400, detail=str(exc)) from exc\n+\n+\n+\n+\n+@router.delete(\"/episodes/{ep_id}/identities/{identity_id}\")\n+def delete_identity(ep_id: str, identity_id: str) -> dict:\n+    payload = _load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    before = len(identities)\n+    payload[\"identities\"] = [item for item in identities if item.get(\"identity_id\") != identity_id]\n+    if len(payload[\"identities\"]) == before:\n+        raise HTTPException(status_code=404, detail=\"Identity not found\")\n+    _update_identity_stats(ep_id, payload)\n+    path = _write_identities(ep_id, payload)\n+    _sync_manifests(ep_id, path)\n+    return {\"deleted\": identity_id, \"remaining\": len(payload[\"identities\"])}\n+\n+\n+@router.delete(\"/episodes/{ep_id}/tracks/{track_id}\")\n+def delete_track(ep_id: str, track_id: int, payload: TrackDeleteRequest = Body(default=TrackDeleteRequest())) -> dict:\n+    faces = _load_faces(ep_id)\n+    if payload.delete_faces:\n+        faces = [row for row in faces if int(row.get(\"track_id\", -1)) != track_id]\n+        faces_path = _write_faces(ep_id, faces)\n+    else:\n+        faces_path = _faces_path(ep_id)\n+    track_rows = _load_tracks(ep_id)\n+    kept_tracks = [row for row in track_rows if int(row.get(\"track_id\", -1)) != track_id]\n+    if len(kept_tracks) == len(track_rows):\n+        raise HTTPException(status_code=404, detail=\"Track not found\")\n+    tracks_path = _write_tracks(ep_id, kept_tracks)\n+    identities = _load_identities(ep_id)\n+    for identity in identities.get(\"identities\", []):\n+        identity[\"track_ids\"] = [tid for tid in identity.get(\"track_ids\", []) if tid != track_id]\n+    _update_identity_stats(ep_id, identities)\n+    identities_path = _write_identities(ep_id, identities)\n+    _recount_track_faces(ep_id)\n+    _sync_manifests(ep_id, faces_path, tracks_path, identities_path)\n+    return {\"track_id\": track_id, \"faces_deleted\": payload.delete_faces}\n+\n+\n+@router.delete(\"/episodes/{ep_id}/frames\")\n+def delete_frame(ep_id: str, payload: FrameDeleteRequest) -> dict:\n+    faces = _load_faces(ep_id)\n+    removed_rows = [\n+        row\n+        for row in faces\n+        if int(row.get(\"track_id\", -1)) == payload.track_id and int(row.get(\"frame_idx\", -1)) == payload.frame_idx\n+    ]\n+    if not removed_rows:\n+        raise HTTPException(status_code=404, detail=\"Face frame not found\")\n+    faces = [row for row in faces if row not in removed_rows]\n+    faces_path = _write_faces(ep_id, faces)\n+    if payload.delete_assets:\n+        frames_root = get_path(ep_id, \"frames_root\")\n+        for row in removed_rows:\n+            thumb_rel = row.get(\"thumb_rel_path\")\n+            if isinstance(thumb_rel, str):\n+                thumb_file = _thumbs_root(ep_id) / thumb_rel\n+                try:\n+                    thumb_file.unlink()\n+                except FileNotFoundError:","path":"apps/api/routers/episodes.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:13Z","updated_at":"2025-11-11T20:00:22Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565596","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565596"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565596"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565596/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":1435,"original_line":1435,"side":"RIGHT","author_association":"NONE","original_position":1435,"position":1435,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565611","pull_request_review_id":3449784939,"id":2515565611,"node_id":"PRRC_kwDOQRcQs86V8Hwr","diff_hunk":"@@ -0,0 +1,1454 @@\n+from __future__ import annotations\n+\n+import json\n+import logging\n+import os\n+import re\n+import sys\n+from collections import defaultdict\n+from datetime import date, datetime\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, List\n+\n+from fastapi import APIRouter, Body, HTTPException, Query\n+from pydantic import BaseModel, Field\n+\n+PROJECT_ROOT = Path(__file__).resolve().parents[3]\n+if str(PROJECT_ROOT) not in sys.path:\n+    sys.path.append(str(PROJECT_ROOT))\n+\n+from py_screenalytics.artifacts import ensure_dirs, get_path\n+\n+from apps.api.services import roster as roster_service\n+from apps.api.services import identities as identity_service\n+from apps.api.services.episodes import EpisodeStore\n+from apps.api.services.storage import (\n+    StorageService,\n+    artifact_prefixes,\n+    delete_local_tree,\n+    delete_s3_prefix,\n+    episode_context_from_id,\n+    v2_artifact_prefixes,\n+)\n+\n+router = APIRouter()\n+EPISODE_STORE = EpisodeStore()\n+STORAGE = StorageService()\n+LOGGER = logging.getLogger(__name__)\n+\n+\n+def _manifests_dir(ep_id: str) -> Path:\n+    return get_path(ep_id, \"detections\").parent\n+\n+\n+def _faces_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"faces.jsonl\"\n+\n+\n+def _faces_ops_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"faces_ops.jsonl\"\n+\n+\n+def _append_face_ops(ep_id: str, entries: Iterable[Dict[str, Any]]) -> None:\n+    entries = list(entries)\n+    if not entries:\n+        return\n+    path = _faces_ops_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    timestamp = datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n+    with path.open(\"a\", encoding=\"utf-8\") as handle:\n+        for entry in entries:\n+            payload = dict(entry)\n+            payload.setdefault(\"ts\", timestamp)\n+            handle.write(json.dumps(payload) + \"\\n\")\n+\n+\n+def _identities_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"identities.json\"\n+\n+\n+def _runs_dir(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"runs\"\n+\n+\n+def _tracks_path(ep_id: str) -> Path:\n+    return get_path(ep_id, \"tracks\")\n+\n+\n+def _thumbs_root(ep_id: str) -> Path:\n+    return get_path(ep_id, \"frames_root\") / \"thumbs\"\n+\n+\n+def _remove_face_assets(ep_id: str, rows: Iterable[Dict[str, Any]]) -> None:\n+    frames_root = get_path(ep_id, \"frames_root\")\n+    thumbs_root = _thumbs_root(ep_id)\n+    for row in rows:\n+        thumb_rel = row.get(\"thumb_rel_path\")\n+        if isinstance(thumb_rel, str):\n+            thumb_file = thumbs_root / thumb_rel\n+            try:\n+                thumb_file.unlink()\n+            except FileNotFoundError:\n+                pass\n+        crop_rel = row.get(\"crop_rel_path\")\n+        if isinstance(crop_rel, str):\n+            crop_file = frames_root / crop_rel\n+            try:\n+                crop_file.unlink()\n+            except FileNotFoundError:\n+                pass\n+\n+\n+def _analytics_root(ep_id: str) -> Path:\n+    data_root = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+    return data_root / \"analytics\" / ep_id\n+\n+\n+def _episode_local_dirs(ep_id: str) -> List[Path]:\n+    dirs = [\n+        get_path(ep_id, \"video\").parent,\n+        get_path(ep_id, \"frames_root\"),\n+        _manifests_dir(ep_id),\n+        _analytics_root(ep_id),\n+    ]\n+    unique: List[Path] = []\n+    seen = set()\n+    for path in dirs:\n+        resolved = path.resolve()\n+        if resolved in seen:\n+            continue\n+        seen.add(resolved)\n+        unique.append(path)\n+    return unique\n+\n+\n+def _safe_int(value) -> int | None:\n+    try:\n+        return int(value)\n+    except (TypeError, ValueError):\n+        return None\n+\n+\n+def _count_nonempty_lines(path: Path) -> int:\n+    if not path.exists():\n+        return 0\n+    try:\n+        with path.open(\"r\", encoding=\"utf-8\") as handle:\n+            return sum(1 for line in handle if line.strip())\n+    except OSError:\n+        return 0\n+\n+\n+def _load_run_marker(ep_id: str, phase: str) -> Dict[str, Any] | None:\n+    marker_path = _runs_dir(ep_id) / f\"{phase}.json\"\n+    if not marker_path.exists():\n+        return None\n+    try:\n+        payload = json.loads(marker_path.read_text(encoding=\"utf-8\"))\n+    except (OSError, json.JSONDecodeError):\n+        return None\n+    return payload if isinstance(payload, dict) else None\n+\n+\n+def _phase_status_from_marker(phase: str, marker: Dict[str, Any]) -> Dict[str, Any]:\n+    status_value = str(marker.get(\"status\") or \"unknown\").lower()\n+    return {\n+        \"phase\": phase,\n+        \"status\": status_value,\n+        \"faces\": _safe_int(marker.get(\"faces\")),\n+        \"identities\": _safe_int(marker.get(\"identities\")),\n+        \"started_at\": marker.get(\"started_at\"),\n+        \"finished_at\": marker.get(\"finished_at\"),\n+        \"version\": marker.get(\"version\"),\n+        \"source\": \"marker\",\n+    }\n+\n+\n+def _faces_phase_status(ep_id: str) -> Dict[str, Any]:\n+    marker = _load_run_marker(ep_id, \"faces_embed\")\n+    if marker:\n+        return _phase_status_from_marker(\"faces_embed\", marker)\n+    faces_path = _faces_path(ep_id)\n+    faces_count = _count_nonempty_lines(faces_path)\n+    status_value = \"success\" if faces_count > 0 else \"missing\"\n+    source = \"output\" if faces_path.exists() else \"absent\"\n+    return {\n+        \"phase\": \"faces_embed\",\n+        \"status\": status_value,\n+        \"faces\": faces_count,\n+        \"identities\": None,\n+        \"started_at\": None,\n+        \"finished_at\": None,\n+        \"version\": None,\n+        \"source\": source,\n+    }\n+\n+\n+def _cluster_phase_status(ep_id: str) -> Dict[str, Any]:\n+    marker = _load_run_marker(ep_id, \"cluster\")\n+    if marker:\n+        return _phase_status_from_marker(\"cluster\", marker)\n+    identities_path = _identities_path(ep_id)\n+    faces_total = 0\n+    identities_count = 0\n+    if identities_path.exists():\n+        try:\n+            payload = json.loads(identities_path.read_text(encoding=\"utf-8\"))\n+        except (OSError, json.JSONDecodeError):\n+            payload = {}\n+        identities = payload.get(\"identities\") if isinstance(payload, dict) else None\n+        if isinstance(identities, list):\n+            identities_count = len(identities)\n+        stats_block = payload.get(\"stats\") if isinstance(payload, dict) else None\n+        if isinstance(stats_block, dict):\n+            faces_total = _safe_int(stats_block.get(\"faces\")) or 0\n+    status_value = \"success\" if identities_count > 0 else \"missing\"\n+    source = \"output\" if identities_path.exists() else \"absent\"\n+    return {\n+        \"phase\": \"cluster\",\n+        \"status\": status_value,\n+        \"faces\": faces_total,\n+        \"identities\": identities_count,\n+        \"started_at\": None,\n+        \"finished_at\": None,\n+        \"version\": None,\n+        \"source\": source,\n+    }\n+\n+\n+def _delete_episode_assets(ep_id: str, options) -> Dict[str, Any]:\n+    record = EPISODE_STORE.get(ep_id)\n+    if not record:\n+        raise HTTPException(status_code=404, detail=\"Episode not found\")\n+    try:\n+        ep_ctx = episode_context_from_id(ep_id)\n+    except ValueError as exc:\n+        raise HTTPException(status_code=400, detail=\"Invalid episode id\") from exc\n+    local_deleted = 0\n+    delete_local = getattr(options, \"delete_local\", True)\n+    if delete_local:\n+        for path in _episode_local_dirs(ep_id):\n+            if not path.exists():\n+                continue\n+            try:\n+                delete_local_tree(path)\n+                local_deleted += 1\n+            except Exception as exc:  # pragma: no cover - best effort cleanup\n+                LOGGER.warning(\"Failed to delete %s: %s\", path, exc)\n+    s3_deleted = 0\n+    include_s3 = bool(getattr(options, \"include_s3\", False) or getattr(options, \"delete_artifacts\", False))\n+    delete_raw = bool(getattr(options, \"delete_raw\", False))\n+    prefixes: Dict[str, str] | None = None\n+    if include_s3 or delete_raw:\n+        prefixes = v2_artifact_prefixes(ep_ctx)\n+    if include_s3 and prefixes:\n+        for key in (\"frames\", \"crops\", \"manifests\", \"analytics\", \"thumbs_tracks\", \"thumbs_identities\"):\n+            prefix = prefixes.get(key)\n+            if prefix:\n+                s3_deleted += delete_s3_prefix(STORAGE.bucket, prefix, storage=STORAGE)\n+    if delete_raw and prefixes:\n+        for key in (\"raw_v2\", \"raw_v1\"):\n+            prefix = prefixes.get(key)\n+            if prefix:\n+                s3_deleted += delete_s3_prefix(STORAGE.bucket, prefix, storage=STORAGE)\n+    removed = EPISODE_STORE.delete(ep_id)\n+    return {\n+        \"ep_id\": ep_id,\n+        \"deleted\": {\"local_dirs\": local_deleted, \"s3_objects\": s3_deleted},\n+        \"removed_from_store\": removed,\n+    }\n+\n+\n+def _delete_all_records(options) -> Dict[str, Any]:\n+    records = EPISODE_STORE.list()\n+    deleted: List[str] = []\n+    totals = {\"local_dirs\": 0, \"s3_objects\": 0}\n+    for record in records:\n+        result = _delete_episode_assets(record.ep_id, options)\n+        deleted.append(result[\"ep_id\"])\n+        totals[\"local_dirs\"] += result[\"deleted\"][\"local_dirs\"]\n+        totals[\"s3_objects\"] += result[\"deleted\"][\"s3_objects\"]\n+    return {\"deleted\": totals, \"episodes\": deleted, \"count\": len(deleted)}\n+\n+\n+FRAME_IDX_RE = re.compile(r\"frame_(\\d+)\\.jpg$\", re.IGNORECASE)\n+TRACK_LIST_MAX_LIMIT = 500\n+\n+\n+def _load_faces(ep_id: str, *, include_skipped: bool = True) -> List[Dict[str, Any]]:\n+    path = _faces_path(ep_id)\n+    if not path.exists():\n+        return []\n+    rows: List[Dict[str, Any]] = []\n+    with path.open(\"r\", encoding=\"utf-8\") as handle:\n+        for line in handle:\n+            line = line.strip()\n+            if not line:\n+                continue\n+            try:\n+                obj = json.loads(line)\n+            except json.JSONDecodeError:\n+                continue\n+            if isinstance(obj, dict):\n+                if not include_skipped and obj.get(\"skip\"):\n+                    continue\n+                rows.append(obj)\n+    return rows\n+\n+\n+def _write_faces(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    path = _faces_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    with path.open(\"w\", encoding=\"utf-8\") as handle:\n+        for row in rows:\n+            handle.write(json.dumps(row) + \"\\n\")\n+    return path\n+\n+\n+def _load_tracks(ep_id: str) -> List[Dict[str, Any]]:\n+    path = _tracks_path(ep_id)\n+    if not path.exists():\n+        return []\n+    rows: List[Dict[str, Any]] = []\n+    with path.open(\"r\", encoding=\"utf-8\") as handle:\n+        for line in handle:\n+            line = line.strip()\n+            if not line:\n+                continue\n+            try:\n+                obj = json.loads(line)\n+            except json.JSONDecodeError:\n+                continue\n+            if isinstance(obj, dict):\n+                rows.append(obj)\n+    return rows\n+\n+\n+def _write_tracks(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    path = _tracks_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    with path.open(\"w\", encoding=\"utf-8\") as handle:\n+        for row in rows:\n+            handle.write(json.dumps(row) + \"\\n\")\n+    return path\n+\n+\n+def _load_identities(ep_id: str) -> Dict[str, Any]:\n+    path = _identities_path(ep_id)\n+    if not path.exists():\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+    try:\n+        return json.loads(path.read_text(encoding=\"utf-8\"))\n+    except json.JSONDecodeError:\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+\n+\n+def _write_identities(ep_id: str, payload: Dict[str, Any]) -> Path:\n+    path = _identities_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n+    return path\n+\n+\n+def _sync_manifests(ep_id: str, *paths: Path) -> None:\n+    try:\n+        ep_ctx = episode_context_from_id(ep_id)\n+    except ValueError:\n+        return\n+    for path in paths:\n+        if path and path.exists():\n+            try:\n+                STORAGE.put_artifact(ep_ctx, \"manifests\", path, path.name)\n+            except Exception:\n+                continue\n+\n+\n+def _identity_lookup(data: Dict[str, Any]) -> Dict[int, str]:\n+    lookup: Dict[int, str] = {}\n+    for identity in data.get(\"identities\", []):\n+        identity_id = identity.get(\"identity_id\")\n+        if not identity_id:\n+            continue\n+        for track_id in identity.get(\"track_ids\", []) or []:\n+            try:\n+                lookup[int(track_id)] = identity_id\n+            except (TypeError, ValueError):\n+                continue\n+    return lookup\n+\n+\n+def _resolve_thumb_url(ep_id: str, rel_path: str | None, s3_key: str | None) -> str | None:\n+    if s3_key:\n+        url = STORAGE.presign_get(s3_key)\n+        if url:\n+            return url\n+    if not rel_path:\n+        return None\n+    local = _thumbs_root(ep_id) / rel_path\n+    if local.exists():\n+        return str(local)\n+    return None\n+\n+\n+def _recount_track_faces(ep_id: str) -> None:\n+    faces = _load_faces(ep_id, include_skipped=False)\n+    counts: Dict[int, int] = defaultdict(int)\n+    for face in faces:\n+        try:\n+            counts[int(face.get(\"track_id\", -1))] += 1\n+        except (TypeError, ValueError):\n+            continue\n+    track_rows = _load_tracks(ep_id)\n+    if not track_rows:\n+        return\n+    for row in track_rows:\n+        try:\n+            track_id = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            track_id = -1\n+        row[\"faces_count\"] = counts.get(track_id, 0)\n+    path = _write_tracks(ep_id, track_rows)\n+    _sync_manifests(ep_id, path)\n+\n+\n+def _update_identity_stats(ep_id: str, payload: Dict[str, Any]) -> None:\n+    faces_count = len(_load_faces(ep_id, include_skipped=False))\n+    payload.setdefault(\"stats\", {})\n+    payload[\"stats\"][\"faces\"] = faces_count\n+    payload[\"stats\"][\"clusters\"] = len(payload.get(\"identities\", []))\n+\n+\n+def _frame_idx_from_name(name: str) -> int | None:\n+    match = FRAME_IDX_RE.search(name)\n+    if not match:\n+        return None\n+    try:\n+        return int(match.group(1))\n+    except ValueError:\n+        return None\n+\n+\n+def _track_face_rows(ep_id: str, track_id: int) -> Dict[int, Dict[str, Any]]:\n+    faces = _load_faces(ep_id, include_skipped=False)\n+    rows: Dict[int, Dict[str, Any]] = {}\n+    for row in faces:\n+        try:\n+            tid = int(row.get(\"track_id\", -1))\n+            frame_idx = int(row.get(\"frame_idx\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        if tid != track_id:\n+            continue\n+        rows.setdefault(frame_idx, row)\n+    return rows\n+\n+\n+def _max_candidate_count(limit: int, offset: int, sample: int) -> int:\n+    limit = max(1, limit)\n+    offset = max(0, offset)\n+    sample = max(1, sample)\n+    return max(1, (offset + limit) * sample)\n+\n+\n+def _apply_sampling(entries: List[Dict[str, Any]], sample: int, offset: int, limit: int) -> List[Dict[str, Any]]:\n+    sample = max(1, sample)\n+    offset = max(0, offset)\n+    limit = max(1, limit)\n+    downsampled = [item for idx, item in enumerate(entries) if idx % sample == 0]\n+    if offset >= len(downsampled):\n+        return []\n+    end = offset + limit if limit else None\n+    return downsampled[offset:end]\n+\n+\n+def _require_episode_context(ep_id: str):\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+    except ValueError as exc:  # pragma: no cover - invalid ids rejected upstream\n+        raise HTTPException(status_code=400, detail=\"Invalid episode id\") from exc\n+    return ctx, artifact_prefixes(ctx)\n+\n+\n+def _media_entry(track_id: int, frame_idx: int, key: str, url: str, meta: Dict[str, Any]) -> Dict[str, Any]:\n+    return {\n+        \"track_id\": track_id,\n+        \"frame_idx\": frame_idx,\n+        \"key\": key,\n+        \"url\": url,\n+        \"w\": meta.get(\"crop_width\") or meta.get(\"width\"),\n+        \"h\": meta.get(\"crop_height\") or meta.get(\"height\"),\n+        \"ts\": meta.get(\"ts\"),\n+    }\n+\n+\n+\n+def _list_track_frame_media(ep_id: str, track_id: int, sample: int, limit: int, offset: int) -> List[Dict[str, Any]]:\n+    sample = max(1, sample)\n+    limit = max(1, min(limit, TRACK_LIST_MAX_LIMIT))\n+    offset = max(0, offset)\n+    face_rows = _track_face_rows(ep_id, track_id)\n+    if not face_rows:\n+        return []\n+    frame_indices = sorted(face_rows.keys())\n+    max_candidates = _max_candidate_count(limit, offset, sample)\n+    frame_indices = frame_indices[:max_candidates]\n+    entries: List[Dict[str, Any]] = []\n+    if STORAGE.backend == \"local\":\n+        frames_dir = get_path(ep_id, \"frames_root\") / \"frames\"\n+        for idx in frame_indices:\n+            path = frames_dir / f\"frame_{idx:06d}.jpg\"\n+            if not path.exists():\n+                continue\n+            path_str = path.as_posix()\n+            meta = face_rows.get(idx, {})\n+            entries.append(_media_entry(track_id, idx, path_str, path_str, meta))\n+    else:\n+        _, prefixes = _require_episode_context(ep_id)\n+        base_prefix = prefixes[\"frames\"]\n+        for idx in frame_indices:\n+            key = f\"{base_prefix}frame_{idx:06d}.jpg\"\n+            url = STORAGE.presign_get(key)\n+            if not url:\n+                continue\n+            meta = face_rows.get(idx, {})\n+            entries.append(_media_entry(track_id, idx, key, url, meta))\n+    entries.sort(key=lambda item: item[\"frame_idx\"])\n+    return _apply_sampling(entries, sample, offset, limit)\n+\n+\n+class EpisodeCreateRequest(BaseModel):\n+    show_slug_or_id: str = Field(..., min_length=1, description=\"Show slug or identifier\")\n+    season_number: int = Field(..., ge=0, le=999, description=\"Season number\")\n+    episode_number: int = Field(..., ge=0, le=999, description=\"Episode number within the season\")\n+    title: str | None = Field(None, max_length=200)\n+    air_date: date | None = None\n+\n+\n+class EpisodeCreateResponse(BaseModel):\n+    ep_id: str\n+\n+\n+class EpisodeSummary(BaseModel):\n+    ep_id: str\n+    show_slug: str\n+    season_number: int\n+    episode_number: int\n+    title: str | None\n+    air_date: str | None\n+\n+\n+class EpisodeListResponse(BaseModel):\n+    episodes: List[EpisodeSummary]\n+\n+\n+class EpisodeUpsert(BaseModel):\n+    ep_id: str = Field(..., min_length=3, description=\"Deterministic ep_id (slug-sXXeYY)\")\n+    show_slug: str = Field(..., min_length=1)\n+    season: int = Field(..., ge=0, le=999)\n+    episode: int = Field(..., ge=0, le=999)\n+    title: str | None = Field(None, max_length=200)\n+    air_date: date | None = None\n+\n+\n+class FaceMoveRequest(BaseModel):\n+    from_track_id: int = Field(..., ge=0)\n+    face_ids: List[str] = Field(..., min_length=1, description=\"Face identifiers to move\")\n+    target_identity_id: str | None = Field(None, description=\"Existing identity to receive frames\")\n+    new_identity_name: str | None = Field(None, description=\"Create a new identity with this name\")\n+    show_id: str | None = Field(None, description=\"Optional show slug for roster updates\")\n+\n+\n+class TrackFrameMoveRequest(BaseModel):\n+    frame_ids: List[int] = Field(..., min_length=1, description=\"Track frame indices to move\")\n+    target_identity_id: str | None = Field(None, description=\"Existing identity target\")\n+    new_identity_name: str | None = Field(None, description=\"Optional new identity name\")\n+    show_id: str | None = Field(None, description=\"Optional show slug override\")\n+\n+\n+class TrackFrameDeleteRequest(BaseModel):\n+    frame_ids: List[int] = Field(..., min_length=1, description=\"Track frame indices to delete\")\n+    delete_assets: bool = True\n+\n+\n+class IdentityRenameRequest(BaseModel):\n+    label: str | None = Field(None, max_length=120)\n+\n+\n+class IdentityNameRequest(BaseModel):\n+    name: str = Field(..., min_length=1, max_length=200)\n+    show: str | None = Field(None, description=\"Optional show slug override\")\n+\n+\n+class IdentityMergeRequest(BaseModel):\n+    source_id: str\n+    target_id: str\n+\n+\n+class TrackMoveRequest(BaseModel):\n+    target_identity_id: str | None = None\n+\n+\n+class TrackDeleteRequest(BaseModel):\n+    delete_faces: bool = True\n+\n+\n+class FrameDeleteRequest(BaseModel):\n+    track_id: int\n+    frame_idx: int\n+    delete_assets: bool = False\n+\n+\n+\n+\n+class DeleteEpisodeIn(BaseModel):\n+    include_s3: bool = False\n+\n+\n+class DeleteAllIn(BaseModel):\n+    confirm: str\n+    include_s3: bool = False\n+\n+\n+class DeleteEpisodeLegacyIn(BaseModel):\n+    delete_artifacts: bool = True\n+    delete_raw: bool = False\n+    delete_local: bool = True\n+\n+\n+class PurgeAllLegacyIn(BaseModel):\n+    confirm: str\n+    delete_artifacts: bool = True\n+    delete_raw: bool = False\n+    delete_local: bool = True\n+\n+\n+class S3VideoItem(BaseModel):\n+    bucket: str\n+    key: str\n+    ep_id: str\n+    show: str | None = None\n+    season: int | None = None\n+    episode: int | None = None\n+    size: int | None = None\n+    last_modified: str | None = None\n+    etag: str | None = None\n+    exists_in_store: bool\n+    key_version: str | None = None\n+\n+\n+class S3VideosResponse(BaseModel):\n+    items: List[S3VideoItem]\n+    count: int\n+\n+\n+class EpisodeS3Status(BaseModel):\n+    bucket: str\n+    v2_key: str | None = None\n+    v2_exists: bool = False\n+    v1_key: str\n+    v1_exists: bool\n+\n+\n+class EpisodeLocalStatus(BaseModel):\n+    path: str\n+    exists: bool\n+\n+\n+class EpisodeDetailResponse(BaseModel):\n+    ep_id: str\n+    show_slug: str\n+    season_number: int\n+    episode_number: int\n+    title: str | None\n+    air_date: str | None\n+    s3: EpisodeS3Status\n+    local: EpisodeLocalStatus\n+\n+\n+class PhaseStatus(BaseModel):\n+    phase: str\n+    status: str\n+    faces: int | None = None\n+    identities: int | None = None\n+    started_at: str | None = None\n+    finished_at: str | None = None\n+    version: str | None = None\n+    source: str | None = None\n+\n+\n+class EpisodeStatusResponse(BaseModel):\n+    ep_id: str\n+    faces_embed: PhaseStatus\n+    cluster: PhaseStatus\n+\n+\n+class AssetUploadResponse(BaseModel):\n+    ep_id: str\n+    method: str\n+    bucket: str\n+    key: str\n+    object_key: str | None = None  # backwards compatibility\n+    upload_url: str | None\n+    expires_in: int | None\n+    headers: Dict[str, str]\n+    path: str | None = None\n+    local_video_path: str\n+    backend: str\n+\n+\n+class EpisodeMirrorResponse(BaseModel):\n+    ep_id: str\n+    local_video_path: str\n+    bytes: int | None = None\n+    etag: str | None = None\n+    used_key_version: str | None = None\n+\n+\n+class EpisodeVideoMeta(BaseModel):\n+    ep_id: str\n+    local_exists: bool\n+    local_video_path: str\n+    width: int | None = None\n+    height: int | None = None\n+    frames: int | None = None\n+    duration_sec: float | None = None\n+    fps_detected: float | None = None\n+\n+\n+class DeleteEpisodeIn(BaseModel):\n+    delete_artifacts: bool = True\n+    delete_raw: bool = False\n+    delete_local: bool = True\n+\n+\n+class PurgeAllIn(BaseModel):\n+    confirm: str\n+    delete_artifacts: bool = True\n+    delete_raw: bool = False\n+    delete_local: bool = True\n+\n+\n+@router.get(\"/episodes\", response_model=EpisodeListResponse, tags=[\"episodes\"])\n+def list_episodes() -> EpisodeListResponse:\n+    records = EPISODE_STORE.list()\n+    episodes = [\n+        EpisodeSummary(\n+            ep_id=record.ep_id,\n+            show_slug=record.show_ref,\n+            season_number=record.season_number,\n+            episode_number=record.episode_number,\n+            title=record.title,\n+            air_date=record.air_date,\n+        )\n+        for record in records\n+    ]\n+    return EpisodeListResponse(episodes=episodes)\n+\n+\n+@router.get(\"/episodes/s3_videos\", response_model=S3VideosResponse, tags=[\"episodes\"])\n+def list_s3_videos(q: str | None = Query(None), limit: int = Query(200, ge=1, le=1000)) -> S3VideosResponse:\n+    raw_items = STORAGE.list_episode_videos_s3(limit=limit)\n+    items: List[S3VideoItem] = []\n+    for obj in raw_items:\n+        ep_id = obj.get(\"ep_id\")\n+        if not isinstance(ep_id, str):\n+            continue\n+        if q and q.lower() not in ep_id.lower():\n+            continue\n+        items.append(\n+            S3VideoItem(\n+                bucket=obj.get(\"bucket\", STORAGE.bucket),\n+                key=str(obj.get(\"key\")),\n+                ep_id=ep_id,\n+                show=obj.get(\"show\"),\n+                season=obj.get(\"season\"),\n+                episode=obj.get(\"episode\"),\n+                size=obj.get(\"size\"),\n+                last_modified=str(obj.get(\"last_modified\")) if obj.get(\"last_modified\") else None,\n+                etag=obj.get(\"etag\"),\n+                exists_in_store=EPISODE_STORE.exists(ep_id),\n+                key_version=obj.get(\"key_version\"),\n+            )\n+        )\n+        if len(items) >= limit:\n+            break\n+    return S3VideosResponse(items=items, count=len(items))\n+\n+\n+@router.post(\"/episodes/{ep_id}/delete\")\n+def delete_episode_new(ep_id: str, body: DeleteEpisodeIn = Body(default=DeleteEpisodeIn())) -> Dict[str, Any]:\n+    return _delete_episode_assets(ep_id, body)\n+\n+\n+@router.post(\"/episodes/delete_all\")\n+def delete_all(body: DeleteAllIn) -> Dict[str, Any]:\n+    if body.confirm.strip() != \"DELETE ALL\":\n+        raise HTTPException(status_code=400, detail=\"Confirmation text mismatch.\")\n+    delete_opts = DeleteEpisodeIn(include_s3=body.include_s3)\n+    return _delete_all_records(delete_opts)\n+\n+\n+@router.delete(\"/episodes/{ep_id}\")\n+def delete_episode(ep_id: str, body: DeleteEpisodeLegacyIn = Body(default=DeleteEpisodeLegacyIn())) -> Dict[str, Any]:\n+    return _delete_episode_assets(ep_id, body)\n+\n+\n+@router.post(\"/episodes/purge_all\")\n+def purge_all(body: PurgeAllLegacyIn) -> Dict[str, Any]:\n+    if body.confirm.strip() != \"DELETE ALL\":\n+        raise HTTPException(status_code=400, detail=\"Confirmation text mismatch.\")\n+    delete_opts = DeleteEpisodeLegacyIn(\n+        delete_artifacts=body.delete_artifacts,\n+        delete_raw=body.delete_raw,\n+        delete_local=body.delete_local,\n+    )\n+    return _delete_all_records(delete_opts)\n+\n+\n+@router.get(\"/episodes/{ep_id}\", response_model=EpisodeDetailResponse, tags=[\"episodes\"])\n+def episode_details(ep_id: str) -> EpisodeDetailResponse:\n+    record = EPISODE_STORE.get(ep_id)\n+    if not record:\n+        raise HTTPException(status_code=404, detail=\"Episode not found\")\n+\n+    local_path = get_path(ep_id, \"video\")\n+    v2_key = STORAGE.video_object_key_v2(record.show_ref, record.season_number, record.episode_number)\n+    v1_key = STORAGE.video_object_key_v1(ep_id)\n+    v2_exists = STORAGE.object_exists(v2_key)\n+    v1_exists = STORAGE.object_exists(v1_key)\n+\n+    return EpisodeDetailResponse(\n+        ep_id=record.ep_id,\n+        show_slug=record.show_ref,\n+        season_number=record.season_number,\n+        episode_number=record.episode_number,\n+        title=record.title,\n+        air_date=record.air_date,\n+        s3=EpisodeS3Status(\n+            bucket=STORAGE.bucket,\n+            v2_key=v2_key,\n+            v2_exists=v2_exists,\n+            v1_key=v1_key,\n+            v1_exists=v1_exists,\n+        ),\n+        local=EpisodeLocalStatus(path=str(local_path), exists=local_path.exists()),\n+    )\n+\n+\n+@router.get(\"/episodes/{ep_id}/progress\", tags=[\"episodes\"])\n+def episode_progress(ep_id: str) -> dict:\n+    progress_path = get_path(ep_id, \"detections\").parent / \"progress.json\"\n+    if not progress_path.exists():\n+        raise HTTPException(status_code=404, detail=\"Progress not available\")\n+    try:\n+        payload = json.loads(progress_path.read_text(encoding=\"utf-8\"))\n+    except json.JSONDecodeError as exc:\n+        raise HTTPException(status_code=503, detail=\"Progress file corrupt\") from exc\n+    return {\"ep_id\": ep_id, \"progress\": payload}\n+\n+\n+@router.get(\"/episodes/{ep_id}/status\", response_model=EpisodeStatusResponse, tags=[\"episodes\"])\n+def episode_run_status(ep_id: str) -> EpisodeStatusResponse:\n+    faces_status = PhaseStatus(**_faces_phase_status(ep_id))\n+    cluster_status = PhaseStatus(**_cluster_phase_status(ep_id))\n+    return EpisodeStatusResponse(ep_id=ep_id, faces_embed=faces_status, cluster=cluster_status)\n+\n+\n+@router.post(\"/episodes\", response_model=EpisodeCreateResponse, tags=[\"episodes\"])\n+def create_episode(payload: EpisodeCreateRequest) -> EpisodeCreateResponse:\n+    record = EPISODE_STORE.upsert(\n+        show_ref=payload.show_slug_or_id,\n+        season_number=payload.season_number,\n+        episode_number=payload.episode_number,\n+        title=payload.title,\n+        air_date=payload.air_date,\n+    )\n+    return EpisodeCreateResponse(ep_id=record.ep_id)\n+\n+\n+@router.post(\"/episodes/upsert_by_id\", tags=[\"episodes\"])\n+def upsert_by_id(payload: EpisodeUpsert) -> dict:\n+    try:\n+        record, created = EPISODE_STORE.upsert_ep_id(\n+            ep_id=payload.ep_id,\n+            show_slug=payload.show_slug,\n+            season=payload.season,\n+            episode=payload.episode,\n+            title=payload.title,\n+            air_date=payload.air_date,\n+        )\n+    except ValueError as exc:\n+        raise HTTPException(status_code=400, detail=str(exc)) from exc\n+    return {\n+        \"ep_id\": record.ep_id,\n+        \"created\": created,\n+        \"show_slug\": record.show_ref,\n+        \"season\": record.season_number,\n+        \"episode\": record.episode_number,\n+        \"title\": record.title,\n+        \"air_date\": record.air_date,\n+    }\n+\n+\n+@router.post(\"/episodes/{ep_id}/assets\", response_model=AssetUploadResponse, tags=[\"episodes\"])\n+def presign_episode_assets(ep_id: str) -> AssetUploadResponse:\n+    record = EPISODE_STORE.get(ep_id)\n+    if not record:\n+        raise HTTPException(status_code=404, detail=\"Episode not found\")\n+\n+    ensure_dirs(ep_id)\n+    v2_key = STORAGE.video_object_key_v2(record.show_ref, record.season_number, record.episode_number)\n+    presigned = STORAGE.presign_episode_video(ep_id, object_key=v2_key)\n+    local_video_path = get_path(ep_id, \"video\")\n+    path = presigned.path or (str(local_video_path) if presigned.method == \"FILE\" else None)\n+\n+    return AssetUploadResponse(\n+        ep_id=presigned.ep_id,\n+        method=presigned.method,\n+        bucket=presigned.bucket,\n+        key=presigned.object_key,\n+        object_key=presigned.object_key,\n+        upload_url=presigned.upload_url,\n+        expires_in=presigned.expires_in,\n+        headers=presigned.headers,\n+        path=path,\n+        local_video_path=str(local_video_path),\n+        backend=presigned.backend,\n+    )\n+\n+\n+@router.post(\n+    \"/episodes/{ep_id}/mirror\",\n+    response_model=EpisodeMirrorResponse,\n+    tags=[\"episodes\"],\n+)\n+def mirror_episode_video(ep_id: str) -> EpisodeMirrorResponse:\n+    record = EPISODE_STORE.get(ep_id)\n+    if not record:\n+        raise HTTPException(status_code=404, detail=\"Episode not found\")\n+\n+    ensure_dirs(ep_id)\n+    try:\n+        result = STORAGE.ensure_local_mirror(\n+            ep_id,\n+            show_ref=record.show_ref,\n+            season_number=record.season_number,\n+            episode_number=record.episode_number,\n+        )\n+    except RuntimeError as exc:\n+        raise HTTPException(status_code=404, detail=str(exc)) from exc\n+    return EpisodeMirrorResponse(\n+        ep_id=ep_id,\n+        local_video_path=str(result.get(\"local_video_path\")),\n+        bytes=result.get(\"bytes\"),\n+        etag=result.get(\"etag\"),\n+        used_key_version=result.get(\"used_key_version\"),\n+    )\n+\n+\n+@router.post(\n+    \"/episodes/{ep_id}/hydrate\",\n+    response_model=EpisodeMirrorResponse,\n+    tags=[\"episodes\"],\n+)\n+def hydrate_episode_video(ep_id: str) -> EpisodeMirrorResponse:\n+    return mirror_episode_video(ep_id)\n+\n+\n+@router.get(\n+    \"/episodes/{ep_id}/video_meta\",\n+    response_model=EpisodeVideoMeta,\n+    tags=[\"episodes\"],\n+)\n+def episode_video_meta(ep_id: str) -> EpisodeVideoMeta:\n+    video_path = get_path(ep_id, \"video\")\n+    if not video_path.exists():\n+        raise HTTPException(status_code=404, detail=\"Local video not found\")\n+\n+    width: int | None = None\n+    height: int | None = None\n+    frames: int | None = None\n+    fps_detected: float | None = None\n+    duration_sec: float | None = None\n+\n+    try:\n+        import cv2  # type: ignore\n+\n+        cap = cv2.VideoCapture(str(video_path))\n+        if cap.isOpened():\n+            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 0) or None\n+            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 0) or None\n+            frames_val = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n+            frames = int(frames_val) if frames_val and frames_val > 0 else None\n+            fps_val = cap.get(cv2.CAP_PROP_FPS)\n+            fps_detected = float(fps_val) if fps_val and fps_val > 0 else None\n+            if frames and fps_detected:\n+                duration_sec = frames / fps_detected\n+        cap.release()\n+    except Exception as exc:  # pragma: no cover - best effort\n+        raise HTTPException(status_code=500, detail=f\"Failed to analyze video: {exc}\") from exc\n+\n+    return EpisodeVideoMeta(\n+        ep_id=ep_id,\n+        local_exists=True,\n+        local_video_path=str(video_path),\n+        width=width,\n+        height=height,\n+        frames=frames,\n+        duration_sec=duration_sec,\n+        fps_detected=fps_detected,\n+    )\n+\n+\n+@router.get(\"/episodes/{ep_id}/identities\")\n+def list_identities(ep_id: str) -> dict:\n+    payload = _load_identities(ep_id)\n+    track_lookup = {int(row.get(\"track_id\", -1)): row for row in _load_tracks(ep_id)}\n+    identities = []\n+    for identity in payload.get(\"identities\", []):\n+        track_ids = []\n+        for raw_tid in identity.get(\"track_ids\", []) or []:\n+            try:\n+                track_ids.append(int(raw_tid))\n+            except (TypeError, ValueError):\n+                continue\n+        faces_total = identity.get(\"size\")\n+        if faces_total is None:\n+            faces_total = sum(int(track_lookup.get(tid, {}).get(\"faces_count\", 0)) for tid in track_ids)\n+        identities.append(\n+            {\n+                \"identity_id\": identity.get(\"identity_id\"),\n+                \"label\": identity.get(\"label\"),\n+                \"name\": identity.get(\"name\"),\n+                \"track_ids\": track_ids,\n+                \"faces\": faces_total,\n+                \"rep_thumbnail_url\": _resolve_thumb_url(\n+                    ep_id,\n+                    identity.get(\"rep_thumb_rel_path\"),\n+                    identity.get(\"rep_thumb_s3_key\"),\n+                ),\n+            }\n+        )\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+        show_slug = ctx.show_slug\n+    except ValueError:\n+        show_slug = None\n+    if show_slug:\n+        for entry in payload.get(\"identities\", []):\n+            name = entry.get(\"name\")\n+            if isinstance(name, str) and name.strip():\n+                try:\n+                    roster_service.add_if_missing(show_slug, name)\n+                except ValueError:\n+                    pass\n+    return {\"identities\": identities, \"stats\": payload.get(\"stats\", {})}\n+\n+\n+@router.get(\"/episodes/{ep_id}/cluster_tracks\")\n+def list_cluster_tracks(\n+    ep_id: str,\n+    limit_per_cluster: int | None = Query(None, ge=1, description=\"Optional max tracks per cluster\"),\n+) -> dict:\n+    try:\n+        return identity_service.cluster_track_summary(ep_id, limit_per_cluster=limit_per_cluster)\n+    except ValueError as exc:\n+        raise HTTPException(status_code=400, detail=str(exc)) from exc\n+\n+\n+@router.get(\"/episodes/{ep_id}/faces_grid\")\n+def faces_grid(ep_id: str, track_id: int | None = Query(None)) -> dict:\n+    faces = _load_faces(ep_id, include_skipped=False)\n+    identity_lookup = _identity_lookup(_load_identities(ep_id))\n+    items: List[dict] = []\n+    for row in faces:\n+        try:\n+            tid = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        if track_id is not None and tid != track_id:\n+            continue\n+        items.append(\n+            {\n+                \"face_id\": row.get(\"face_id\"),\n+                \"track_id\": tid,\n+                \"frame_idx\": row.get(\"frame_idx\"),\n+                \"ts\": row.get(\"ts\"),\n+                \"thumbnail_url\": _resolve_thumb_url(ep_id, row.get(\"thumb_rel_path\"), row.get(\"thumb_s3_key\")),\n+                \"identity_id\": identity_lookup.get(tid),\n+            }\n+        )\n+    return {\"faces\": items, \"count\": len(items)}\n+\n+\n+@router.get(\"/episodes/{ep_id}/identities/{identity_id}\")\n+def identity_detail(ep_id: str, identity_id: str) -> dict:\n+    payload = _load_identities(ep_id)\n+    identity = next((item for item in payload.get(\"identities\", []) if item.get(\"identity_id\") == identity_id), None)\n+    if not identity:\n+        raise HTTPException(status_code=404, detail=\"Identity not found\")\n+    track_lookup = {int(row.get(\"track_id\", -1)): row for row in _load_tracks(ep_id)}\n+    tracks_payload = []\n+    for raw_tid in identity.get(\"track_ids\", []) or []:\n+        try:\n+            tid = int(raw_tid)\n+        except (TypeError, ValueError):\n+            continue\n+        track_row = track_lookup.get(tid, {})\n+        tracks_payload.append(\n+            {\n+                \"track_id\": tid,\n+                \"faces_count\": track_row.get(\"faces_count\", 0),\n+                \"thumbnail_url\": _resolve_thumb_url(ep_id, track_row.get(\"thumb_rel_path\"), track_row.get(\"thumb_s3_key\")),\n+            }\n+        )\n+    return {\n+        \"identity\": {\n+            \"identity_id\": identity_id,\n+            \"label\": identity.get(\"label\"),\n+            \"name\": identity.get(\"name\"),\n+            \"track_ids\": identity.get(\"track_ids\", []),\n+            \"rep_thumbnail_url\": _resolve_thumb_url(\n+                ep_id,\n+                identity.get(\"rep_thumb_rel_path\"),\n+                identity.get(\"rep_thumb_s3_key\"),\n+            ),\n+        },\n+        \"tracks\": tracks_payload,\n+    }\n+\n+\n+@router.get(\"/episodes/{ep_id}/tracks/{track_id}\")\n+def track_detail(ep_id: str, track_id: int) -> dict:\n+    faces = [row for row in _load_faces(ep_id, include_skipped=False) if int(row.get(\"track_id\", -1)) == track_id]\n+    frames = [\n+        {\n+            \"face_id\": row.get(\"face_id\"),\n+            \"frame_idx\": row.get(\"frame_idx\"),\n+            \"ts\": row.get(\"ts\"),\n+            \"thumbnail_url\": _resolve_thumb_url(ep_id, row.get(\"thumb_rel_path\"), row.get(\"thumb_s3_key\")),\n+            \"skip\": row.get(\"skip\"),\n+        }\n+        for row in faces\n+    ]\n+    track_row = next((row for row in _load_tracks(ep_id) if int(row.get(\"track_id\", -1)) == track_id), None)\n+    return {\n+        \"track_id\": track_id,\n+        \"faces_count\": len(frames),\n+        \"thumbnail_url\": _resolve_thumb_url(\n+            ep_id,\n+            (track_row or {}).get(\"thumb_rel_path\"),\n+            (track_row or {}).get(\"thumb_s3_key\"),\n+        ),\n+        \"frames\": frames,\n+    }\n+\n+\n+@router.get(\"/episodes/{ep_id}/tracks/{track_id}/crops\")\n+def list_track_crops(\n+    ep_id: str,\n+    track_id: int,\n+    sample: int = Query(5, ge=1, le=100, description=\"Return every Nth crop\"),\n+    limit: int = Query(200, ge=1, le=TRACK_LIST_MAX_LIMIT),\n+    start_after: str | None = Query(None, description=\"Opaque cursor returned by the previous call\"),\n+) -> Dict[str, Any]:\n+    ctx, _ = _require_episode_context(ep_id)\n+    payload = STORAGE.list_track_crops(ctx, track_id, sample=sample, max_keys=limit, start_after=start_after)\n+    face_rows = _track_face_rows(ep_id, track_id)\n+    items = payload.get(\"items\", []) if isinstance(payload, dict) else []\n+    for item in items:\n+        frame_idx = item.get(\"frame_idx\")\n+        try:\n+            frame_int = int(frame_idx)\n+        except (TypeError, ValueError):\n+            continue\n+        meta = face_rows.get(frame_int, {})\n+        if meta:\n+            if \"w\" not in item and \"crop_width\" in meta:\n+                item[\"w\"] = meta.get(\"crop_width\") or meta.get(\"width\")\n+            if \"h\" not in item and \"crop_height\" in meta:\n+                item[\"h\"] = meta.get(\"crop_height\") or meta.get(\"height\")\n+            item.setdefault(\"ts\", meta.get(\"ts\"))\n+    return payload\n+\n+\n+@router.get(\"/episodes/{ep_id}/tracks/{track_id}/frames\")\n+def list_track_frames(\n+    ep_id: str,\n+    track_id: int,\n+    sample: int = Query(5, ge=1, le=100, description=\"Return every Nth frame\"),\n+    limit: int = Query(200, ge=1, le=TRACK_LIST_MAX_LIMIT),\n+    offset: int = Query(0, ge=0),\n+) -> List[Dict[str, Any]]:\n+    return _list_track_frame_media(ep_id, track_id, sample, limit, offset)\n+\n+\n+@router.post(\"/episodes/{ep_id}/tracks/{track_id}/frames/move\")\n+def move_track_frames(ep_id: str, track_id: int, body: TrackFrameMoveRequest) -> dict:\n+    frame_ids = sorted({int(idx) for idx in body.frame_ids or []})\n+    if not frame_ids:\n+        raise HTTPException(status_code=400, detail=\"frame_ids_required\")\n+    face_rows = _track_face_rows(ep_id, track_id)\n+    if not face_rows:\n+        raise HTTPException(status_code=404, detail=\"track_not_found\")\n+    selected_faces: List[str] = []\n+    ops: List[Dict[str, Any]] = []\n+    for frame_idx in frame_ids:\n+        row = face_rows.get(frame_idx)\n+        if not row:\n+            raise HTTPException(status_code=404, detail=f\"frame_not_found:{frame_idx}\")\n+        face_id = row.get(\"face_id\")\n+        if not face_id:\n+            raise HTTPException(status_code=400, detail=f\"face_id_missing:{frame_idx}\")\n+        selected_faces.append(str(face_id))\n+        ops.append({\"frame_idx\": frame_idx, \"face_id\": face_id})\n+    try:\n+        result = identity_service.move_frames(\n+            ep_id,\n+            track_id,\n+            selected_faces,\n+            target_identity_id=body.target_identity_id,\n+            new_identity_name=body.new_identity_name,\n+            show_id=body.show_id,\n+        )\n+    except ValueError as exc:\n+        raise HTTPException(status_code=400, detail=str(exc)) from exc\n+    _append_face_ops(\n+        ep_id,\n+        [\n+            {\n+                \"op\": \"move_frame\",\n+                \"frame_idx\": entry[\"frame_idx\"],\n+                \"face_id\": entry[\"face_id\"],\n+                \"source_track_id\": track_id,\n+                \"target_track_id\": result.get(\"new_track_id\"),\n+                \"target_identity_id\": result.get(\"target_identity_id\") or body.target_identity_id,\n+            }\n+            for entry in ops\n+        ],\n+    )\n+    return {\n+        \"moved\": len(selected_faces),\n+        \"frame_ids\": frame_ids,\n+        \"new_track_id\": result.get(\"new_track_id\"),\n+        \"target_identity_id\": result.get(\"target_identity_id\"),\n+        \"target_name\": result.get(\"target_name\"),\n+        \"clusters\": result.get(\"clusters\"),\n+    }\n+\n+\n+@router.delete(\"/episodes/{ep_id}/tracks/{track_id}/frames\")\n+def delete_track_frames(ep_id: str, track_id: int, body: TrackFrameDeleteRequest) -> dict:\n+    frame_ids = sorted({int(idx) for idx in body.frame_ids or []})\n+    if not frame_ids:\n+        raise HTTPException(status_code=400, detail=\"frame_ids_required\")\n+    faces = _load_faces(ep_id)\n+    removed: List[Dict[str, Any]] = []\n+    kept: List[Dict[str, Any]] = []\n+    target_set = set(frame_ids)\n+    for row in faces:\n+        try:\n+            tid = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            tid = -1\n+        frame_idx = row.get(\"frame_idx\")\n+        try:\n+            frame_val = int(frame_idx)\n+        except (TypeError, ValueError):\n+            frame_val = None\n+        if tid == track_id and frame_val in target_set:\n+            removed.append(row)\n+        else:\n+            kept.append(row)\n+    if not removed:\n+        raise HTTPException(status_code=404, detail=\"frames_not_found\")\n+    faces_path = _write_faces(ep_id, kept)\n+    if body.delete_assets:\n+        _remove_face_assets(ep_id, removed)\n+    _append_face_ops(\n+        ep_id,\n+        [\n+            {\n+                \"op\": \"delete_frame\",\n+                \"track_id\": track_id,\n+                \"frame_idx\": int(row.get(\"frame_idx\", -1)),\n+                \"face_id\": row.get(\"face_id\"),\n+            }\n+            for row in removed\n+        ],\n+    )\n+    _recount_track_faces(ep_id)\n+    identities = _load_identities(ep_id)\n+    _update_identity_stats(ep_id, identities)\n+    identities_path = _write_identities(ep_id, identities)\n+    _sync_manifests(ep_id, faces_path, identities_path)\n+    return {\n+        \"track_id\": track_id,\n+        \"deleted\": len(removed),\n+        \"remaining\": len(kept),\n+    }\n+\n+\n+@router.post(\"/episodes/{ep_id}/identities/{identity_id}/rename\")\n+def rename_identity(ep_id: str, identity_id: str, body: IdentityRenameRequest) -> dict:\n+    payload = _load_identities(ep_id)\n+    identity = next((item for item in payload.get(\"identities\", []) if item.get(\"identity_id\") == identity_id), None)\n+    if not identity:\n+        raise HTTPException(status_code=404, detail=\"Identity not found\")\n+    label = (body.label or \"\").strip()\n+    identity[\"label\"] = label or None\n+    _update_identity_stats(ep_id, payload)\n+    path = _write_identities(ep_id, payload)\n+    _sync_manifests(ep_id, path)\n+    return {\"identity_id\": identity_id, \"label\": identity[\"label\"]}\n+\n+\n+@router.post(\"/episodes/{ep_id}/identities/{identity_id}/name\")\n+def assign_identity_name(ep_id: str, identity_id: str, body: IdentityNameRequest) -> dict:\n+    try:\n+        return identity_service.assign_identity_name(ep_id, identity_id, body.name, body.show)\n+    except ValueError as exc:\n+        raise HTTPException(status_code=400, detail=str(exc)) from exc\n+\n+\n+@router.post(\"/episodes/{ep_id}/identities/merge\")\n+def merge_identities(ep_id: str, body: IdentityMergeRequest) -> dict:\n+    payload = _load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    source = next((item for item in identities if item.get(\"identity_id\") == body.source_id), None)\n+    target = next((item for item in identities if item.get(\"identity_id\") == body.target_id), None)\n+    if not source or not target:\n+        raise HTTPException(status_code=404, detail=\"Source or target identity not found\")\n+    merged_track_ids = set(target.get(\"track_ids\", []) or [])\n+    for tid in source.get(\"track_ids\", []) or []:\n+        merged_track_ids.add(tid)\n+    target[\"track_ids\"] = sorted({int(x) for x in merged_track_ids})\n+    payload[\"identities\"] = [item for item in identities if item.get(\"identity_id\") != body.source_id]\n+    _update_identity_stats(ep_id, payload)\n+    path = _write_identities(ep_id, payload)\n+    _sync_manifests(ep_id, path)\n+    return {\"target_id\": body.target_id, \"track_ids\": target[\"track_ids\"]}\n+\n+\n+@router.post(\"/episodes/{ep_id}/tracks/{track_id}/move\")\n+def move_track(ep_id: str, track_id: int, body: TrackMoveRequest) -> dict:\n+    payload = _load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    source_identity = None\n+    target_identity = None\n+    for identity in identities:\n+        if body.target_identity_id and identity.get(\"identity_id\") == body.target_identity_id:\n+            target_identity = identity\n+        if track_id in identity.get(\"track_ids\", []):\n+            source_identity = identity\n+    if body.target_identity_id and target_identity is None:\n+        raise HTTPException(status_code=404, detail=\"Target identity not found\")\n+    if source_identity and track_id in source_identity.get(\"track_ids\", []):\n+        source_identity[\"track_ids\"] = [tid for tid in source_identity[\"track_ids\"] if tid != track_id]\n+    if target_identity is not None:\n+        if track_id not in target_identity.get(\"track_ids\", []):\n+            target_identity.setdefault(\"track_ids\", []).append(track_id)\n+            target_identity[\"track_ids\"] = sorted(target_identity[\"track_ids\"])\n+    _update_identity_stats(ep_id, payload)\n+    path = _write_identities(ep_id, payload)\n+    _sync_manifests(ep_id, path)\n+    return {\n+        \"identity_id\": body.target_identity_id,\n+        \"track_ids\": target_identity[\"track_ids\"] if target_identity else [],\n+    }\n+\n+\n+@router.post(\"/episodes/{ep_id}/faces/move_frames\")\n+def move_faces(ep_id: str, body: FaceMoveRequest) -> dict:\n+    try:\n+        return identity_service.move_frames(\n+            ep_id,\n+            body.from_track_id,\n+            body.face_ids,\n+            target_identity_id=body.target_identity_id,\n+            new_identity_name=body.new_identity_name,\n+            show_id=body.show_id,\n+        )\n+    except ValueError as exc:\n+        raise HTTPException(status_code=400, detail=str(exc)) from exc\n+\n+\n+\n+\n+@router.delete(\"/episodes/{ep_id}/identities/{identity_id}\")\n+def delete_identity(ep_id: str, identity_id: str) -> dict:\n+    payload = _load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    before = len(identities)\n+    payload[\"identities\"] = [item for item in identities if item.get(\"identity_id\") != identity_id]\n+    if len(payload[\"identities\"]) == before:\n+        raise HTTPException(status_code=404, detail=\"Identity not found\")\n+    _update_identity_stats(ep_id, payload)\n+    path = _write_identities(ep_id, payload)\n+    _sync_manifests(ep_id, path)\n+    return {\"deleted\": identity_id, \"remaining\": len(payload[\"identities\"])}\n+\n+\n+@router.delete(\"/episodes/{ep_id}/tracks/{track_id}\")\n+def delete_track(ep_id: str, track_id: int, payload: TrackDeleteRequest = Body(default=TrackDeleteRequest())) -> dict:\n+    faces = _load_faces(ep_id)\n+    if payload.delete_faces:\n+        faces = [row for row in faces if int(row.get(\"track_id\", -1)) != track_id]\n+        faces_path = _write_faces(ep_id, faces)\n+    else:\n+        faces_path = _faces_path(ep_id)\n+    track_rows = _load_tracks(ep_id)\n+    kept_tracks = [row for row in track_rows if int(row.get(\"track_id\", -1)) != track_id]\n+    if len(kept_tracks) == len(track_rows):\n+        raise HTTPException(status_code=404, detail=\"Track not found\")\n+    tracks_path = _write_tracks(ep_id, kept_tracks)\n+    identities = _load_identities(ep_id)\n+    for identity in identities.get(\"identities\", []):\n+        identity[\"track_ids\"] = [tid for tid in identity.get(\"track_ids\", []) if tid != track_id]\n+    _update_identity_stats(ep_id, identities)\n+    identities_path = _write_identities(ep_id, identities)\n+    _recount_track_faces(ep_id)\n+    _sync_manifests(ep_id, faces_path, tracks_path, identities_path)\n+    return {\"track_id\": track_id, \"faces_deleted\": payload.delete_faces}\n+\n+\n+@router.delete(\"/episodes/{ep_id}/frames\")\n+def delete_frame(ep_id: str, payload: FrameDeleteRequest) -> dict:\n+    faces = _load_faces(ep_id)\n+    removed_rows = [\n+        row\n+        for row in faces\n+        if int(row.get(\"track_id\", -1)) == payload.track_id and int(row.get(\"frame_idx\", -1)) == payload.frame_idx\n+    ]\n+    if not removed_rows:\n+        raise HTTPException(status_code=404, detail=\"Face frame not found\")\n+    faces = [row for row in faces if row not in removed_rows]\n+    faces_path = _write_faces(ep_id, faces)\n+    if payload.delete_assets:\n+        frames_root = get_path(ep_id, \"frames_root\")\n+        for row in removed_rows:\n+            thumb_rel = row.get(\"thumb_rel_path\")\n+            if isinstance(thumb_rel, str):\n+                thumb_file = _thumbs_root(ep_id) / thumb_rel\n+                try:\n+                    thumb_file.unlink()\n+                except FileNotFoundError:\n+                    pass\n+            crop_rel = row.get(\"crop_rel_path\")\n+            if isinstance(crop_rel, str):\n+                crop_file = frames_root / crop_rel\n+                try:\n+                    crop_file.unlink()\n+                except FileNotFoundError:","path":"apps/api/routers/episodes.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.\n```suggestion\n                except FileNotFoundError:\n                    # It is safe to ignore if the file does not exist (already deleted)\n```","created_at":"2025-11-11T20:00:13Z","updated_at":"2025-11-11T20:00:22Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565611","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565611"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565611"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565611/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":1442,"original_line":1442,"side":"RIGHT","author_association":"NONE","original_position":1442,"position":1442,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565626","pull_request_review_id":3449784939,"id":2515565626,"node_id":"PRRC_kwDOQRcQs86V8Hw6","diff_hunk":"@@ -0,0 +1,691 @@\n+from __future__ import annotations\n+\n+import csv\n+import json\n+import logging\n+import os\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, List, Sequence\n+\n+from py_screenalytics.artifacts import get_path\n+\n+from apps.api.services import roster as roster_service\n+from apps.api.services.storage import StorageService, artifact_prefixes, episode_context_from_id\n+from apps.shared.storage import s3_write_json, use_s3\n+\n+STORAGE = StorageService()\n+LOGGER = logging.getLogger(__name__)\n+\n+\n+def _manifests_dir(ep_id: str) -> Path:\n+    return get_path(ep_id, \"detections\").parent\n+\n+\n+def _faces_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"faces.jsonl\"\n+\n+\n+def _identities_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"identities.json\"\n+\n+\n+def _tracks_path(ep_id: str) -> Path:\n+    return get_path(ep_id, \"tracks\")\n+\n+\n+def _thumbs_root(ep_id: str) -> Path:\n+    return get_path(ep_id, \"frames_root\") / \"thumbs\"\n+\n+\n+def _thumbnail_url(ep_id: str, rel_path: str | None, s3_key: str | None) -> str | None:\n+    if s3_key:\n+        url = STORAGE.presign_get(str(s3_key))\n+        if url:\n+            return url\n+    if not rel_path:\n+        return None\n+    local = _thumbs_root(ep_id) / rel_path\n+    if local.exists():\n+        return str(local)\n+    return None\n+\n+\n+def _analytics_root(ep_id: str) -> Path:\n+    data_root = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+    return data_root / \"analytics\" / ep_id\n+\n+\n+def _screentime_csv_path(ep_id: str) -> Path:\n+    return _analytics_root(ep_id) / \"screentime.csv\"\n+\n+\n+def _read_json_lines(path: Path) -> List[Dict[str, Any]]:\n+    if not path.exists():\n+        return []\n+    rows: List[Dict[str, Any]] = []\n+    with path.open(\"r\", encoding=\"utf-8\") as handle:\n+        for line in handle:\n+            line = line.strip()\n+            if not line:\n+                continue\n+            try:\n+                obj = json.loads(line)\n+            except json.JSONDecodeError:\n+                continue\n+            if isinstance(obj, dict):\n+                rows.append(obj)\n+    return rows\n+\n+\n+def _write_json_lines(path: Path, rows: List[Dict[str, Any]]) -> Path:\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    with path.open(\"w\", encoding=\"utf-8\") as handle:\n+        for row in rows:\n+            handle.write(json.dumps(row) + \"\\n\")\n+    return path\n+\n+\n+def load_faces(ep_id: str) -> List[Dict[str, Any]]:\n+    return _read_json_lines(_faces_path(ep_id))\n+\n+\n+def write_faces(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    return _write_json_lines(_faces_path(ep_id), rows)\n+\n+\n+def load_tracks(ep_id: str) -> List[Dict[str, Any]]:\n+    return _read_json_lines(_tracks_path(ep_id))\n+\n+\n+def write_tracks(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    return _write_json_lines(_tracks_path(ep_id), rows)\n+\n+\n+def load_identities(ep_id: str) -> Dict[str, Any]:\n+    path = _identities_path(ep_id)\n+    if not path.exists():\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+    try:\n+        return json.loads(path.read_text(encoding=\"utf-8\"))\n+    except json.JSONDecodeError:\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+\n+\n+def write_identities(ep_id: str, payload: Dict[str, Any]) -> Path:\n+    path = _identities_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n+    return path\n+\n+\n+def update_identity_stats(ep_id: str, payload: Dict[str, Any]) -> None:\n+    faces_count = len(load_faces(ep_id))\n+    payload.setdefault(\"stats\", {})\n+    payload[\"stats\"][\"faces\"] = faces_count\n+    payload[\"stats\"][\"clusters\"] = len(payload.get(\"identities\", []))\n+\n+\n+def sync_manifests(ep_id: str, *paths: Path) -> None:\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+    except ValueError:\n+        return\n+    for path in paths:\n+        if path and path.exists():\n+            try:\n+                STORAGE.put_artifact(ctx, \"manifests\", path, path.name)\n+            except Exception:\n+                continue\n+\n+\n+def _identity_rows(payload: Dict[str, Any]) -> List[Dict[str, Any]]:\n+    identities = payload.get(\"identities\")\n+    if isinstance(identities, list):\n+        return identities\n+    if isinstance(payload, list):  # legacy format\n+        return payload  # type: ignore[return-value]\n+    return []\n+\n+\n+def _identity_name_lookup(payload: Dict[str, Any]) -> Dict[str, str]:\n+    mapping: Dict[str, str] = {}\n+    for entry in _identity_rows(payload):\n+        key = entry.get(\"identity_id\") or entry.get(\"id\")\n+        name = entry.get(\"name\")\n+        if not key or not name:\n+            continue\n+        mapping[str(key)] = str(name)\n+    return mapping\n+\n+\n+def _annotate_screentime_csv(ep_id: str, payload: Dict[str, Any]) -> None:\n+    csv_path = _screentime_csv_path(ep_id)\n+    if not csv_path.exists():\n+        return\n+    try:\n+        with csv_path.open(\"r\", encoding=\"utf-8\", newline=\"\") as handle:\n+            reader = csv.DictReader(handle)\n+            rows = list(reader)\n+            fieldnames = reader.fieldnames or []\n+    except (OSError, csv.Error):\n+        return\n+    if not rows or not fieldnames:\n+        return\n+    key_field = \"identity_id\" if \"identity_id\" in fieldnames else None\n+    if key_field is None and \"person_id\" in fieldnames:\n+        key_field = \"person_id\"\n+    if key_field is None:\n+        return\n+    lookup = _identity_name_lookup(payload)\n+    if not lookup:\n+        return\n+    updated = False\n+    for row in rows:\n+        identifier = str(row.get(key_field, \"\") or \"\").strip()\n+        if not identifier:\n+            continue\n+        name = lookup.get(identifier)\n+        if not name or row.get(\"name\") == name:\n+            continue\n+        row[\"name\"] = name\n+        updated = True\n+    if not updated:\n+        return\n+    out_fields = list(fieldnames)\n+    if \"name\" not in out_fields:\n+        out_fields.append(\"name\")\n+    try:\n+        with csv_path.open(\"w\", encoding=\"utf-8\", newline=\"\") as handle:\n+            writer = csv.DictWriter(handle, fieldnames=out_fields)\n+            writer.writeheader()\n+            for row in rows:\n+                if \"name\" not in row:\n+                    row[\"name\"] = \"\"\n+                writer.writerow(row)\n+    except (OSError, csv.Error) as exc:\n+        LOGGER.warning(\"Failed to update screentime.csv for %s: %s\", ep_id, exc)\n+\n+\n+def _next_identity_id(entries: Sequence[Dict[str, Any]]) -> str:\n+    max_idx = 0\n+    for entry in entries:\n+        raw = entry.get(\"identity_id\") or entry.get(\"id\")\n+        if not raw:\n+            continue\n+        digits = \"\".join(ch for ch in str(raw) if ch.isdigit())\n+        try:\n+            idx = int(digits)\n+        except ValueError:\n+            continue\n+        max_idx = max(max_idx, idx)\n+    return f\"id_{max_idx + 1:04d}\"\n+\n+\n+def _track_dir_name(track_id: int) -> str:\n+    return f\"track_{int(track_id):04d}\"\n+\n+\n+def _crop_rel(track_id: int, frame_idx: int) -> str:\n+    return f\"crops/{_track_dir_name(track_id)}/frame_{int(frame_idx):06d}.jpg\"\n+\n+\n+def _thumb_rel(track_id: int, frame_idx: int) -> str:\n+    return f\"{_track_dir_name(track_id)}/thumb_{int(frame_idx):06d}.jpg\"\n+\n+\n+def _faces_by_track(rows: Sequence[Dict[str, Any]]) -> Dict[int, List[Dict[str, Any]]]:\n+    grouped: Dict[int, List[Dict[str, Any]]] = {}\n+    for row in rows:\n+        try:\n+            track_id = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        grouped.setdefault(track_id, []).append(row)\n+    for items in grouped.values():\n+        items.sort(key=lambda r: int(r.get(\"frame_idx\", 0)))\n+    return grouped\n+\n+\n+def _next_track_id(tracks: Sequence[Dict[str, Any]]) -> int:\n+    max_id = 0\n+    for entry in tracks:\n+        try:\n+            max_id = max(max_id, int(entry.get(\"track_id\", 0)))\n+        except (TypeError, ValueError):\n+            continue\n+    return max_id + 1 if max_id >= 0 else 1\n+\n+\n+def _track_lookup(tracks: Sequence[Dict[str, Any]]) -> Dict[int, Dict[str, Any]]:\n+    lookup: Dict[int, Dict[str, Any]] = {}\n+    for entry in tracks:\n+        try:\n+            track_id = int(entry.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        if track_id >= 0:\n+            lookup[track_id] = entry\n+    return lookup\n+\n+\n+def cluster_track_summary(\n+    ep_id: str,\n+    *,\n+    include: Iterable[str] | None = None,\n+    limit_per_cluster: int | None = None,\n+) -> Dict[str, Any]:\n+    include_set = {identity.lower() for identity in include} if include else None\n+    identities_payload = load_identities(ep_id)\n+    tracks = load_tracks(ep_id)\n+    faces = load_faces(ep_id)\n+    track_lookup = _track_lookup(tracks)\n+    face_counts: Dict[int, int] = {}\n+    for row in faces:\n+        try:\n+            tid = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        face_counts[tid] = face_counts.get(tid, 0) + 1\n+\n+    clusters: List[Dict[str, Any]] = []\n+    for identity in identities_payload.get(\"identities\", []):\n+        identity_id = identity.get(\"identity_id\")\n+        if not identity_id:\n+            continue\n+        if include_set and identity_id.lower() not in include_set:\n+            continue\n+        track_ids = identity.get(\"track_ids\", []) or []\n+        tracks_payload: List[Dict[str, Any]] = []\n+        for raw_tid in track_ids:\n+            try:\n+                tid = int(raw_tid)\n+            except (TypeError, ValueError):\n+                continue\n+            track_row = track_lookup.get(tid)\n+            if not track_row:\n+                continue\n+            thumb_url = _thumbnail_url(ep_id, track_row.get(\"thumb_rel_path\"), track_row.get(\"thumb_s3_key\"))\n+            tracks_payload.append(\n+                {\n+                    \"track_id\": tid,\n+                    \"faces\": track_row.get(\"faces_count\") or face_counts.get(tid) or 0,\n+                    \"frames\": track_row.get(\"frame_count\"),\n+                    \"rep_thumb_url\": thumb_url,\n+                }\n+            )\n+        if limit_per_cluster:\n+            limit = max(1, int(limit_per_cluster))\n+            tracks_payload = tracks_payload[:limit]\n+        clusters.append(\n+            {\n+                \"identity_id\": identity_id,\n+                \"name\": identity.get(\"name\"),\n+                \"label\": identity.get(\"label\"),\n+                \"counts\": {\n+                    \"tracks\": len(track_ids),\n+                    \"faces\": identity.get(\"faces\")\n+                    or sum(face_counts.get(int(tid), 0) for tid in track_ids),\n+                },\n+                \"tracks\": tracks_payload,\n+            }\n+        )\n+    return {\"ep_id\": ep_id, \"clusters\": clusters}\n+\n+\n+def _rebuild_track_entry(track_entry: Dict[str, Any], face_rows: Sequence[Dict[str, Any]]) -> Dict[str, Any]:\n+    if not face_rows:\n+        return track_entry\n+    sorted_rows = sorted(face_rows, key=lambda row: int(row.get(\"frame_idx\", 0)))\n+    track_entry[\"frame_count\"] = len(sorted_rows)\n+    track_entry[\"faces_count\"] = len(sorted_rows)\n+    ts_values = [row.get(\"ts\") for row in sorted_rows if isinstance(row.get(\"ts\"), (int, float))]\n+    if ts_values:\n+        track_entry[\"first_ts\"] = float(min(ts_values))\n+        track_entry[\"last_ts\"] = float(max(ts_values))\n+    sample_limit = min(len(sorted_rows), 10)\n+    bboxes = []\n+    for row in sorted_rows[:sample_limit]:\n+        entry = {\n+            \"frame_idx\": row.get(\"frame_idx\"),\n+            \"ts\": row.get(\"ts\"),\n+            \"bbox_xyxy\": row.get(\"bbox_xyxy\"),\n+            \"landmarks\": row.get(\"landmarks\"),\n+        }\n+        bboxes.append(entry)\n+    if bboxes:\n+        track_entry[\"bboxes_sampled\"] = bboxes\n+    thumb_rel = sorted_rows[0].get(\"thumb_rel_path\")\n+    thumb_s3 = sorted_rows[0].get(\"thumb_s3_key\")\n+    if thumb_rel:\n+        track_entry[\"thumb_rel_path\"] = thumb_rel\n+    if thumb_s3:\n+        track_entry[\"thumb_s3_key\"] = thumb_s3\n+    return track_entry\n+\n+\n+def _write_track_index(ep_id: str, track_id: int, face_rows: Sequence[Dict[str, Any]], ctx) -> None:\n+    frames_root = get_path(ep_id, \"frames_root\")\n+    crops_dir = frames_root / \"crops\" / _track_dir_name(track_id)\n+    index_path = crops_dir / \"index.json\"\n+    if not face_rows:\n+        try:\n+            index_path.unlink()\n+        except FileNotFoundError:","path":"apps/api/services/identities.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:14Z","updated_at":"2025-11-11T20:00:22Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565626","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565626"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565626"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565626/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":373,"original_line":373,"side":"RIGHT","author_association":"NONE","original_position":373,"position":373,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565636","pull_request_review_id":3449784939,"id":2515565636,"node_id":"PRRC_kwDOQRcQs86V8HxE","diff_hunk":"@@ -0,0 +1,691 @@\n+from __future__ import annotations\n+\n+import csv\n+import json\n+import logging\n+import os\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, List, Sequence\n+\n+from py_screenalytics.artifacts import get_path\n+\n+from apps.api.services import roster as roster_service\n+from apps.api.services.storage import StorageService, artifact_prefixes, episode_context_from_id\n+from apps.shared.storage import s3_write_json, use_s3\n+\n+STORAGE = StorageService()\n+LOGGER = logging.getLogger(__name__)\n+\n+\n+def _manifests_dir(ep_id: str) -> Path:\n+    return get_path(ep_id, \"detections\").parent\n+\n+\n+def _faces_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"faces.jsonl\"\n+\n+\n+def _identities_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"identities.json\"\n+\n+\n+def _tracks_path(ep_id: str) -> Path:\n+    return get_path(ep_id, \"tracks\")\n+\n+\n+def _thumbs_root(ep_id: str) -> Path:\n+    return get_path(ep_id, \"frames_root\") / \"thumbs\"\n+\n+\n+def _thumbnail_url(ep_id: str, rel_path: str | None, s3_key: str | None) -> str | None:\n+    if s3_key:\n+        url = STORAGE.presign_get(str(s3_key))\n+        if url:\n+            return url\n+    if not rel_path:\n+        return None\n+    local = _thumbs_root(ep_id) / rel_path\n+    if local.exists():\n+        return str(local)\n+    return None\n+\n+\n+def _analytics_root(ep_id: str) -> Path:\n+    data_root = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+    return data_root / \"analytics\" / ep_id\n+\n+\n+def _screentime_csv_path(ep_id: str) -> Path:\n+    return _analytics_root(ep_id) / \"screentime.csv\"\n+\n+\n+def _read_json_lines(path: Path) -> List[Dict[str, Any]]:\n+    if not path.exists():\n+        return []\n+    rows: List[Dict[str, Any]] = []\n+    with path.open(\"r\", encoding=\"utf-8\") as handle:\n+        for line in handle:\n+            line = line.strip()\n+            if not line:\n+                continue\n+            try:\n+                obj = json.loads(line)\n+            except json.JSONDecodeError:\n+                continue\n+            if isinstance(obj, dict):\n+                rows.append(obj)\n+    return rows\n+\n+\n+def _write_json_lines(path: Path, rows: List[Dict[str, Any]]) -> Path:\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    with path.open(\"w\", encoding=\"utf-8\") as handle:\n+        for row in rows:\n+            handle.write(json.dumps(row) + \"\\n\")\n+    return path\n+\n+\n+def load_faces(ep_id: str) -> List[Dict[str, Any]]:\n+    return _read_json_lines(_faces_path(ep_id))\n+\n+\n+def write_faces(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    return _write_json_lines(_faces_path(ep_id), rows)\n+\n+\n+def load_tracks(ep_id: str) -> List[Dict[str, Any]]:\n+    return _read_json_lines(_tracks_path(ep_id))\n+\n+\n+def write_tracks(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    return _write_json_lines(_tracks_path(ep_id), rows)\n+\n+\n+def load_identities(ep_id: str) -> Dict[str, Any]:\n+    path = _identities_path(ep_id)\n+    if not path.exists():\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+    try:\n+        return json.loads(path.read_text(encoding=\"utf-8\"))\n+    except json.JSONDecodeError:\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+\n+\n+def write_identities(ep_id: str, payload: Dict[str, Any]) -> Path:\n+    path = _identities_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n+    return path\n+\n+\n+def update_identity_stats(ep_id: str, payload: Dict[str, Any]) -> None:\n+    faces_count = len(load_faces(ep_id))\n+    payload.setdefault(\"stats\", {})\n+    payload[\"stats\"][\"faces\"] = faces_count\n+    payload[\"stats\"][\"clusters\"] = len(payload.get(\"identities\", []))\n+\n+\n+def sync_manifests(ep_id: str, *paths: Path) -> None:\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+    except ValueError:\n+        return\n+    for path in paths:\n+        if path and path.exists():\n+            try:\n+                STORAGE.put_artifact(ctx, \"manifests\", path, path.name)\n+            except Exception:\n+                continue\n+\n+\n+def _identity_rows(payload: Dict[str, Any]) -> List[Dict[str, Any]]:\n+    identities = payload.get(\"identities\")\n+    if isinstance(identities, list):\n+        return identities\n+    if isinstance(payload, list):  # legacy format\n+        return payload  # type: ignore[return-value]\n+    return []\n+\n+\n+def _identity_name_lookup(payload: Dict[str, Any]) -> Dict[str, str]:\n+    mapping: Dict[str, str] = {}\n+    for entry in _identity_rows(payload):\n+        key = entry.get(\"identity_id\") or entry.get(\"id\")\n+        name = entry.get(\"name\")\n+        if not key or not name:\n+            continue\n+        mapping[str(key)] = str(name)\n+    return mapping\n+\n+\n+def _annotate_screentime_csv(ep_id: str, payload: Dict[str, Any]) -> None:\n+    csv_path = _screentime_csv_path(ep_id)\n+    if not csv_path.exists():\n+        return\n+    try:\n+        with csv_path.open(\"r\", encoding=\"utf-8\", newline=\"\") as handle:\n+            reader = csv.DictReader(handle)\n+            rows = list(reader)\n+            fieldnames = reader.fieldnames or []\n+    except (OSError, csv.Error):\n+        return\n+    if not rows or not fieldnames:\n+        return\n+    key_field = \"identity_id\" if \"identity_id\" in fieldnames else None\n+    if key_field is None and \"person_id\" in fieldnames:\n+        key_field = \"person_id\"\n+    if key_field is None:\n+        return\n+    lookup = _identity_name_lookup(payload)\n+    if not lookup:\n+        return\n+    updated = False\n+    for row in rows:\n+        identifier = str(row.get(key_field, \"\") or \"\").strip()\n+        if not identifier:\n+            continue\n+        name = lookup.get(identifier)\n+        if not name or row.get(\"name\") == name:\n+            continue\n+        row[\"name\"] = name\n+        updated = True\n+    if not updated:\n+        return\n+    out_fields = list(fieldnames)\n+    if \"name\" not in out_fields:\n+        out_fields.append(\"name\")\n+    try:\n+        with csv_path.open(\"w\", encoding=\"utf-8\", newline=\"\") as handle:\n+            writer = csv.DictWriter(handle, fieldnames=out_fields)\n+            writer.writeheader()\n+            for row in rows:\n+                if \"name\" not in row:\n+                    row[\"name\"] = \"\"\n+                writer.writerow(row)\n+    except (OSError, csv.Error) as exc:\n+        LOGGER.warning(\"Failed to update screentime.csv for %s: %s\", ep_id, exc)\n+\n+\n+def _next_identity_id(entries: Sequence[Dict[str, Any]]) -> str:\n+    max_idx = 0\n+    for entry in entries:\n+        raw = entry.get(\"identity_id\") or entry.get(\"id\")\n+        if not raw:\n+            continue\n+        digits = \"\".join(ch for ch in str(raw) if ch.isdigit())\n+        try:\n+            idx = int(digits)\n+        except ValueError:\n+            continue\n+        max_idx = max(max_idx, idx)\n+    return f\"id_{max_idx + 1:04d}\"\n+\n+\n+def _track_dir_name(track_id: int) -> str:\n+    return f\"track_{int(track_id):04d}\"\n+\n+\n+def _crop_rel(track_id: int, frame_idx: int) -> str:\n+    return f\"crops/{_track_dir_name(track_id)}/frame_{int(frame_idx):06d}.jpg\"\n+\n+\n+def _thumb_rel(track_id: int, frame_idx: int) -> str:\n+    return f\"{_track_dir_name(track_id)}/thumb_{int(frame_idx):06d}.jpg\"\n+\n+\n+def _faces_by_track(rows: Sequence[Dict[str, Any]]) -> Dict[int, List[Dict[str, Any]]]:\n+    grouped: Dict[int, List[Dict[str, Any]]] = {}\n+    for row in rows:\n+        try:\n+            track_id = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        grouped.setdefault(track_id, []).append(row)\n+    for items in grouped.values():\n+        items.sort(key=lambda r: int(r.get(\"frame_idx\", 0)))\n+    return grouped\n+\n+\n+def _next_track_id(tracks: Sequence[Dict[str, Any]]) -> int:\n+    max_id = 0\n+    for entry in tracks:\n+        try:\n+            max_id = max(max_id, int(entry.get(\"track_id\", 0)))\n+        except (TypeError, ValueError):\n+            continue\n+    return max_id + 1 if max_id >= 0 else 1\n+\n+\n+def _track_lookup(tracks: Sequence[Dict[str, Any]]) -> Dict[int, Dict[str, Any]]:\n+    lookup: Dict[int, Dict[str, Any]] = {}\n+    for entry in tracks:\n+        try:\n+            track_id = int(entry.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        if track_id >= 0:\n+            lookup[track_id] = entry\n+    return lookup\n+\n+\n+def cluster_track_summary(\n+    ep_id: str,\n+    *,\n+    include: Iterable[str] | None = None,\n+    limit_per_cluster: int | None = None,\n+) -> Dict[str, Any]:\n+    include_set = {identity.lower() for identity in include} if include else None\n+    identities_payload = load_identities(ep_id)\n+    tracks = load_tracks(ep_id)\n+    faces = load_faces(ep_id)\n+    track_lookup = _track_lookup(tracks)\n+    face_counts: Dict[int, int] = {}\n+    for row in faces:\n+        try:\n+            tid = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        face_counts[tid] = face_counts.get(tid, 0) + 1\n+\n+    clusters: List[Dict[str, Any]] = []\n+    for identity in identities_payload.get(\"identities\", []):\n+        identity_id = identity.get(\"identity_id\")\n+        if not identity_id:\n+            continue\n+        if include_set and identity_id.lower() not in include_set:\n+            continue\n+        track_ids = identity.get(\"track_ids\", []) or []\n+        tracks_payload: List[Dict[str, Any]] = []\n+        for raw_tid in track_ids:\n+            try:\n+                tid = int(raw_tid)\n+            except (TypeError, ValueError):\n+                continue\n+            track_row = track_lookup.get(tid)\n+            if not track_row:\n+                continue\n+            thumb_url = _thumbnail_url(ep_id, track_row.get(\"thumb_rel_path\"), track_row.get(\"thumb_s3_key\"))\n+            tracks_payload.append(\n+                {\n+                    \"track_id\": tid,\n+                    \"faces\": track_row.get(\"faces_count\") or face_counts.get(tid) or 0,\n+                    \"frames\": track_row.get(\"frame_count\"),\n+                    \"rep_thumb_url\": thumb_url,\n+                }\n+            )\n+        if limit_per_cluster:\n+            limit = max(1, int(limit_per_cluster))\n+            tracks_payload = tracks_payload[:limit]\n+        clusters.append(\n+            {\n+                \"identity_id\": identity_id,\n+                \"name\": identity.get(\"name\"),\n+                \"label\": identity.get(\"label\"),\n+                \"counts\": {\n+                    \"tracks\": len(track_ids),\n+                    \"faces\": identity.get(\"faces\")\n+                    or sum(face_counts.get(int(tid), 0) for tid in track_ids),\n+                },\n+                \"tracks\": tracks_payload,\n+            }\n+        )\n+    return {\"ep_id\": ep_id, \"clusters\": clusters}\n+\n+\n+def _rebuild_track_entry(track_entry: Dict[str, Any], face_rows: Sequence[Dict[str, Any]]) -> Dict[str, Any]:\n+    if not face_rows:\n+        return track_entry\n+    sorted_rows = sorted(face_rows, key=lambda row: int(row.get(\"frame_idx\", 0)))\n+    track_entry[\"frame_count\"] = len(sorted_rows)\n+    track_entry[\"faces_count\"] = len(sorted_rows)\n+    ts_values = [row.get(\"ts\") for row in sorted_rows if isinstance(row.get(\"ts\"), (int, float))]\n+    if ts_values:\n+        track_entry[\"first_ts\"] = float(min(ts_values))\n+        track_entry[\"last_ts\"] = float(max(ts_values))\n+    sample_limit = min(len(sorted_rows), 10)\n+    bboxes = []\n+    for row in sorted_rows[:sample_limit]:\n+        entry = {\n+            \"frame_idx\": row.get(\"frame_idx\"),\n+            \"ts\": row.get(\"ts\"),\n+            \"bbox_xyxy\": row.get(\"bbox_xyxy\"),\n+            \"landmarks\": row.get(\"landmarks\"),\n+        }\n+        bboxes.append(entry)\n+    if bboxes:\n+        track_entry[\"bboxes_sampled\"] = bboxes\n+    thumb_rel = sorted_rows[0].get(\"thumb_rel_path\")\n+    thumb_s3 = sorted_rows[0].get(\"thumb_s3_key\")\n+    if thumb_rel:\n+        track_entry[\"thumb_rel_path\"] = thumb_rel\n+    if thumb_s3:\n+        track_entry[\"thumb_s3_key\"] = thumb_s3\n+    return track_entry\n+\n+\n+def _write_track_index(ep_id: str, track_id: int, face_rows: Sequence[Dict[str, Any]], ctx) -> None:\n+    frames_root = get_path(ep_id, \"frames_root\")\n+    crops_dir = frames_root / \"crops\" / _track_dir_name(track_id)\n+    index_path = crops_dir / \"index.json\"\n+    if not face_rows:\n+        try:\n+            index_path.unlink()\n+        except FileNotFoundError:\n+            pass\n+        return\n+    crops_dir.mkdir(parents=True, exist_ok=True)\n+    entries = [\n+        {\n+            \"key\": f\"{_track_dir_name(track_id)}/frame_{int(row.get('frame_idx', 0)):06d}.jpg\",\n+            \"frame_idx\": int(row.get(\"frame_idx\", 0)),\n+            \"ts\": row.get(\"ts\"),\n+        }\n+        for row in sorted(face_rows, key=lambda r: int(r.get(\"frame_idx\", 0)))\n+    ]\n+    index_path.write_text(json.dumps(entries, indent=2), encoding=\"utf-8\")\n+    try:\n+        STORAGE.put_artifact(ctx, \"crops\", index_path, f\"{_track_dir_name(track_id)}/index.json\")\n+    except Exception:","path":"apps/api/services/identities.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:14Z","updated_at":"2025-11-11T20:00:23Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565636","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565636"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565636"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565636/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":388,"original_line":388,"side":"RIGHT","author_association":"NONE","original_position":388,"position":388,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565647","pull_request_review_id":3449784939,"id":2515565647,"node_id":"PRRC_kwDOQRcQs86V8HxP","diff_hunk":"@@ -0,0 +1,691 @@\n+from __future__ import annotations\n+\n+import csv\n+import json\n+import logging\n+import os\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, List, Sequence\n+\n+from py_screenalytics.artifacts import get_path\n+\n+from apps.api.services import roster as roster_service\n+from apps.api.services.storage import StorageService, artifact_prefixes, episode_context_from_id\n+from apps.shared.storage import s3_write_json, use_s3\n+\n+STORAGE = StorageService()\n+LOGGER = logging.getLogger(__name__)\n+\n+\n+def _manifests_dir(ep_id: str) -> Path:\n+    return get_path(ep_id, \"detections\").parent\n+\n+\n+def _faces_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"faces.jsonl\"\n+\n+\n+def _identities_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"identities.json\"\n+\n+\n+def _tracks_path(ep_id: str) -> Path:\n+    return get_path(ep_id, \"tracks\")\n+\n+\n+def _thumbs_root(ep_id: str) -> Path:\n+    return get_path(ep_id, \"frames_root\") / \"thumbs\"\n+\n+\n+def _thumbnail_url(ep_id: str, rel_path: str | None, s3_key: str | None) -> str | None:\n+    if s3_key:\n+        url = STORAGE.presign_get(str(s3_key))\n+        if url:\n+            return url\n+    if not rel_path:\n+        return None\n+    local = _thumbs_root(ep_id) / rel_path\n+    if local.exists():\n+        return str(local)\n+    return None\n+\n+\n+def _analytics_root(ep_id: str) -> Path:\n+    data_root = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+    return data_root / \"analytics\" / ep_id\n+\n+\n+def _screentime_csv_path(ep_id: str) -> Path:\n+    return _analytics_root(ep_id) / \"screentime.csv\"\n+\n+\n+def _read_json_lines(path: Path) -> List[Dict[str, Any]]:\n+    if not path.exists():\n+        return []\n+    rows: List[Dict[str, Any]] = []\n+    with path.open(\"r\", encoding=\"utf-8\") as handle:\n+        for line in handle:\n+            line = line.strip()\n+            if not line:\n+                continue\n+            try:\n+                obj = json.loads(line)\n+            except json.JSONDecodeError:\n+                continue\n+            if isinstance(obj, dict):\n+                rows.append(obj)\n+    return rows\n+\n+\n+def _write_json_lines(path: Path, rows: List[Dict[str, Any]]) -> Path:\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    with path.open(\"w\", encoding=\"utf-8\") as handle:\n+        for row in rows:\n+            handle.write(json.dumps(row) + \"\\n\")\n+    return path\n+\n+\n+def load_faces(ep_id: str) -> List[Dict[str, Any]]:\n+    return _read_json_lines(_faces_path(ep_id))\n+\n+\n+def write_faces(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    return _write_json_lines(_faces_path(ep_id), rows)\n+\n+\n+def load_tracks(ep_id: str) -> List[Dict[str, Any]]:\n+    return _read_json_lines(_tracks_path(ep_id))\n+\n+\n+def write_tracks(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    return _write_json_lines(_tracks_path(ep_id), rows)\n+\n+\n+def load_identities(ep_id: str) -> Dict[str, Any]:\n+    path = _identities_path(ep_id)\n+    if not path.exists():\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+    try:\n+        return json.loads(path.read_text(encoding=\"utf-8\"))\n+    except json.JSONDecodeError:\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+\n+\n+def write_identities(ep_id: str, payload: Dict[str, Any]) -> Path:\n+    path = _identities_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n+    return path\n+\n+\n+def update_identity_stats(ep_id: str, payload: Dict[str, Any]) -> None:\n+    faces_count = len(load_faces(ep_id))\n+    payload.setdefault(\"stats\", {})\n+    payload[\"stats\"][\"faces\"] = faces_count\n+    payload[\"stats\"][\"clusters\"] = len(payload.get(\"identities\", []))\n+\n+\n+def sync_manifests(ep_id: str, *paths: Path) -> None:\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+    except ValueError:\n+        return\n+    for path in paths:\n+        if path and path.exists():\n+            try:\n+                STORAGE.put_artifact(ctx, \"manifests\", path, path.name)\n+            except Exception:\n+                continue\n+\n+\n+def _identity_rows(payload: Dict[str, Any]) -> List[Dict[str, Any]]:\n+    identities = payload.get(\"identities\")\n+    if isinstance(identities, list):\n+        return identities\n+    if isinstance(payload, list):  # legacy format\n+        return payload  # type: ignore[return-value]\n+    return []\n+\n+\n+def _identity_name_lookup(payload: Dict[str, Any]) -> Dict[str, str]:\n+    mapping: Dict[str, str] = {}\n+    for entry in _identity_rows(payload):\n+        key = entry.get(\"identity_id\") or entry.get(\"id\")\n+        name = entry.get(\"name\")\n+        if not key or not name:\n+            continue\n+        mapping[str(key)] = str(name)\n+    return mapping\n+\n+\n+def _annotate_screentime_csv(ep_id: str, payload: Dict[str, Any]) -> None:\n+    csv_path = _screentime_csv_path(ep_id)\n+    if not csv_path.exists():\n+        return\n+    try:\n+        with csv_path.open(\"r\", encoding=\"utf-8\", newline=\"\") as handle:\n+            reader = csv.DictReader(handle)\n+            rows = list(reader)\n+            fieldnames = reader.fieldnames or []\n+    except (OSError, csv.Error):\n+        return\n+    if not rows or not fieldnames:\n+        return\n+    key_field = \"identity_id\" if \"identity_id\" in fieldnames else None\n+    if key_field is None and \"person_id\" in fieldnames:\n+        key_field = \"person_id\"\n+    if key_field is None:\n+        return\n+    lookup = _identity_name_lookup(payload)\n+    if not lookup:\n+        return\n+    updated = False\n+    for row in rows:\n+        identifier = str(row.get(key_field, \"\") or \"\").strip()\n+        if not identifier:\n+            continue\n+        name = lookup.get(identifier)\n+        if not name or row.get(\"name\") == name:\n+            continue\n+        row[\"name\"] = name\n+        updated = True\n+    if not updated:\n+        return\n+    out_fields = list(fieldnames)\n+    if \"name\" not in out_fields:\n+        out_fields.append(\"name\")\n+    try:\n+        with csv_path.open(\"w\", encoding=\"utf-8\", newline=\"\") as handle:\n+            writer = csv.DictWriter(handle, fieldnames=out_fields)\n+            writer.writeheader()\n+            for row in rows:\n+                if \"name\" not in row:\n+                    row[\"name\"] = \"\"\n+                writer.writerow(row)\n+    except (OSError, csv.Error) as exc:\n+        LOGGER.warning(\"Failed to update screentime.csv for %s: %s\", ep_id, exc)\n+\n+\n+def _next_identity_id(entries: Sequence[Dict[str, Any]]) -> str:\n+    max_idx = 0\n+    for entry in entries:\n+        raw = entry.get(\"identity_id\") or entry.get(\"id\")\n+        if not raw:\n+            continue\n+        digits = \"\".join(ch for ch in str(raw) if ch.isdigit())\n+        try:\n+            idx = int(digits)\n+        except ValueError:\n+            continue\n+        max_idx = max(max_idx, idx)\n+    return f\"id_{max_idx + 1:04d}\"\n+\n+\n+def _track_dir_name(track_id: int) -> str:\n+    return f\"track_{int(track_id):04d}\"\n+\n+\n+def _crop_rel(track_id: int, frame_idx: int) -> str:\n+    return f\"crops/{_track_dir_name(track_id)}/frame_{int(frame_idx):06d}.jpg\"\n+\n+\n+def _thumb_rel(track_id: int, frame_idx: int) -> str:\n+    return f\"{_track_dir_name(track_id)}/thumb_{int(frame_idx):06d}.jpg\"\n+\n+\n+def _faces_by_track(rows: Sequence[Dict[str, Any]]) -> Dict[int, List[Dict[str, Any]]]:\n+    grouped: Dict[int, List[Dict[str, Any]]] = {}\n+    for row in rows:\n+        try:\n+            track_id = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        grouped.setdefault(track_id, []).append(row)\n+    for items in grouped.values():\n+        items.sort(key=lambda r: int(r.get(\"frame_idx\", 0)))\n+    return grouped\n+\n+\n+def _next_track_id(tracks: Sequence[Dict[str, Any]]) -> int:\n+    max_id = 0\n+    for entry in tracks:\n+        try:\n+            max_id = max(max_id, int(entry.get(\"track_id\", 0)))\n+        except (TypeError, ValueError):\n+            continue\n+    return max_id + 1 if max_id >= 0 else 1\n+\n+\n+def _track_lookup(tracks: Sequence[Dict[str, Any]]) -> Dict[int, Dict[str, Any]]:\n+    lookup: Dict[int, Dict[str, Any]] = {}\n+    for entry in tracks:\n+        try:\n+            track_id = int(entry.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        if track_id >= 0:\n+            lookup[track_id] = entry\n+    return lookup\n+\n+\n+def cluster_track_summary(\n+    ep_id: str,\n+    *,\n+    include: Iterable[str] | None = None,\n+    limit_per_cluster: int | None = None,\n+) -> Dict[str, Any]:\n+    include_set = {identity.lower() for identity in include} if include else None\n+    identities_payload = load_identities(ep_id)\n+    tracks = load_tracks(ep_id)\n+    faces = load_faces(ep_id)\n+    track_lookup = _track_lookup(tracks)\n+    face_counts: Dict[int, int] = {}\n+    for row in faces:\n+        try:\n+            tid = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        face_counts[tid] = face_counts.get(tid, 0) + 1\n+\n+    clusters: List[Dict[str, Any]] = []\n+    for identity in identities_payload.get(\"identities\", []):\n+        identity_id = identity.get(\"identity_id\")\n+        if not identity_id:\n+            continue\n+        if include_set and identity_id.lower() not in include_set:\n+            continue\n+        track_ids = identity.get(\"track_ids\", []) or []\n+        tracks_payload: List[Dict[str, Any]] = []\n+        for raw_tid in track_ids:\n+            try:\n+                tid = int(raw_tid)\n+            except (TypeError, ValueError):\n+                continue\n+            track_row = track_lookup.get(tid)\n+            if not track_row:\n+                continue\n+            thumb_url = _thumbnail_url(ep_id, track_row.get(\"thumb_rel_path\"), track_row.get(\"thumb_s3_key\"))\n+            tracks_payload.append(\n+                {\n+                    \"track_id\": tid,\n+                    \"faces\": track_row.get(\"faces_count\") or face_counts.get(tid) or 0,\n+                    \"frames\": track_row.get(\"frame_count\"),\n+                    \"rep_thumb_url\": thumb_url,\n+                }\n+            )\n+        if limit_per_cluster:\n+            limit = max(1, int(limit_per_cluster))\n+            tracks_payload = tracks_payload[:limit]\n+        clusters.append(\n+            {\n+                \"identity_id\": identity_id,\n+                \"name\": identity.get(\"name\"),\n+                \"label\": identity.get(\"label\"),\n+                \"counts\": {\n+                    \"tracks\": len(track_ids),\n+                    \"faces\": identity.get(\"faces\")\n+                    or sum(face_counts.get(int(tid), 0) for tid in track_ids),\n+                },\n+                \"tracks\": tracks_payload,\n+            }\n+        )\n+    return {\"ep_id\": ep_id, \"clusters\": clusters}\n+\n+\n+def _rebuild_track_entry(track_entry: Dict[str, Any], face_rows: Sequence[Dict[str, Any]]) -> Dict[str, Any]:\n+    if not face_rows:\n+        return track_entry\n+    sorted_rows = sorted(face_rows, key=lambda row: int(row.get(\"frame_idx\", 0)))\n+    track_entry[\"frame_count\"] = len(sorted_rows)\n+    track_entry[\"faces_count\"] = len(sorted_rows)\n+    ts_values = [row.get(\"ts\") for row in sorted_rows if isinstance(row.get(\"ts\"), (int, float))]\n+    if ts_values:\n+        track_entry[\"first_ts\"] = float(min(ts_values))\n+        track_entry[\"last_ts\"] = float(max(ts_values))\n+    sample_limit = min(len(sorted_rows), 10)\n+    bboxes = []\n+    for row in sorted_rows[:sample_limit]:\n+        entry = {\n+            \"frame_idx\": row.get(\"frame_idx\"),\n+            \"ts\": row.get(\"ts\"),\n+            \"bbox_xyxy\": row.get(\"bbox_xyxy\"),\n+            \"landmarks\": row.get(\"landmarks\"),\n+        }\n+        bboxes.append(entry)\n+    if bboxes:\n+        track_entry[\"bboxes_sampled\"] = bboxes\n+    thumb_rel = sorted_rows[0].get(\"thumb_rel_path\")\n+    thumb_s3 = sorted_rows[0].get(\"thumb_s3_key\")\n+    if thumb_rel:\n+        track_entry[\"thumb_rel_path\"] = thumb_rel\n+    if thumb_s3:\n+        track_entry[\"thumb_s3_key\"] = thumb_s3\n+    return track_entry\n+\n+\n+def _write_track_index(ep_id: str, track_id: int, face_rows: Sequence[Dict[str, Any]], ctx) -> None:\n+    frames_root = get_path(ep_id, \"frames_root\")\n+    crops_dir = frames_root / \"crops\" / _track_dir_name(track_id)\n+    index_path = crops_dir / \"index.json\"\n+    if not face_rows:\n+        try:\n+            index_path.unlink()\n+        except FileNotFoundError:\n+            pass\n+        return\n+    crops_dir.mkdir(parents=True, exist_ok=True)\n+    entries = [\n+        {\n+            \"key\": f\"{_track_dir_name(track_id)}/frame_{int(row.get('frame_idx', 0)):06d}.jpg\",\n+            \"frame_idx\": int(row.get(\"frame_idx\", 0)),\n+            \"ts\": row.get(\"ts\"),\n+        }\n+        for row in sorted(face_rows, key=lambda r: int(r.get(\"frame_idx\", 0)))\n+    ]\n+    index_path.write_text(json.dumps(entries, indent=2), encoding=\"utf-8\")\n+    try:\n+        STORAGE.put_artifact(ctx, \"crops\", index_path, f\"{_track_dir_name(track_id)}/index.json\")\n+    except Exception:\n+        pass\n+\n+\n+def rename_identity(ep_id: str, identity_id: str, label: str | None) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identity = next((item for item in payload.get(\"identities\", []) if item.get(\"identity_id\") == identity_id), None)\n+    if not identity:\n+        raise ValueError(\"identity_not_found\")\n+    normalized = (label or \"\").strip()\n+    identity[\"label\"] = normalized or None\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return identity\n+\n+\n+def merge_identities(ep_id: str, source_id: str, target_id: str) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    source = next((item for item in identities if item.get(\"identity_id\") == source_id), None)\n+    target = next((item for item in identities if item.get(\"identity_id\") == target_id), None)\n+    if not source or not target:\n+        raise ValueError(\"identity_not_found\")\n+    merged = set(target.get(\"track_ids\", []) or [])\n+    for tid in source.get(\"track_ids\", []) or []:\n+        merged.add(int(tid))\n+    target[\"track_ids\"] = sorted({int(val) for val in merged})\n+    payload[\"identities\"] = [item for item in identities if item.get(\"identity_id\") != source_id]\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return target\n+\n+\n+def move_track(ep_id: str, track_id: int, target_identity_id: str | None) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    source_identity = None\n+    target_identity = None\n+    for identity in identities:\n+        track_ids = identity.get(\"track_ids\", []) or []\n+        if track_id in track_ids:\n+            source_identity = identity\n+        if target_identity_id and identity.get(\"identity_id\") == target_identity_id:\n+            target_identity = identity\n+    if target_identity_id and target_identity is None:\n+        raise ValueError(\"target_not_found\")\n+    if source_identity and track_id in source_identity.get(\"track_ids\", []):\n+        source_identity[\"track_ids\"] = [tid for tid in source_identity[\"track_ids\"] if tid != track_id]\n+    if target_identity is not None:\n+        target_identity.setdefault(\"track_ids\", [])\n+        if track_id not in target_identity[\"track_ids\"]:\n+            target_identity[\"track_ids\"].append(track_id)\n+            target_identity[\"track_ids\"] = sorted(target_identity[\"track_ids\"])\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return {\"identity_id\": target_identity_id, \"track_ids\": target_identity[\"track_ids\"] if target_identity else []}\n+\n+\n+def drop_track(ep_id: str, track_id: int) -> Dict[str, Any]:\n+    tracks = load_tracks(ep_id)\n+    kept_tracks = [row for row in tracks if int(row.get(\"track_id\", -1)) != track_id]\n+    if len(kept_tracks) == len(tracks):\n+        raise ValueError(\"track_not_found\")\n+    tracks_path = write_tracks(ep_id, kept_tracks)\n+    identities = load_identities(ep_id)\n+    for identity in identities.get(\"identities\", []):\n+        identity[\"track_ids\"] = [tid for tid in identity.get(\"track_ids\", []) if tid != track_id]\n+    update_identity_stats(ep_id, identities)\n+    identities_path = write_identities(ep_id, identities)\n+    sync_manifests(ep_id, tracks_path, identities_path)\n+    return {\"track_id\": track_id, \"remaining_tracks\": len(kept_tracks)}\n+\n+\n+def drop_frame(ep_id: str, track_id: int, frame_idx: int, delete_assets: bool = False) -> Dict[str, Any]:\n+    faces = load_faces(ep_id)\n+    removed = [\n+        row\n+        for row in faces\n+        if int(row.get(\"track_id\", -1)) == track_id and int(row.get(\"frame_idx\", -1)) == frame_idx\n+    ]\n+    if not removed:\n+        raise ValueError(\"frame_not_found\")\n+    faces = [row for row in faces if row not in removed]\n+    faces_path = write_faces(ep_id, faces)\n+    thumbs_root = get_path(ep_id, \"frames_root\") / \"thumbs\"\n+    crops_root = get_path(ep_id, \"frames_root\")\n+    if delete_assets:\n+        for row in removed:\n+            thumb_rel = row.get(\"thumb_rel_path\")\n+            if isinstance(thumb_rel, str):\n+                thumb_abs = thumbs_root / thumb_rel\n+                try:\n+                    thumb_abs.unlink()\n+                except FileNotFoundError:","path":"apps/api/services/identities.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:14Z","updated_at":"2025-11-11T20:00:23Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565647","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565647"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565647"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565647/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":484,"original_line":484,"side":"RIGHT","author_association":"NONE","original_position":484,"position":484,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565658","pull_request_review_id":3449784939,"id":2515565658,"node_id":"PRRC_kwDOQRcQs86V8Hxa","diff_hunk":"@@ -0,0 +1,691 @@\n+from __future__ import annotations\n+\n+import csv\n+import json\n+import logging\n+import os\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, List, Sequence\n+\n+from py_screenalytics.artifacts import get_path\n+\n+from apps.api.services import roster as roster_service\n+from apps.api.services.storage import StorageService, artifact_prefixes, episode_context_from_id\n+from apps.shared.storage import s3_write_json, use_s3\n+\n+STORAGE = StorageService()\n+LOGGER = logging.getLogger(__name__)\n+\n+\n+def _manifests_dir(ep_id: str) -> Path:\n+    return get_path(ep_id, \"detections\").parent\n+\n+\n+def _faces_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"faces.jsonl\"\n+\n+\n+def _identities_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"identities.json\"\n+\n+\n+def _tracks_path(ep_id: str) -> Path:\n+    return get_path(ep_id, \"tracks\")\n+\n+\n+def _thumbs_root(ep_id: str) -> Path:\n+    return get_path(ep_id, \"frames_root\") / \"thumbs\"\n+\n+\n+def _thumbnail_url(ep_id: str, rel_path: str | None, s3_key: str | None) -> str | None:\n+    if s3_key:\n+        url = STORAGE.presign_get(str(s3_key))\n+        if url:\n+            return url\n+    if not rel_path:\n+        return None\n+    local = _thumbs_root(ep_id) / rel_path\n+    if local.exists():\n+        return str(local)\n+    return None\n+\n+\n+def _analytics_root(ep_id: str) -> Path:\n+    data_root = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+    return data_root / \"analytics\" / ep_id\n+\n+\n+def _screentime_csv_path(ep_id: str) -> Path:\n+    return _analytics_root(ep_id) / \"screentime.csv\"\n+\n+\n+def _read_json_lines(path: Path) -> List[Dict[str, Any]]:\n+    if not path.exists():\n+        return []\n+    rows: List[Dict[str, Any]] = []\n+    with path.open(\"r\", encoding=\"utf-8\") as handle:\n+        for line in handle:\n+            line = line.strip()\n+            if not line:\n+                continue\n+            try:\n+                obj = json.loads(line)\n+            except json.JSONDecodeError:\n+                continue\n+            if isinstance(obj, dict):\n+                rows.append(obj)\n+    return rows\n+\n+\n+def _write_json_lines(path: Path, rows: List[Dict[str, Any]]) -> Path:\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    with path.open(\"w\", encoding=\"utf-8\") as handle:\n+        for row in rows:\n+            handle.write(json.dumps(row) + \"\\n\")\n+    return path\n+\n+\n+def load_faces(ep_id: str) -> List[Dict[str, Any]]:\n+    return _read_json_lines(_faces_path(ep_id))\n+\n+\n+def write_faces(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    return _write_json_lines(_faces_path(ep_id), rows)\n+\n+\n+def load_tracks(ep_id: str) -> List[Dict[str, Any]]:\n+    return _read_json_lines(_tracks_path(ep_id))\n+\n+\n+def write_tracks(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    return _write_json_lines(_tracks_path(ep_id), rows)\n+\n+\n+def load_identities(ep_id: str) -> Dict[str, Any]:\n+    path = _identities_path(ep_id)\n+    if not path.exists():\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+    try:\n+        return json.loads(path.read_text(encoding=\"utf-8\"))\n+    except json.JSONDecodeError:\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+\n+\n+def write_identities(ep_id: str, payload: Dict[str, Any]) -> Path:\n+    path = _identities_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n+    return path\n+\n+\n+def update_identity_stats(ep_id: str, payload: Dict[str, Any]) -> None:\n+    faces_count = len(load_faces(ep_id))\n+    payload.setdefault(\"stats\", {})\n+    payload[\"stats\"][\"faces\"] = faces_count\n+    payload[\"stats\"][\"clusters\"] = len(payload.get(\"identities\", []))\n+\n+\n+def sync_manifests(ep_id: str, *paths: Path) -> None:\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+    except ValueError:\n+        return\n+    for path in paths:\n+        if path and path.exists():\n+            try:\n+                STORAGE.put_artifact(ctx, \"manifests\", path, path.name)\n+            except Exception:\n+                continue\n+\n+\n+def _identity_rows(payload: Dict[str, Any]) -> List[Dict[str, Any]]:\n+    identities = payload.get(\"identities\")\n+    if isinstance(identities, list):\n+        return identities\n+    if isinstance(payload, list):  # legacy format\n+        return payload  # type: ignore[return-value]\n+    return []\n+\n+\n+def _identity_name_lookup(payload: Dict[str, Any]) -> Dict[str, str]:\n+    mapping: Dict[str, str] = {}\n+    for entry in _identity_rows(payload):\n+        key = entry.get(\"identity_id\") or entry.get(\"id\")\n+        name = entry.get(\"name\")\n+        if not key or not name:\n+            continue\n+        mapping[str(key)] = str(name)\n+    return mapping\n+\n+\n+def _annotate_screentime_csv(ep_id: str, payload: Dict[str, Any]) -> None:\n+    csv_path = _screentime_csv_path(ep_id)\n+    if not csv_path.exists():\n+        return\n+    try:\n+        with csv_path.open(\"r\", encoding=\"utf-8\", newline=\"\") as handle:\n+            reader = csv.DictReader(handle)\n+            rows = list(reader)\n+            fieldnames = reader.fieldnames or []\n+    except (OSError, csv.Error):\n+        return\n+    if not rows or not fieldnames:\n+        return\n+    key_field = \"identity_id\" if \"identity_id\" in fieldnames else None\n+    if key_field is None and \"person_id\" in fieldnames:\n+        key_field = \"person_id\"\n+    if key_field is None:\n+        return\n+    lookup = _identity_name_lookup(payload)\n+    if not lookup:\n+        return\n+    updated = False\n+    for row in rows:\n+        identifier = str(row.get(key_field, \"\") or \"\").strip()\n+        if not identifier:\n+            continue\n+        name = lookup.get(identifier)\n+        if not name or row.get(\"name\") == name:\n+            continue\n+        row[\"name\"] = name\n+        updated = True\n+    if not updated:\n+        return\n+    out_fields = list(fieldnames)\n+    if \"name\" not in out_fields:\n+        out_fields.append(\"name\")\n+    try:\n+        with csv_path.open(\"w\", encoding=\"utf-8\", newline=\"\") as handle:\n+            writer = csv.DictWriter(handle, fieldnames=out_fields)\n+            writer.writeheader()\n+            for row in rows:\n+                if \"name\" not in row:\n+                    row[\"name\"] = \"\"\n+                writer.writerow(row)\n+    except (OSError, csv.Error) as exc:\n+        LOGGER.warning(\"Failed to update screentime.csv for %s: %s\", ep_id, exc)\n+\n+\n+def _next_identity_id(entries: Sequence[Dict[str, Any]]) -> str:\n+    max_idx = 0\n+    for entry in entries:\n+        raw = entry.get(\"identity_id\") or entry.get(\"id\")\n+        if not raw:\n+            continue\n+        digits = \"\".join(ch for ch in str(raw) if ch.isdigit())\n+        try:\n+            idx = int(digits)\n+        except ValueError:\n+            continue\n+        max_idx = max(max_idx, idx)\n+    return f\"id_{max_idx + 1:04d}\"\n+\n+\n+def _track_dir_name(track_id: int) -> str:\n+    return f\"track_{int(track_id):04d}\"\n+\n+\n+def _crop_rel(track_id: int, frame_idx: int) -> str:\n+    return f\"crops/{_track_dir_name(track_id)}/frame_{int(frame_idx):06d}.jpg\"\n+\n+\n+def _thumb_rel(track_id: int, frame_idx: int) -> str:\n+    return f\"{_track_dir_name(track_id)}/thumb_{int(frame_idx):06d}.jpg\"\n+\n+\n+def _faces_by_track(rows: Sequence[Dict[str, Any]]) -> Dict[int, List[Dict[str, Any]]]:\n+    grouped: Dict[int, List[Dict[str, Any]]] = {}\n+    for row in rows:\n+        try:\n+            track_id = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        grouped.setdefault(track_id, []).append(row)\n+    for items in grouped.values():\n+        items.sort(key=lambda r: int(r.get(\"frame_idx\", 0)))\n+    return grouped\n+\n+\n+def _next_track_id(tracks: Sequence[Dict[str, Any]]) -> int:\n+    max_id = 0\n+    for entry in tracks:\n+        try:\n+            max_id = max(max_id, int(entry.get(\"track_id\", 0)))\n+        except (TypeError, ValueError):\n+            continue\n+    return max_id + 1 if max_id >= 0 else 1\n+\n+\n+def _track_lookup(tracks: Sequence[Dict[str, Any]]) -> Dict[int, Dict[str, Any]]:\n+    lookup: Dict[int, Dict[str, Any]] = {}\n+    for entry in tracks:\n+        try:\n+            track_id = int(entry.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        if track_id >= 0:\n+            lookup[track_id] = entry\n+    return lookup\n+\n+\n+def cluster_track_summary(\n+    ep_id: str,\n+    *,\n+    include: Iterable[str] | None = None,\n+    limit_per_cluster: int | None = None,\n+) -> Dict[str, Any]:\n+    include_set = {identity.lower() for identity in include} if include else None\n+    identities_payload = load_identities(ep_id)\n+    tracks = load_tracks(ep_id)\n+    faces = load_faces(ep_id)\n+    track_lookup = _track_lookup(tracks)\n+    face_counts: Dict[int, int] = {}\n+    for row in faces:\n+        try:\n+            tid = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        face_counts[tid] = face_counts.get(tid, 0) + 1\n+\n+    clusters: List[Dict[str, Any]] = []\n+    for identity in identities_payload.get(\"identities\", []):\n+        identity_id = identity.get(\"identity_id\")\n+        if not identity_id:\n+            continue\n+        if include_set and identity_id.lower() not in include_set:\n+            continue\n+        track_ids = identity.get(\"track_ids\", []) or []\n+        tracks_payload: List[Dict[str, Any]] = []\n+        for raw_tid in track_ids:\n+            try:\n+                tid = int(raw_tid)\n+            except (TypeError, ValueError):\n+                continue\n+            track_row = track_lookup.get(tid)\n+            if not track_row:\n+                continue\n+            thumb_url = _thumbnail_url(ep_id, track_row.get(\"thumb_rel_path\"), track_row.get(\"thumb_s3_key\"))\n+            tracks_payload.append(\n+                {\n+                    \"track_id\": tid,\n+                    \"faces\": track_row.get(\"faces_count\") or face_counts.get(tid) or 0,\n+                    \"frames\": track_row.get(\"frame_count\"),\n+                    \"rep_thumb_url\": thumb_url,\n+                }\n+            )\n+        if limit_per_cluster:\n+            limit = max(1, int(limit_per_cluster))\n+            tracks_payload = tracks_payload[:limit]\n+        clusters.append(\n+            {\n+                \"identity_id\": identity_id,\n+                \"name\": identity.get(\"name\"),\n+                \"label\": identity.get(\"label\"),\n+                \"counts\": {\n+                    \"tracks\": len(track_ids),\n+                    \"faces\": identity.get(\"faces\")\n+                    or sum(face_counts.get(int(tid), 0) for tid in track_ids),\n+                },\n+                \"tracks\": tracks_payload,\n+            }\n+        )\n+    return {\"ep_id\": ep_id, \"clusters\": clusters}\n+\n+\n+def _rebuild_track_entry(track_entry: Dict[str, Any], face_rows: Sequence[Dict[str, Any]]) -> Dict[str, Any]:\n+    if not face_rows:\n+        return track_entry\n+    sorted_rows = sorted(face_rows, key=lambda row: int(row.get(\"frame_idx\", 0)))\n+    track_entry[\"frame_count\"] = len(sorted_rows)\n+    track_entry[\"faces_count\"] = len(sorted_rows)\n+    ts_values = [row.get(\"ts\") for row in sorted_rows if isinstance(row.get(\"ts\"), (int, float))]\n+    if ts_values:\n+        track_entry[\"first_ts\"] = float(min(ts_values))\n+        track_entry[\"last_ts\"] = float(max(ts_values))\n+    sample_limit = min(len(sorted_rows), 10)\n+    bboxes = []\n+    for row in sorted_rows[:sample_limit]:\n+        entry = {\n+            \"frame_idx\": row.get(\"frame_idx\"),\n+            \"ts\": row.get(\"ts\"),\n+            \"bbox_xyxy\": row.get(\"bbox_xyxy\"),\n+            \"landmarks\": row.get(\"landmarks\"),\n+        }\n+        bboxes.append(entry)\n+    if bboxes:\n+        track_entry[\"bboxes_sampled\"] = bboxes\n+    thumb_rel = sorted_rows[0].get(\"thumb_rel_path\")\n+    thumb_s3 = sorted_rows[0].get(\"thumb_s3_key\")\n+    if thumb_rel:\n+        track_entry[\"thumb_rel_path\"] = thumb_rel\n+    if thumb_s3:\n+        track_entry[\"thumb_s3_key\"] = thumb_s3\n+    return track_entry\n+\n+\n+def _write_track_index(ep_id: str, track_id: int, face_rows: Sequence[Dict[str, Any]], ctx) -> None:\n+    frames_root = get_path(ep_id, \"frames_root\")\n+    crops_dir = frames_root / \"crops\" / _track_dir_name(track_id)\n+    index_path = crops_dir / \"index.json\"\n+    if not face_rows:\n+        try:\n+            index_path.unlink()\n+        except FileNotFoundError:\n+            pass\n+        return\n+    crops_dir.mkdir(parents=True, exist_ok=True)\n+    entries = [\n+        {\n+            \"key\": f\"{_track_dir_name(track_id)}/frame_{int(row.get('frame_idx', 0)):06d}.jpg\",\n+            \"frame_idx\": int(row.get(\"frame_idx\", 0)),\n+            \"ts\": row.get(\"ts\"),\n+        }\n+        for row in sorted(face_rows, key=lambda r: int(r.get(\"frame_idx\", 0)))\n+    ]\n+    index_path.write_text(json.dumps(entries, indent=2), encoding=\"utf-8\")\n+    try:\n+        STORAGE.put_artifact(ctx, \"crops\", index_path, f\"{_track_dir_name(track_id)}/index.json\")\n+    except Exception:\n+        pass\n+\n+\n+def rename_identity(ep_id: str, identity_id: str, label: str | None) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identity = next((item for item in payload.get(\"identities\", []) if item.get(\"identity_id\") == identity_id), None)\n+    if not identity:\n+        raise ValueError(\"identity_not_found\")\n+    normalized = (label or \"\").strip()\n+    identity[\"label\"] = normalized or None\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return identity\n+\n+\n+def merge_identities(ep_id: str, source_id: str, target_id: str) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    source = next((item for item in identities if item.get(\"identity_id\") == source_id), None)\n+    target = next((item for item in identities if item.get(\"identity_id\") == target_id), None)\n+    if not source or not target:\n+        raise ValueError(\"identity_not_found\")\n+    merged = set(target.get(\"track_ids\", []) or [])\n+    for tid in source.get(\"track_ids\", []) or []:\n+        merged.add(int(tid))\n+    target[\"track_ids\"] = sorted({int(val) for val in merged})\n+    payload[\"identities\"] = [item for item in identities if item.get(\"identity_id\") != source_id]\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return target\n+\n+\n+def move_track(ep_id: str, track_id: int, target_identity_id: str | None) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    source_identity = None\n+    target_identity = None\n+    for identity in identities:\n+        track_ids = identity.get(\"track_ids\", []) or []\n+        if track_id in track_ids:\n+            source_identity = identity\n+        if target_identity_id and identity.get(\"identity_id\") == target_identity_id:\n+            target_identity = identity\n+    if target_identity_id and target_identity is None:\n+        raise ValueError(\"target_not_found\")\n+    if source_identity and track_id in source_identity.get(\"track_ids\", []):\n+        source_identity[\"track_ids\"] = [tid for tid in source_identity[\"track_ids\"] if tid != track_id]\n+    if target_identity is not None:\n+        target_identity.setdefault(\"track_ids\", [])\n+        if track_id not in target_identity[\"track_ids\"]:\n+            target_identity[\"track_ids\"].append(track_id)\n+            target_identity[\"track_ids\"] = sorted(target_identity[\"track_ids\"])\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return {\"identity_id\": target_identity_id, \"track_ids\": target_identity[\"track_ids\"] if target_identity else []}\n+\n+\n+def drop_track(ep_id: str, track_id: int) -> Dict[str, Any]:\n+    tracks = load_tracks(ep_id)\n+    kept_tracks = [row for row in tracks if int(row.get(\"track_id\", -1)) != track_id]\n+    if len(kept_tracks) == len(tracks):\n+        raise ValueError(\"track_not_found\")\n+    tracks_path = write_tracks(ep_id, kept_tracks)\n+    identities = load_identities(ep_id)\n+    for identity in identities.get(\"identities\", []):\n+        identity[\"track_ids\"] = [tid for tid in identity.get(\"track_ids\", []) if tid != track_id]\n+    update_identity_stats(ep_id, identities)\n+    identities_path = write_identities(ep_id, identities)\n+    sync_manifests(ep_id, tracks_path, identities_path)\n+    return {\"track_id\": track_id, \"remaining_tracks\": len(kept_tracks)}\n+\n+\n+def drop_frame(ep_id: str, track_id: int, frame_idx: int, delete_assets: bool = False) -> Dict[str, Any]:\n+    faces = load_faces(ep_id)\n+    removed = [\n+        row\n+        for row in faces\n+        if int(row.get(\"track_id\", -1)) == track_id and int(row.get(\"frame_idx\", -1)) == frame_idx\n+    ]\n+    if not removed:\n+        raise ValueError(\"frame_not_found\")\n+    faces = [row for row in faces if row not in removed]\n+    faces_path = write_faces(ep_id, faces)\n+    thumbs_root = get_path(ep_id, \"frames_root\") / \"thumbs\"\n+    crops_root = get_path(ep_id, \"frames_root\")\n+    if delete_assets:\n+        for row in removed:\n+            thumb_rel = row.get(\"thumb_rel_path\")\n+            if isinstance(thumb_rel, str):\n+                thumb_abs = thumbs_root / thumb_rel\n+                try:\n+                    thumb_abs.unlink()\n+                except FileNotFoundError:\n+                    pass\n+            crop_rel = row.get(\"crop_rel_path\")\n+            if isinstance(crop_rel, str):\n+                crop_abs = crops_root / crop_rel\n+                try:\n+                    crop_abs.unlink()\n+                except FileNotFoundError:","path":"apps/api/services/identities.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:15Z","updated_at":"2025-11-11T20:00:23Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565658","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565658"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565658"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565658/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":491,"original_line":491,"side":"RIGHT","author_association":"NONE","original_position":491,"position":491,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565674","pull_request_review_id":3449784939,"id":2515565674,"node_id":"PRRC_kwDOQRcQs86V8Hxq","diff_hunk":"@@ -0,0 +1,691 @@\n+from __future__ import annotations\n+\n+import csv\n+import json\n+import logging\n+import os\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, List, Sequence\n+\n+from py_screenalytics.artifacts import get_path\n+\n+from apps.api.services import roster as roster_service\n+from apps.api.services.storage import StorageService, artifact_prefixes, episode_context_from_id\n+from apps.shared.storage import s3_write_json, use_s3\n+\n+STORAGE = StorageService()\n+LOGGER = logging.getLogger(__name__)\n+\n+\n+def _manifests_dir(ep_id: str) -> Path:\n+    return get_path(ep_id, \"detections\").parent\n+\n+\n+def _faces_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"faces.jsonl\"\n+\n+\n+def _identities_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"identities.json\"\n+\n+\n+def _tracks_path(ep_id: str) -> Path:\n+    return get_path(ep_id, \"tracks\")\n+\n+\n+def _thumbs_root(ep_id: str) -> Path:\n+    return get_path(ep_id, \"frames_root\") / \"thumbs\"\n+\n+\n+def _thumbnail_url(ep_id: str, rel_path: str | None, s3_key: str | None) -> str | None:\n+    if s3_key:\n+        url = STORAGE.presign_get(str(s3_key))\n+        if url:\n+            return url\n+    if not rel_path:\n+        return None\n+    local = _thumbs_root(ep_id) / rel_path\n+    if local.exists():\n+        return str(local)\n+    return None\n+\n+\n+def _analytics_root(ep_id: str) -> Path:\n+    data_root = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+    return data_root / \"analytics\" / ep_id\n+\n+\n+def _screentime_csv_path(ep_id: str) -> Path:\n+    return _analytics_root(ep_id) / \"screentime.csv\"\n+\n+\n+def _read_json_lines(path: Path) -> List[Dict[str, Any]]:\n+    if not path.exists():\n+        return []\n+    rows: List[Dict[str, Any]] = []\n+    with path.open(\"r\", encoding=\"utf-8\") as handle:\n+        for line in handle:\n+            line = line.strip()\n+            if not line:\n+                continue\n+            try:\n+                obj = json.loads(line)\n+            except json.JSONDecodeError:\n+                continue\n+            if isinstance(obj, dict):\n+                rows.append(obj)\n+    return rows\n+\n+\n+def _write_json_lines(path: Path, rows: List[Dict[str, Any]]) -> Path:\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    with path.open(\"w\", encoding=\"utf-8\") as handle:\n+        for row in rows:\n+            handle.write(json.dumps(row) + \"\\n\")\n+    return path\n+\n+\n+def load_faces(ep_id: str) -> List[Dict[str, Any]]:\n+    return _read_json_lines(_faces_path(ep_id))\n+\n+\n+def write_faces(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    return _write_json_lines(_faces_path(ep_id), rows)\n+\n+\n+def load_tracks(ep_id: str) -> List[Dict[str, Any]]:\n+    return _read_json_lines(_tracks_path(ep_id))\n+\n+\n+def write_tracks(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    return _write_json_lines(_tracks_path(ep_id), rows)\n+\n+\n+def load_identities(ep_id: str) -> Dict[str, Any]:\n+    path = _identities_path(ep_id)\n+    if not path.exists():\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+    try:\n+        return json.loads(path.read_text(encoding=\"utf-8\"))\n+    except json.JSONDecodeError:\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+\n+\n+def write_identities(ep_id: str, payload: Dict[str, Any]) -> Path:\n+    path = _identities_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n+    return path\n+\n+\n+def update_identity_stats(ep_id: str, payload: Dict[str, Any]) -> None:\n+    faces_count = len(load_faces(ep_id))\n+    payload.setdefault(\"stats\", {})\n+    payload[\"stats\"][\"faces\"] = faces_count\n+    payload[\"stats\"][\"clusters\"] = len(payload.get(\"identities\", []))\n+\n+\n+def sync_manifests(ep_id: str, *paths: Path) -> None:\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+    except ValueError:\n+        return\n+    for path in paths:\n+        if path and path.exists():\n+            try:\n+                STORAGE.put_artifact(ctx, \"manifests\", path, path.name)\n+            except Exception:\n+                continue\n+\n+\n+def _identity_rows(payload: Dict[str, Any]) -> List[Dict[str, Any]]:\n+    identities = payload.get(\"identities\")\n+    if isinstance(identities, list):\n+        return identities\n+    if isinstance(payload, list):  # legacy format\n+        return payload  # type: ignore[return-value]\n+    return []\n+\n+\n+def _identity_name_lookup(payload: Dict[str, Any]) -> Dict[str, str]:\n+    mapping: Dict[str, str] = {}\n+    for entry in _identity_rows(payload):\n+        key = entry.get(\"identity_id\") or entry.get(\"id\")\n+        name = entry.get(\"name\")\n+        if not key or not name:\n+            continue\n+        mapping[str(key)] = str(name)\n+    return mapping\n+\n+\n+def _annotate_screentime_csv(ep_id: str, payload: Dict[str, Any]) -> None:\n+    csv_path = _screentime_csv_path(ep_id)\n+    if not csv_path.exists():\n+        return\n+    try:\n+        with csv_path.open(\"r\", encoding=\"utf-8\", newline=\"\") as handle:\n+            reader = csv.DictReader(handle)\n+            rows = list(reader)\n+            fieldnames = reader.fieldnames or []\n+    except (OSError, csv.Error):\n+        return\n+    if not rows or not fieldnames:\n+        return\n+    key_field = \"identity_id\" if \"identity_id\" in fieldnames else None\n+    if key_field is None and \"person_id\" in fieldnames:\n+        key_field = \"person_id\"\n+    if key_field is None:\n+        return\n+    lookup = _identity_name_lookup(payload)\n+    if not lookup:\n+        return\n+    updated = False\n+    for row in rows:\n+        identifier = str(row.get(key_field, \"\") or \"\").strip()\n+        if not identifier:\n+            continue\n+        name = lookup.get(identifier)\n+        if not name or row.get(\"name\") == name:\n+            continue\n+        row[\"name\"] = name\n+        updated = True\n+    if not updated:\n+        return\n+    out_fields = list(fieldnames)\n+    if \"name\" not in out_fields:\n+        out_fields.append(\"name\")\n+    try:\n+        with csv_path.open(\"w\", encoding=\"utf-8\", newline=\"\") as handle:\n+            writer = csv.DictWriter(handle, fieldnames=out_fields)\n+            writer.writeheader()\n+            for row in rows:\n+                if \"name\" not in row:\n+                    row[\"name\"] = \"\"\n+                writer.writerow(row)\n+    except (OSError, csv.Error) as exc:\n+        LOGGER.warning(\"Failed to update screentime.csv for %s: %s\", ep_id, exc)\n+\n+\n+def _next_identity_id(entries: Sequence[Dict[str, Any]]) -> str:\n+    max_idx = 0\n+    for entry in entries:\n+        raw = entry.get(\"identity_id\") or entry.get(\"id\")\n+        if not raw:\n+            continue\n+        digits = \"\".join(ch for ch in str(raw) if ch.isdigit())\n+        try:\n+            idx = int(digits)\n+        except ValueError:\n+            continue\n+        max_idx = max(max_idx, idx)\n+    return f\"id_{max_idx + 1:04d}\"\n+\n+\n+def _track_dir_name(track_id: int) -> str:\n+    return f\"track_{int(track_id):04d}\"\n+\n+\n+def _crop_rel(track_id: int, frame_idx: int) -> str:\n+    return f\"crops/{_track_dir_name(track_id)}/frame_{int(frame_idx):06d}.jpg\"\n+\n+\n+def _thumb_rel(track_id: int, frame_idx: int) -> str:\n+    return f\"{_track_dir_name(track_id)}/thumb_{int(frame_idx):06d}.jpg\"\n+\n+\n+def _faces_by_track(rows: Sequence[Dict[str, Any]]) -> Dict[int, List[Dict[str, Any]]]:\n+    grouped: Dict[int, List[Dict[str, Any]]] = {}\n+    for row in rows:\n+        try:\n+            track_id = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        grouped.setdefault(track_id, []).append(row)\n+    for items in grouped.values():\n+        items.sort(key=lambda r: int(r.get(\"frame_idx\", 0)))\n+    return grouped\n+\n+\n+def _next_track_id(tracks: Sequence[Dict[str, Any]]) -> int:\n+    max_id = 0\n+    for entry in tracks:\n+        try:\n+            max_id = max(max_id, int(entry.get(\"track_id\", 0)))\n+        except (TypeError, ValueError):\n+            continue\n+    return max_id + 1 if max_id >= 0 else 1\n+\n+\n+def _track_lookup(tracks: Sequence[Dict[str, Any]]) -> Dict[int, Dict[str, Any]]:\n+    lookup: Dict[int, Dict[str, Any]] = {}\n+    for entry in tracks:\n+        try:\n+            track_id = int(entry.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        if track_id >= 0:\n+            lookup[track_id] = entry\n+    return lookup\n+\n+\n+def cluster_track_summary(\n+    ep_id: str,\n+    *,\n+    include: Iterable[str] | None = None,\n+    limit_per_cluster: int | None = None,\n+) -> Dict[str, Any]:\n+    include_set = {identity.lower() for identity in include} if include else None\n+    identities_payload = load_identities(ep_id)\n+    tracks = load_tracks(ep_id)\n+    faces = load_faces(ep_id)\n+    track_lookup = _track_lookup(tracks)\n+    face_counts: Dict[int, int] = {}\n+    for row in faces:\n+        try:\n+            tid = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        face_counts[tid] = face_counts.get(tid, 0) + 1\n+\n+    clusters: List[Dict[str, Any]] = []\n+    for identity in identities_payload.get(\"identities\", []):\n+        identity_id = identity.get(\"identity_id\")\n+        if not identity_id:\n+            continue\n+        if include_set and identity_id.lower() not in include_set:\n+            continue\n+        track_ids = identity.get(\"track_ids\", []) or []\n+        tracks_payload: List[Dict[str, Any]] = []\n+        for raw_tid in track_ids:\n+            try:\n+                tid = int(raw_tid)\n+            except (TypeError, ValueError):\n+                continue\n+            track_row = track_lookup.get(tid)\n+            if not track_row:\n+                continue\n+            thumb_url = _thumbnail_url(ep_id, track_row.get(\"thumb_rel_path\"), track_row.get(\"thumb_s3_key\"))\n+            tracks_payload.append(\n+                {\n+                    \"track_id\": tid,\n+                    \"faces\": track_row.get(\"faces_count\") or face_counts.get(tid) or 0,\n+                    \"frames\": track_row.get(\"frame_count\"),\n+                    \"rep_thumb_url\": thumb_url,\n+                }\n+            )\n+        if limit_per_cluster:\n+            limit = max(1, int(limit_per_cluster))\n+            tracks_payload = tracks_payload[:limit]\n+        clusters.append(\n+            {\n+                \"identity_id\": identity_id,\n+                \"name\": identity.get(\"name\"),\n+                \"label\": identity.get(\"label\"),\n+                \"counts\": {\n+                    \"tracks\": len(track_ids),\n+                    \"faces\": identity.get(\"faces\")\n+                    or sum(face_counts.get(int(tid), 0) for tid in track_ids),\n+                },\n+                \"tracks\": tracks_payload,\n+            }\n+        )\n+    return {\"ep_id\": ep_id, \"clusters\": clusters}\n+\n+\n+def _rebuild_track_entry(track_entry: Dict[str, Any], face_rows: Sequence[Dict[str, Any]]) -> Dict[str, Any]:\n+    if not face_rows:\n+        return track_entry\n+    sorted_rows = sorted(face_rows, key=lambda row: int(row.get(\"frame_idx\", 0)))\n+    track_entry[\"frame_count\"] = len(sorted_rows)\n+    track_entry[\"faces_count\"] = len(sorted_rows)\n+    ts_values = [row.get(\"ts\") for row in sorted_rows if isinstance(row.get(\"ts\"), (int, float))]\n+    if ts_values:\n+        track_entry[\"first_ts\"] = float(min(ts_values))\n+        track_entry[\"last_ts\"] = float(max(ts_values))\n+    sample_limit = min(len(sorted_rows), 10)\n+    bboxes = []\n+    for row in sorted_rows[:sample_limit]:\n+        entry = {\n+            \"frame_idx\": row.get(\"frame_idx\"),\n+            \"ts\": row.get(\"ts\"),\n+            \"bbox_xyxy\": row.get(\"bbox_xyxy\"),\n+            \"landmarks\": row.get(\"landmarks\"),\n+        }\n+        bboxes.append(entry)\n+    if bboxes:\n+        track_entry[\"bboxes_sampled\"] = bboxes\n+    thumb_rel = sorted_rows[0].get(\"thumb_rel_path\")\n+    thumb_s3 = sorted_rows[0].get(\"thumb_s3_key\")\n+    if thumb_rel:\n+        track_entry[\"thumb_rel_path\"] = thumb_rel\n+    if thumb_s3:\n+        track_entry[\"thumb_s3_key\"] = thumb_s3\n+    return track_entry\n+\n+\n+def _write_track_index(ep_id: str, track_id: int, face_rows: Sequence[Dict[str, Any]], ctx) -> None:\n+    frames_root = get_path(ep_id, \"frames_root\")\n+    crops_dir = frames_root / \"crops\" / _track_dir_name(track_id)\n+    index_path = crops_dir / \"index.json\"\n+    if not face_rows:\n+        try:\n+            index_path.unlink()\n+        except FileNotFoundError:\n+            pass\n+        return\n+    crops_dir.mkdir(parents=True, exist_ok=True)\n+    entries = [\n+        {\n+            \"key\": f\"{_track_dir_name(track_id)}/frame_{int(row.get('frame_idx', 0)):06d}.jpg\",\n+            \"frame_idx\": int(row.get(\"frame_idx\", 0)),\n+            \"ts\": row.get(\"ts\"),\n+        }\n+        for row in sorted(face_rows, key=lambda r: int(r.get(\"frame_idx\", 0)))\n+    ]\n+    index_path.write_text(json.dumps(entries, indent=2), encoding=\"utf-8\")\n+    try:\n+        STORAGE.put_artifact(ctx, \"crops\", index_path, f\"{_track_dir_name(track_id)}/index.json\")\n+    except Exception:\n+        pass\n+\n+\n+def rename_identity(ep_id: str, identity_id: str, label: str | None) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identity = next((item for item in payload.get(\"identities\", []) if item.get(\"identity_id\") == identity_id), None)\n+    if not identity:\n+        raise ValueError(\"identity_not_found\")\n+    normalized = (label or \"\").strip()\n+    identity[\"label\"] = normalized or None\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return identity\n+\n+\n+def merge_identities(ep_id: str, source_id: str, target_id: str) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    source = next((item for item in identities if item.get(\"identity_id\") == source_id), None)\n+    target = next((item for item in identities if item.get(\"identity_id\") == target_id), None)\n+    if not source or not target:\n+        raise ValueError(\"identity_not_found\")\n+    merged = set(target.get(\"track_ids\", []) or [])\n+    for tid in source.get(\"track_ids\", []) or []:\n+        merged.add(int(tid))\n+    target[\"track_ids\"] = sorted({int(val) for val in merged})\n+    payload[\"identities\"] = [item for item in identities if item.get(\"identity_id\") != source_id]\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return target\n+\n+\n+def move_track(ep_id: str, track_id: int, target_identity_id: str | None) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    source_identity = None\n+    target_identity = None\n+    for identity in identities:\n+        track_ids = identity.get(\"track_ids\", []) or []\n+        if track_id in track_ids:\n+            source_identity = identity\n+        if target_identity_id and identity.get(\"identity_id\") == target_identity_id:\n+            target_identity = identity\n+    if target_identity_id and target_identity is None:\n+        raise ValueError(\"target_not_found\")\n+    if source_identity and track_id in source_identity.get(\"track_ids\", []):\n+        source_identity[\"track_ids\"] = [tid for tid in source_identity[\"track_ids\"] if tid != track_id]\n+    if target_identity is not None:\n+        target_identity.setdefault(\"track_ids\", [])\n+        if track_id not in target_identity[\"track_ids\"]:\n+            target_identity[\"track_ids\"].append(track_id)\n+            target_identity[\"track_ids\"] = sorted(target_identity[\"track_ids\"])\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return {\"identity_id\": target_identity_id, \"track_ids\": target_identity[\"track_ids\"] if target_identity else []}\n+\n+\n+def drop_track(ep_id: str, track_id: int) -> Dict[str, Any]:\n+    tracks = load_tracks(ep_id)\n+    kept_tracks = [row for row in tracks if int(row.get(\"track_id\", -1)) != track_id]\n+    if len(kept_tracks) == len(tracks):\n+        raise ValueError(\"track_not_found\")\n+    tracks_path = write_tracks(ep_id, kept_tracks)\n+    identities = load_identities(ep_id)\n+    for identity in identities.get(\"identities\", []):\n+        identity[\"track_ids\"] = [tid for tid in identity.get(\"track_ids\", []) if tid != track_id]\n+    update_identity_stats(ep_id, identities)\n+    identities_path = write_identities(ep_id, identities)\n+    sync_manifests(ep_id, tracks_path, identities_path)\n+    return {\"track_id\": track_id, \"remaining_tracks\": len(kept_tracks)}\n+\n+\n+def drop_frame(ep_id: str, track_id: int, frame_idx: int, delete_assets: bool = False) -> Dict[str, Any]:\n+    faces = load_faces(ep_id)\n+    removed = [\n+        row\n+        for row in faces\n+        if int(row.get(\"track_id\", -1)) == track_id and int(row.get(\"frame_idx\", -1)) == frame_idx\n+    ]\n+    if not removed:\n+        raise ValueError(\"frame_not_found\")\n+    faces = [row for row in faces if row not in removed]\n+    faces_path = write_faces(ep_id, faces)\n+    thumbs_root = get_path(ep_id, \"frames_root\") / \"thumbs\"\n+    crops_root = get_path(ep_id, \"frames_root\")\n+    if delete_assets:\n+        for row in removed:\n+            thumb_rel = row.get(\"thumb_rel_path\")\n+            if isinstance(thumb_rel, str):\n+                thumb_abs = thumbs_root / thumb_rel\n+                try:\n+                    thumb_abs.unlink()\n+                except FileNotFoundError:\n+                    pass\n+            crop_rel = row.get(\"crop_rel_path\")\n+            if isinstance(crop_rel, str):\n+                crop_abs = crops_root / crop_rel\n+                try:\n+                    crop_abs.unlink()\n+                except FileNotFoundError:\n+                    pass\n+    identities = load_identities(ep_id)\n+    update_identity_stats(ep_id, identities)\n+    identities_path = write_identities(ep_id, identities)\n+    sync_manifests(ep_id, faces_path, identities_path)\n+    return {\"track_id\": track_id, \"frame_idx\": frame_idx, \"removed\": len(removed)}\n+\n+\n+def assign_identity_name(ep_id: str, identity_id: str, name: str, show: str | None = None) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    entries = _identity_rows(payload)\n+    target = next((entry for entry in entries if (entry.get(\"identity_id\") or entry.get(\"id\")) == identity_id), None)\n+    if target is None:\n+        raise ValueError(\"identity_not_found\")\n+    trimmed = (name or \"\").strip()\n+    if not trimmed:\n+        raise ValueError(\"name_required\")\n+    target[\"name\"] = trimmed\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    if use_s3():\n+        try:\n+            s3_write_json(f\"artifacts/manifests/{ep_id}/identities.json\", payload)\n+        except Exception as exc:  # pragma: no cover - best effort sync\n+            LOGGER.warning(\"Failed to mirror identities for %s: %s\", ep_id, exc)\n+    _annotate_screentime_csv(ep_id, payload)\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+        show_slug = show or ctx.show_slug\n+    except ValueError:\n+        show_slug = show or \"\"\n+    if show_slug:\n+        try:\n+            roster_service.add_if_missing(show_slug, trimmed)\n+        except ValueError:","path":"apps/api/services/identities.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:15Z","updated_at":"2025-11-11T20:00:23Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565674","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565674"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565674"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565674/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":527,"original_line":527,"side":"RIGHT","author_association":"NONE","original_position":527,"position":527,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565701","pull_request_review_id":3449784939,"id":2515565701,"node_id":"PRRC_kwDOQRcQs86V8HyF","diff_hunk":"@@ -0,0 +1,691 @@\n+from __future__ import annotations\n+\n+import csv\n+import json\n+import logging\n+import os\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, List, Sequence\n+\n+from py_screenalytics.artifacts import get_path\n+\n+from apps.api.services import roster as roster_service\n+from apps.api.services.storage import StorageService, artifact_prefixes, episode_context_from_id\n+from apps.shared.storage import s3_write_json, use_s3\n+\n+STORAGE = StorageService()\n+LOGGER = logging.getLogger(__name__)\n+\n+\n+def _manifests_dir(ep_id: str) -> Path:\n+    return get_path(ep_id, \"detections\").parent\n+\n+\n+def _faces_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"faces.jsonl\"\n+\n+\n+def _identities_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"identities.json\"\n+\n+\n+def _tracks_path(ep_id: str) -> Path:\n+    return get_path(ep_id, \"tracks\")\n+\n+\n+def _thumbs_root(ep_id: str) -> Path:\n+    return get_path(ep_id, \"frames_root\") / \"thumbs\"\n+\n+\n+def _thumbnail_url(ep_id: str, rel_path: str | None, s3_key: str | None) -> str | None:\n+    if s3_key:\n+        url = STORAGE.presign_get(str(s3_key))\n+        if url:\n+            return url\n+    if not rel_path:\n+        return None\n+    local = _thumbs_root(ep_id) / rel_path\n+    if local.exists():\n+        return str(local)\n+    return None\n+\n+\n+def _analytics_root(ep_id: str) -> Path:\n+    data_root = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+    return data_root / \"analytics\" / ep_id\n+\n+\n+def _screentime_csv_path(ep_id: str) -> Path:\n+    return _analytics_root(ep_id) / \"screentime.csv\"\n+\n+\n+def _read_json_lines(path: Path) -> List[Dict[str, Any]]:\n+    if not path.exists():\n+        return []\n+    rows: List[Dict[str, Any]] = []\n+    with path.open(\"r\", encoding=\"utf-8\") as handle:\n+        for line in handle:\n+            line = line.strip()\n+            if not line:\n+                continue\n+            try:\n+                obj = json.loads(line)\n+            except json.JSONDecodeError:\n+                continue\n+            if isinstance(obj, dict):\n+                rows.append(obj)\n+    return rows\n+\n+\n+def _write_json_lines(path: Path, rows: List[Dict[str, Any]]) -> Path:\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    with path.open(\"w\", encoding=\"utf-8\") as handle:\n+        for row in rows:\n+            handle.write(json.dumps(row) + \"\\n\")\n+    return path\n+\n+\n+def load_faces(ep_id: str) -> List[Dict[str, Any]]:\n+    return _read_json_lines(_faces_path(ep_id))\n+\n+\n+def write_faces(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    return _write_json_lines(_faces_path(ep_id), rows)\n+\n+\n+def load_tracks(ep_id: str) -> List[Dict[str, Any]]:\n+    return _read_json_lines(_tracks_path(ep_id))\n+\n+\n+def write_tracks(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    return _write_json_lines(_tracks_path(ep_id), rows)\n+\n+\n+def load_identities(ep_id: str) -> Dict[str, Any]:\n+    path = _identities_path(ep_id)\n+    if not path.exists():\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+    try:\n+        return json.loads(path.read_text(encoding=\"utf-8\"))\n+    except json.JSONDecodeError:\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+\n+\n+def write_identities(ep_id: str, payload: Dict[str, Any]) -> Path:\n+    path = _identities_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n+    return path\n+\n+\n+def update_identity_stats(ep_id: str, payload: Dict[str, Any]) -> None:\n+    faces_count = len(load_faces(ep_id))\n+    payload.setdefault(\"stats\", {})\n+    payload[\"stats\"][\"faces\"] = faces_count\n+    payload[\"stats\"][\"clusters\"] = len(payload.get(\"identities\", []))\n+\n+\n+def sync_manifests(ep_id: str, *paths: Path) -> None:\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+    except ValueError:\n+        return\n+    for path in paths:\n+        if path and path.exists():\n+            try:\n+                STORAGE.put_artifact(ctx, \"manifests\", path, path.name)\n+            except Exception:\n+                continue\n+\n+\n+def _identity_rows(payload: Dict[str, Any]) -> List[Dict[str, Any]]:\n+    identities = payload.get(\"identities\")\n+    if isinstance(identities, list):\n+        return identities\n+    if isinstance(payload, list):  # legacy format\n+        return payload  # type: ignore[return-value]\n+    return []\n+\n+\n+def _identity_name_lookup(payload: Dict[str, Any]) -> Dict[str, str]:\n+    mapping: Dict[str, str] = {}\n+    for entry in _identity_rows(payload):\n+        key = entry.get(\"identity_id\") or entry.get(\"id\")\n+        name = entry.get(\"name\")\n+        if not key or not name:\n+            continue\n+        mapping[str(key)] = str(name)\n+    return mapping\n+\n+\n+def _annotate_screentime_csv(ep_id: str, payload: Dict[str, Any]) -> None:\n+    csv_path = _screentime_csv_path(ep_id)\n+    if not csv_path.exists():\n+        return\n+    try:\n+        with csv_path.open(\"r\", encoding=\"utf-8\", newline=\"\") as handle:\n+            reader = csv.DictReader(handle)\n+            rows = list(reader)\n+            fieldnames = reader.fieldnames or []\n+    except (OSError, csv.Error):\n+        return\n+    if not rows or not fieldnames:\n+        return\n+    key_field = \"identity_id\" if \"identity_id\" in fieldnames else None\n+    if key_field is None and \"person_id\" in fieldnames:\n+        key_field = \"person_id\"\n+    if key_field is None:\n+        return\n+    lookup = _identity_name_lookup(payload)\n+    if not lookup:\n+        return\n+    updated = False\n+    for row in rows:\n+        identifier = str(row.get(key_field, \"\") or \"\").strip()\n+        if not identifier:\n+            continue\n+        name = lookup.get(identifier)\n+        if not name or row.get(\"name\") == name:\n+            continue\n+        row[\"name\"] = name\n+        updated = True\n+    if not updated:\n+        return\n+    out_fields = list(fieldnames)\n+    if \"name\" not in out_fields:\n+        out_fields.append(\"name\")\n+    try:\n+        with csv_path.open(\"w\", encoding=\"utf-8\", newline=\"\") as handle:\n+            writer = csv.DictWriter(handle, fieldnames=out_fields)\n+            writer.writeheader()\n+            for row in rows:\n+                if \"name\" not in row:\n+                    row[\"name\"] = \"\"\n+                writer.writerow(row)\n+    except (OSError, csv.Error) as exc:\n+        LOGGER.warning(\"Failed to update screentime.csv for %s: %s\", ep_id, exc)\n+\n+\n+def _next_identity_id(entries: Sequence[Dict[str, Any]]) -> str:\n+    max_idx = 0\n+    for entry in entries:\n+        raw = entry.get(\"identity_id\") or entry.get(\"id\")\n+        if not raw:\n+            continue\n+        digits = \"\".join(ch for ch in str(raw) if ch.isdigit())\n+        try:\n+            idx = int(digits)\n+        except ValueError:\n+            continue\n+        max_idx = max(max_idx, idx)\n+    return f\"id_{max_idx + 1:04d}\"\n+\n+\n+def _track_dir_name(track_id: int) -> str:\n+    return f\"track_{int(track_id):04d}\"\n+\n+\n+def _crop_rel(track_id: int, frame_idx: int) -> str:\n+    return f\"crops/{_track_dir_name(track_id)}/frame_{int(frame_idx):06d}.jpg\"\n+\n+\n+def _thumb_rel(track_id: int, frame_idx: int) -> str:\n+    return f\"{_track_dir_name(track_id)}/thumb_{int(frame_idx):06d}.jpg\"\n+\n+\n+def _faces_by_track(rows: Sequence[Dict[str, Any]]) -> Dict[int, List[Dict[str, Any]]]:\n+    grouped: Dict[int, List[Dict[str, Any]]] = {}\n+    for row in rows:\n+        try:\n+            track_id = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        grouped.setdefault(track_id, []).append(row)\n+    for items in grouped.values():\n+        items.sort(key=lambda r: int(r.get(\"frame_idx\", 0)))\n+    return grouped\n+\n+\n+def _next_track_id(tracks: Sequence[Dict[str, Any]]) -> int:\n+    max_id = 0\n+    for entry in tracks:\n+        try:\n+            max_id = max(max_id, int(entry.get(\"track_id\", 0)))\n+        except (TypeError, ValueError):\n+            continue\n+    return max_id + 1 if max_id >= 0 else 1\n+\n+\n+def _track_lookup(tracks: Sequence[Dict[str, Any]]) -> Dict[int, Dict[str, Any]]:\n+    lookup: Dict[int, Dict[str, Any]] = {}\n+    for entry in tracks:\n+        try:\n+            track_id = int(entry.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        if track_id >= 0:\n+            lookup[track_id] = entry\n+    return lookup\n+\n+\n+def cluster_track_summary(\n+    ep_id: str,\n+    *,\n+    include: Iterable[str] | None = None,\n+    limit_per_cluster: int | None = None,\n+) -> Dict[str, Any]:\n+    include_set = {identity.lower() for identity in include} if include else None\n+    identities_payload = load_identities(ep_id)\n+    tracks = load_tracks(ep_id)\n+    faces = load_faces(ep_id)\n+    track_lookup = _track_lookup(tracks)\n+    face_counts: Dict[int, int] = {}\n+    for row in faces:\n+        try:\n+            tid = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        face_counts[tid] = face_counts.get(tid, 0) + 1\n+\n+    clusters: List[Dict[str, Any]] = []\n+    for identity in identities_payload.get(\"identities\", []):\n+        identity_id = identity.get(\"identity_id\")\n+        if not identity_id:\n+            continue\n+        if include_set and identity_id.lower() not in include_set:\n+            continue\n+        track_ids = identity.get(\"track_ids\", []) or []\n+        tracks_payload: List[Dict[str, Any]] = []\n+        for raw_tid in track_ids:\n+            try:\n+                tid = int(raw_tid)\n+            except (TypeError, ValueError):\n+                continue\n+            track_row = track_lookup.get(tid)\n+            if not track_row:\n+                continue\n+            thumb_url = _thumbnail_url(ep_id, track_row.get(\"thumb_rel_path\"), track_row.get(\"thumb_s3_key\"))\n+            tracks_payload.append(\n+                {\n+                    \"track_id\": tid,\n+                    \"faces\": track_row.get(\"faces_count\") or face_counts.get(tid) or 0,\n+                    \"frames\": track_row.get(\"frame_count\"),\n+                    \"rep_thumb_url\": thumb_url,\n+                }\n+            )\n+        if limit_per_cluster:\n+            limit = max(1, int(limit_per_cluster))\n+            tracks_payload = tracks_payload[:limit]\n+        clusters.append(\n+            {\n+                \"identity_id\": identity_id,\n+                \"name\": identity.get(\"name\"),\n+                \"label\": identity.get(\"label\"),\n+                \"counts\": {\n+                    \"tracks\": len(track_ids),\n+                    \"faces\": identity.get(\"faces\")\n+                    or sum(face_counts.get(int(tid), 0) for tid in track_ids),\n+                },\n+                \"tracks\": tracks_payload,\n+            }\n+        )\n+    return {\"ep_id\": ep_id, \"clusters\": clusters}\n+\n+\n+def _rebuild_track_entry(track_entry: Dict[str, Any], face_rows: Sequence[Dict[str, Any]]) -> Dict[str, Any]:\n+    if not face_rows:\n+        return track_entry\n+    sorted_rows = sorted(face_rows, key=lambda row: int(row.get(\"frame_idx\", 0)))\n+    track_entry[\"frame_count\"] = len(sorted_rows)\n+    track_entry[\"faces_count\"] = len(sorted_rows)\n+    ts_values = [row.get(\"ts\") for row in sorted_rows if isinstance(row.get(\"ts\"), (int, float))]\n+    if ts_values:\n+        track_entry[\"first_ts\"] = float(min(ts_values))\n+        track_entry[\"last_ts\"] = float(max(ts_values))\n+    sample_limit = min(len(sorted_rows), 10)\n+    bboxes = []\n+    for row in sorted_rows[:sample_limit]:\n+        entry = {\n+            \"frame_idx\": row.get(\"frame_idx\"),\n+            \"ts\": row.get(\"ts\"),\n+            \"bbox_xyxy\": row.get(\"bbox_xyxy\"),\n+            \"landmarks\": row.get(\"landmarks\"),\n+        }\n+        bboxes.append(entry)\n+    if bboxes:\n+        track_entry[\"bboxes_sampled\"] = bboxes\n+    thumb_rel = sorted_rows[0].get(\"thumb_rel_path\")\n+    thumb_s3 = sorted_rows[0].get(\"thumb_s3_key\")\n+    if thumb_rel:\n+        track_entry[\"thumb_rel_path\"] = thumb_rel\n+    if thumb_s3:\n+        track_entry[\"thumb_s3_key\"] = thumb_s3\n+    return track_entry\n+\n+\n+def _write_track_index(ep_id: str, track_id: int, face_rows: Sequence[Dict[str, Any]], ctx) -> None:\n+    frames_root = get_path(ep_id, \"frames_root\")\n+    crops_dir = frames_root / \"crops\" / _track_dir_name(track_id)\n+    index_path = crops_dir / \"index.json\"\n+    if not face_rows:\n+        try:\n+            index_path.unlink()\n+        except FileNotFoundError:\n+            pass\n+        return\n+    crops_dir.mkdir(parents=True, exist_ok=True)\n+    entries = [\n+        {\n+            \"key\": f\"{_track_dir_name(track_id)}/frame_{int(row.get('frame_idx', 0)):06d}.jpg\",\n+            \"frame_idx\": int(row.get(\"frame_idx\", 0)),\n+            \"ts\": row.get(\"ts\"),\n+        }\n+        for row in sorted(face_rows, key=lambda r: int(r.get(\"frame_idx\", 0)))\n+    ]\n+    index_path.write_text(json.dumps(entries, indent=2), encoding=\"utf-8\")\n+    try:\n+        STORAGE.put_artifact(ctx, \"crops\", index_path, f\"{_track_dir_name(track_id)}/index.json\")\n+    except Exception:\n+        pass\n+\n+\n+def rename_identity(ep_id: str, identity_id: str, label: str | None) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identity = next((item for item in payload.get(\"identities\", []) if item.get(\"identity_id\") == identity_id), None)\n+    if not identity:\n+        raise ValueError(\"identity_not_found\")\n+    normalized = (label or \"\").strip()\n+    identity[\"label\"] = normalized or None\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return identity\n+\n+\n+def merge_identities(ep_id: str, source_id: str, target_id: str) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    source = next((item for item in identities if item.get(\"identity_id\") == source_id), None)\n+    target = next((item for item in identities if item.get(\"identity_id\") == target_id), None)\n+    if not source or not target:\n+        raise ValueError(\"identity_not_found\")\n+    merged = set(target.get(\"track_ids\", []) or [])\n+    for tid in source.get(\"track_ids\", []) or []:\n+        merged.add(int(tid))\n+    target[\"track_ids\"] = sorted({int(val) for val in merged})\n+    payload[\"identities\"] = [item for item in identities if item.get(\"identity_id\") != source_id]\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return target\n+\n+\n+def move_track(ep_id: str, track_id: int, target_identity_id: str | None) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    source_identity = None\n+    target_identity = None\n+    for identity in identities:\n+        track_ids = identity.get(\"track_ids\", []) or []\n+        if track_id in track_ids:\n+            source_identity = identity\n+        if target_identity_id and identity.get(\"identity_id\") == target_identity_id:\n+            target_identity = identity\n+    if target_identity_id and target_identity is None:\n+        raise ValueError(\"target_not_found\")\n+    if source_identity and track_id in source_identity.get(\"track_ids\", []):\n+        source_identity[\"track_ids\"] = [tid for tid in source_identity[\"track_ids\"] if tid != track_id]\n+    if target_identity is not None:\n+        target_identity.setdefault(\"track_ids\", [])\n+        if track_id not in target_identity[\"track_ids\"]:\n+            target_identity[\"track_ids\"].append(track_id)\n+            target_identity[\"track_ids\"] = sorted(target_identity[\"track_ids\"])\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return {\"identity_id\": target_identity_id, \"track_ids\": target_identity[\"track_ids\"] if target_identity else []}\n+\n+\n+def drop_track(ep_id: str, track_id: int) -> Dict[str, Any]:\n+    tracks = load_tracks(ep_id)\n+    kept_tracks = [row for row in tracks if int(row.get(\"track_id\", -1)) != track_id]\n+    if len(kept_tracks) == len(tracks):\n+        raise ValueError(\"track_not_found\")\n+    tracks_path = write_tracks(ep_id, kept_tracks)\n+    identities = load_identities(ep_id)\n+    for identity in identities.get(\"identities\", []):\n+        identity[\"track_ids\"] = [tid for tid in identity.get(\"track_ids\", []) if tid != track_id]\n+    update_identity_stats(ep_id, identities)\n+    identities_path = write_identities(ep_id, identities)\n+    sync_manifests(ep_id, tracks_path, identities_path)\n+    return {\"track_id\": track_id, \"remaining_tracks\": len(kept_tracks)}\n+\n+\n+def drop_frame(ep_id: str, track_id: int, frame_idx: int, delete_assets: bool = False) -> Dict[str, Any]:\n+    faces = load_faces(ep_id)\n+    removed = [\n+        row\n+        for row in faces\n+        if int(row.get(\"track_id\", -1)) == track_id and int(row.get(\"frame_idx\", -1)) == frame_idx\n+    ]\n+    if not removed:\n+        raise ValueError(\"frame_not_found\")\n+    faces = [row for row in faces if row not in removed]\n+    faces_path = write_faces(ep_id, faces)\n+    thumbs_root = get_path(ep_id, \"frames_root\") / \"thumbs\"\n+    crops_root = get_path(ep_id, \"frames_root\")\n+    if delete_assets:\n+        for row in removed:\n+            thumb_rel = row.get(\"thumb_rel_path\")\n+            if isinstance(thumb_rel, str):\n+                thumb_abs = thumbs_root / thumb_rel\n+                try:\n+                    thumb_abs.unlink()\n+                except FileNotFoundError:\n+                    pass\n+            crop_rel = row.get(\"crop_rel_path\")\n+            if isinstance(crop_rel, str):\n+                crop_abs = crops_root / crop_rel\n+                try:\n+                    crop_abs.unlink()\n+                except FileNotFoundError:\n+                    pass\n+    identities = load_identities(ep_id)\n+    update_identity_stats(ep_id, identities)\n+    identities_path = write_identities(ep_id, identities)\n+    sync_manifests(ep_id, faces_path, identities_path)\n+    return {\"track_id\": track_id, \"frame_idx\": frame_idx, \"removed\": len(removed)}\n+\n+\n+def assign_identity_name(ep_id: str, identity_id: str, name: str, show: str | None = None) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    entries = _identity_rows(payload)\n+    target = next((entry for entry in entries if (entry.get(\"identity_id\") or entry.get(\"id\")) == identity_id), None)\n+    if target is None:\n+        raise ValueError(\"identity_not_found\")\n+    trimmed = (name or \"\").strip()\n+    if not trimmed:\n+        raise ValueError(\"name_required\")\n+    target[\"name\"] = trimmed\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    if use_s3():\n+        try:\n+            s3_write_json(f\"artifacts/manifests/{ep_id}/identities.json\", payload)\n+        except Exception as exc:  # pragma: no cover - best effort sync\n+            LOGGER.warning(\"Failed to mirror identities for %s: %s\", ep_id, exc)\n+    _annotate_screentime_csv(ep_id, payload)\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+        show_slug = show or ctx.show_slug\n+    except ValueError:\n+        show_slug = show or \"\"\n+    if show_slug:\n+        try:\n+            roster_service.add_if_missing(show_slug, trimmed)\n+        except ValueError:\n+            pass\n+    return {\"ep_id\": ep_id, \"identity_id\": identity_id, \"name\": trimmed}\n+\n+\n+def move_frames(\n+    ep_id: str,\n+    from_track_id: int,\n+    face_ids: Sequence[str],\n+    *,\n+    target_identity_id: str | None = None,\n+    new_identity_name: str | None = None,\n+    show_id: str | None = None,\n+) -> Dict[str, Any]:\n+    if not face_ids:\n+        raise ValueError(\"face_ids_required\")\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+    except ValueError as exc:\n+        raise ValueError(\"invalid_ep_id\") from exc\n+\n+    faces = load_faces(ep_id)\n+    face_map = {str(row.get(\"face_id\")): row for row in faces}\n+    selected: List[Dict[str, Any]] = []\n+    for face_id in set(face_ids):\n+        row = face_map.get(face_id)\n+        if not row:\n+            raise ValueError(f\"face_not_found:{face_id}\")\n+        try:\n+            track_id = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            raise ValueError(f\"face_track_invalid:{face_id}\")\n+        if track_id != from_track_id:\n+            raise ValueError(f\"face_not_in_track:{face_id}\")\n+        selected.append(row)\n+\n+    if not selected:\n+        raise ValueError(\"selected_faces_missing\")\n+\n+    identities_payload = load_identities(ep_id)\n+    identities = _identity_rows(identities_payload)\n+    source_identity = next(\n+        (entry for entry in identities if from_track_id in (entry.get(\"track_ids\") or [])),\n+        None,\n+    )\n+\n+    target_identity = None\n+    if target_identity_id:\n+        target_identity = next(\n+            (entry for entry in identities if (entry.get(\"identity_id\") or entry.get(\"id\")) == target_identity_id),\n+            None,\n+        )\n+    trimmed_name = (new_identity_name or \"\").strip()\n+    if target_identity is None and trimmed_name:\n+        target_identity = next(\n+            (\n+                entry\n+                for entry in identities\n+                if isinstance(entry.get(\"name\"), str) and entry[\"name\"].lower() == trimmed_name.lower()\n+            ),\n+            None,\n+        )\n+    if target_identity is None and trimmed_name:\n+        new_identity_id = _next_identity_id(identities)\n+        target_identity = {\"identity_id\": new_identity_id, \"label\": None, \"track_ids\": []}\n+        identities.append(target_identity)\n+    if target_identity is None:\n+        raise ValueError(\"target_identity_not_found\")\n+    target_identity.setdefault(\"track_ids\", [])\n+    if trimmed_name:\n+        target_identity[\"name\"] = trimmed_name\n+        try:\n+            roster_service.add_if_missing(show_id or ctx.show_slug, trimmed_name)\n+        except ValueError:","path":"apps/api/services/identities.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:15Z","updated_at":"2025-11-11T20:00:23Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565701","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565701"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565701"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565701/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":600,"original_line":600,"side":"RIGHT","author_association":"NONE","original_position":600,"position":600,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565729","pull_request_review_id":3449784939,"id":2515565729,"node_id":"PRRC_kwDOQRcQs86V8Hyh","diff_hunk":"@@ -0,0 +1,690 @@\n+from __future__ import annotations\n+\n+import asyncio\n+import json\n+import os\n+import subprocess\n+import sys\n+from pathlib import Path\n+from typing import List, Literal\n+\n+from fastapi import APIRouter, HTTPException, Request\n+from fastapi.responses import StreamingResponse\n+from pydantic import BaseModel, Field\n+\n+PROJECT_ROOT = Path(__file__).resolve().parents[3]\n+if str(PROJECT_ROOT) not in sys.path:\n+    sys.path.append(str(PROJECT_ROOT))\n+\n+from py_screenalytics.artifacts import ensure_dirs, get_path\n+\n+from apps.api.services.episodes import EpisodeStore\n+from apps.api.services import jobs as jobs_service\n+from apps.api.services.jobs import JobNotFoundError, JobService\n+from apps.api.services.storage import artifact_prefixes, episode_context_from_id\n+\n+router = APIRouter()\n+JOB_SERVICE = JobService()\n+EPISODE_STORE = EpisodeStore()\n+DETECTOR_CHOICES = {\"retinaface\", \"yolov8face\"}\n+TRACKER_CHOICES = {\"bytetrack\", \"strongsort\"}\n+DEFAULT_DETECTOR_ENV = os.getenv(\"DEFAULT_DETECTOR\", \"retinaface\").lower()\n+DEFAULT_TRACKER_ENV = os.getenv(\"DEFAULT_TRACKER\", \"bytetrack\").lower()\n+SCENE_DETECT_DEFAULT = getattr(jobs_service, \"SCENE_DETECT_DEFAULT\", True)\n+SCENE_THRESHOLD_DEFAULT = getattr(jobs_service, \"SCENE_THRESHOLD_DEFAULT\", 0.30)\n+SCENE_MIN_LEN_DEFAULT = getattr(jobs_service, \"SCENE_MIN_LEN_DEFAULT\", 12)\n+SCENE_WARMUP_DETS_DEFAULT = getattr(jobs_service, \"SCENE_WARMUP_DETS_DEFAULT\", 3)\n+\n+\n+LEGACY_SUFFIX = \"st\" \"ub\"\n+LEGACY_KEYS = (\"use_\" + LEGACY_SUFFIX, LEGACY_SUFFIX)\n+\n+\n+def _has_legacy_marker(payload: dict | None, query_params: dict[str, str]) -> bool:\n+    if any(key in query_params for key in LEGACY_KEYS):\n+        return True\n+    if payload is None:\n+        return False\n+    return any(key in payload for key in LEGACY_KEYS)\n+\n+\n+async def _reject_legacy_payload(request: Request) -> None:\n+    query_markers = dict(request.query_params)\n+    payload_data: dict | None = None\n+    try:\n+        body_bytes = await request.body()\n+    except Exception:\n+        body_bytes = b\"\"\n+    if body_bytes:\n+        try:\n+            parsed = json.loads(body_bytes)\n+        except json.JSONDecodeError:\n+            parsed = None\n+        if isinstance(parsed, dict):\n+            payload_data = parsed\n+    if _has_legacy_marker(payload_data, query_markers):\n+        raise HTTPException(status_code=400, detail=\"Stub mode is not supported.\")\n+\n+\n+class DetectRequest(BaseModel):\n+    ep_id: str = Field(..., description=\"Episode identifier\")\n+    video: str = Field(..., description=\"Source video path or URL\")\n+    stride: int = Field(5, description=\"Frame stride for detection sampling\")\n+    fps: float | None = Field(None, description=\"Optional target FPS for sampling\")\n+    device: Literal[\"auto\", \"cpu\", \"mps\", \"cuda\"] = Field(\"auto\", description=\"Execution device\")\n+\n+\n+class TrackRequest(BaseModel):\n+    ep_id: str = Field(..., description=\"Episode identifier\")\n+\n+\n+class DetectTrackRequest(BaseModel):\n+    ep_id: str = Field(..., description=\"Episode identifier\")\n+    stride: int = Field(5, description=\"Frame stride for detection sampling\")\n+    fps: float | None = Field(None, description=\"Optional target FPS for sampling\")\n+    device: Literal[\"auto\", \"cpu\", \"mps\", \"cuda\"] = Field(\"auto\", description=\"Execution device\")\n+    save_frames: bool = Field(False, description=\"Sample full-frame JPGs to S3/local frames root\")\n+    save_crops: bool = Field(False, description=\"Save per-track crops (requires tracks)\")\n+    jpeg_quality: int = Field(85, ge=1, le=100, description=\"JPEG quality for frame/crop exports\")\n+    detector: str = Field(\n+        DEFAULT_DETECTOR_ENV,\n+        description=\"Face detector backend (retinaface or yolov8face)\",\n+    )\n+    tracker: str = Field(\n+        DEFAULT_TRACKER_ENV,\n+        description=\"Tracker backend (bytetrack or strongsort)\",\n+    )\n+    max_gap: int | None = Field(30, ge=1, description=\"Frame gap before forcing a new track\")\n+    det_thresh: float | None = Field(\n+        0.5,\n+        ge=0.0,\n+        le=1.0,\n+        description=\"RetinaFace detection threshold (0-1)\",\n+    )\n+    scene_detect: bool = Field(\n+        SCENE_DETECT_DEFAULT,\n+        description=\"Enable scene-cut histogram detection prepass\",\n+    )\n+    scene_threshold: float = Field(\n+        SCENE_THRESHOLD_DEFAULT,\n+        ge=0.0,\n+        le=2.0,\n+        description=\"Scene-cut threshold (1 - HSV histogram correlation)\",\n+    )\n+    scene_min_len: int = Field(\n+        SCENE_MIN_LEN_DEFAULT,\n+        ge=1,\n+        description=\"Minimum frames between detected cuts\",\n+    )\n+    scene_warmup_dets: int = Field(\n+        SCENE_WARMUP_DETS_DEFAULT,\n+        ge=0,\n+        description=\"Frames forced to run detection immediately after a cut\",\n+    )\n+\n+class FacesEmbedRequest(BaseModel):\n+    ep_id: str = Field(..., description=\"Episode identifier\")\n+    device: Literal[\"auto\", \"cpu\", \"mps\", \"cuda\"] | None = Field(\n+        None,\n+        description=\"Execution device (defaults to server auto-detect)\",\n+    )\n+    save_frames: bool = Field(False, description=\"Export sampled frames alongside crops\")\n+    save_crops: bool = Field(False, description=\"Export crops to data/frames + S3\")\n+    jpeg_quality: int = Field(85, ge=1, le=100, description=\"JPEG quality for face crops\")\n+    thumb_size: int = Field(256, ge=64, le=512, description=\"Square thumbnail size\")\n+\n+class ClusterRequest(BaseModel):\n+    ep_id: str = Field(..., description=\"Episode identifier\")\n+    device: Literal[\"auto\", \"cpu\", \"mps\", \"cuda\"] | None = Field(\n+        None,\n+        description=\"Execution device (defaults to server auto-detect)\",\n+    )\n+    cluster_thresh: float = Field(0.6, gt=0.0, le=1.0, description=\"Cosine distance threshold for clustering\")\n+    min_cluster_size: int = Field(2, ge=1, description=\"Minimum tracks per identity\")\n+\n+def _artifact_summary(ep_id: str) -> dict:\n+    ensure_dirs(ep_id)\n+    return {\n+        \"video\": str(get_path(ep_id, \"video\")),\n+        \"detections\": str(get_path(ep_id, \"detections\")),\n+        \"tracks\": str(get_path(ep_id, \"tracks\")),\n+        \"frames_root\": str(get_path(ep_id, \"frames_root\")),\n+    }\n+\n+\n+def _validate_episode_ready(ep_id: str) -> Path:\n+    record = EPISODE_STORE.get(ep_id)\n+    if not record:\n+        raise HTTPException(status_code=400, detail=\"Episode not tracked yet; create it via /episodes/upsert_by_id.\")\n+    video_path = get_path(ep_id, \"video\")\n+    if not video_path.exists():\n+        raise HTTPException(status_code=400, detail=\"Episode video not mirrored locally; run Mirror from S3.\")\n+    if not video_path.is_file():\n+        raise HTTPException(status_code=400, detail=\"Episode video path is invalid.\")\n+    try:\n+        with video_path.open(\"rb\"):\n+            pass\n+    except OSError as exc:\n+        raise HTTPException(status_code=400, detail=f\"Episode video unreadable: {exc}\") from exc\n+    return video_path\n+\n+\n+def _s3_prefixes(ep_id: str) -> dict | None:\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+    except ValueError:\n+        return None\n+    return artifact_prefixes(ctx)\n+\n+\n+def _progress_file_path(ep_id: str) -> Path:\n+    manifests_dir = get_path(ep_id, \"detections\").parent\n+    manifests_dir.mkdir(parents=True, exist_ok=True)\n+    return manifests_dir / \"progress.json\"\n+\n+\n+def _normalize_detector(detector: str | None) -> str:\n+    fallback = DEFAULT_DETECTOR_ENV or \"retinaface\"\n+    value = (detector or fallback).strip().lower()\n+    if value not in DETECTOR_CHOICES:\n+        raise HTTPException(status_code=400, detail=f\"Unsupported detector '{detector}'\")\n+    return value\n+\n+\n+def _normalize_tracker(tracker: str | None) -> str:\n+    fallback = DEFAULT_TRACKER_ENV or \"bytetrack\"\n+    value = (tracker or fallback).strip().lower()\n+    if value not in TRACKER_CHOICES:\n+        raise HTTPException(status_code=400, detail=f\"Unsupported tracker '{tracker}'\")\n+    return value\n+\n+\n+def _build_detect_track_command(\n+    req: DetectTrackRequest,\n+    video_path: Path,\n+    progress_path: Path,\n+    detector_value: str,\n+    tracker_value: str,\n+    det_thresh: float | None,\n+    scene_detect: bool,\n+    scene_threshold: float,\n+    scene_min_len: int,\n+    scene_warmup_dets: int,\n+) -> List[str]:\n+    command: List[str] = [\n+        sys.executable,\n+        str(PROJECT_ROOT / \"tools\" / \"episode_run.py\"),\n+        \"--ep-id\",\n+        req.ep_id,\n+        \"--video\",\n+        str(video_path),\n+        \"--stride\",\n+        str(req.stride),\n+        \"--device\",\n+        req.device,\n+        \"--progress-file\",\n+        str(progress_path),\n+    ]\n+    if req.fps is not None and req.fps > 0:\n+        command += [\"--fps\", str(req.fps)]\n+    if req.save_frames:\n+        command.append(\"--save-frames\")\n+    if req.save_crops:\n+        command.append(\"--save-crops\")\n+    if req.jpeg_quality and req.jpeg_quality != 85:\n+        command += [\"--jpeg-quality\", str(req.jpeg_quality)]\n+    command += [\"--detector\", detector_value]\n+    command += [\"--tracker\", tracker_value]\n+    if req.max_gap:\n+        command += [\"--max-gap\", str(req.max_gap)]\n+    if det_thresh is not None:\n+        command += [\"--det-thresh\", str(det_thresh)]\n+    command += [\"--scene-detect\", \"on\" if scene_detect else \"off\"]\n+    command += [\"--scene-threshold\", str(scene_threshold)]\n+    command += [\"--scene-min-len\", str(max(scene_min_len, 1))]\n+    command += [\"--scene-warmup-dets\", str(max(scene_warmup_dets, 0))]\n+    return command\n+\n+\n+def _build_faces_command(req: FacesEmbedRequest, progress_path: Path) -> List[str]:\n+    device_value = req.device or \"auto\"\n+    command: List[str] = [\n+        sys.executable,\n+        str(PROJECT_ROOT / \"tools\" / \"episode_run.py\"),\n+        \"--ep-id\",\n+        req.ep_id,\n+        \"--faces-embed\",\n+        \"--device\",\n+        device_value,\n+        \"--progress-file\",\n+        str(progress_path),\n+    ]\n+    if req.save_frames:\n+        command.append(\"--save-frames\")\n+    if req.save_crops:\n+        command.append(\"--save-crops\")\n+    if req.jpeg_quality and req.jpeg_quality != 85:\n+        command += [\"--jpeg-quality\", str(req.jpeg_quality)]\n+    return command\n+\n+\n+def _build_cluster_command(req: ClusterRequest, progress_path: Path) -> List[str]:\n+    device_value = req.device or \"auto\"\n+    command: List[str] = [\n+        sys.executable,\n+        str(PROJECT_ROOT / \"tools\" / \"episode_run.py\"),\n+        \"--ep-id\",\n+        req.ep_id,\n+        \"--cluster\",\n+        \"--device\",\n+        device_value,\n+        \"--progress-file\",\n+        str(progress_path),\n+    ]\n+    command += [\"--cluster-thresh\", str(req.cluster_thresh)]\n+    command += [\"--min-cluster-size\", str(req.min_cluster_size)]\n+    return command\n+\n+\n+def _format_sse(event_name: str, payload: dict) -> str:\n+    return f\"event: {event_name}\\ndata: {json.dumps(payload)}\\n\\n\"\n+\n+\n+def _parse_progress_line(line: str) -> dict | None:\n+    try:\n+        payload = json.loads(line)\n+    except json.JSONDecodeError:\n+        return None\n+    if not isinstance(payload, dict):\n+        return None\n+    if \"phase\" not in payload:\n+        return None\n+    return payload\n+\n+\n+def _count_lines(path: Path) -> int:\n+    if not path.exists():\n+        return 0\n+    with path.open(\"r\", encoding=\"utf-8\") as handle:\n+        return sum(1 for line in handle if line.strip())\n+\n+\n+def _wants_sse(request: Request) -> bool:\n+    accept = (request.headers.get(\"accept\") or \"\").lower()\n+    return \"text/event-stream\" in accept\n+\n+\n+def _run_job_with_optional_sse(command: List[str], request: Request):\n+    env = os.environ.copy()\n+    if _wants_sse(request):\n+        generator = _stream_progress_command(command, env, request)\n+        return StreamingResponse(generator, media_type=\"text/event-stream\", headers={\"Cache-Control\": \"no-cache\"})\n+    completed = subprocess.run(\n+        command,\n+        cwd=str(PROJECT_ROOT),\n+        capture_output=True,\n+        text=True,\n+        check=False,\n+        env=env,\n+    )\n+    if completed.returncode != 0:\n+        raise HTTPException(\n+            status_code=500,\n+            detail={\n+                \"error\": \"episode_run_failed\",\n+                \"stderr\": completed.stderr.strip(),\n+            },\n+        )\n+    return completed\n+\n+\n+async def _stream_progress_command(command: List[str], env: dict, request: Request):\n+    proc = await asyncio.create_subprocess_exec(\n+        *command,\n+        cwd=str(PROJECT_ROOT),\n+        stdout=asyncio.subprocess.PIPE,\n+        stderr=asyncio.subprocess.PIPE,\n+        env=env,\n+    )\n+    saw_terminal = False\n+    try:\n+        assert proc.stdout is not None\n+        async for raw_line in proc.stdout:\n+            line = raw_line.decode().strip()\n+            if not line:\n+                continue\n+            payload = _parse_progress_line(line)\n+            if payload is None:\n+                continue\n+            phase = str(payload.get(\"phase\", \"\")).lower()\n+            if phase == \"done\":\n+                event_name = \"done\"\n+                saw_terminal = True\n+            elif phase == \"error\":\n+                event_name = \"error\"\n+                saw_terminal = True\n+            else:\n+                event_name = \"progress\"\n+            yield _format_sse(event_name, payload)\n+            if await request.is_disconnected():\n+                proc.terminate()\n+                break\n+        await proc.wait()\n+        if proc.returncode not in (0, None) and not saw_terminal:\n+            stderr_text = \"\"\n+            if proc.stderr:\n+                stderr_bytes = await proc.stderr.read()\n+                stderr_text = stderr_bytes.decode().strip()\n+            payload = {\n+                \"phase\": \"error\",\n+                \"error\": stderr_text or f\"episode_run exited with code {proc.returncode}\",\n+                \"return_code\": proc.returncode,\n+            }\n+            yield _format_sse(\"error\", payload)\n+    finally:\n+        if proc.returncode is None:\n+            proc.kill()\n+\n+\n+@router.post(\"/detect\")\n+async def enqueue_detect(req: DetectRequest, request: Request) -> dict:\n+    await _reject_legacy_payload(request)\n+    artifacts = _artifact_summary(req.ep_id)\n+\n+    command: List[str] = [\n+        \"python\",\n+        \"tools/episode_run.py\",\n+        \"--ep-id\",\n+        req.ep_id,\n+        \"--video\",\n+        req.video,\n+        \"--stride\",\n+        str(req.stride),\n+    ]\n+    if req.fps is not None:\n+        command += [\"--fps\", str(req.fps)]\n+    command += [\"--device\", req.device]\n+\n+    return {\n+        \"job\": \"detect\",\n+        \"ep_id\": req.ep_id,\n+        \"queued\": False,\n+        \"command\": command,\n+        \"artifacts\": artifacts,\n+        \"note\": \"Queue integration pending; run command manually for now.\",\n+    }\n+\n+\n+@router.post(\"/track\")\n+def enqueue_track(req: TrackRequest) -> dict:\n+    artifacts = _artifact_summary(req.ep_id)\n+    command = [\n+        \"python\",\n+        \"FEATURES/tracking/src/bytetrack_runner.py\",\n+        \"--ep-id\",\n+        req.ep_id,\n+        \"--detections\",\n+        artifacts[\"detections\"],\n+        \"--output\",\n+        artifacts[\"tracks\"],\n+    ]\n+    return {\n+        \"job\": \"track\",\n+        \"ep_id\": req.ep_id,\n+        \"queued\": False,\n+        \"command\": command,\n+        \"artifacts\": artifacts,\n+        \"note\": \"Tracking runs locally; detection must be prepared first.\",\n+    }\n+\n+\n+@router.post(\"/detect_track\")\n+async def run_detect_track(req: DetectTrackRequest, request: Request):\n+    await _reject_legacy_payload(request)\n+    artifacts = _artifact_summary(req.ep_id)\n+    video_path = _validate_episode_ready(req.ep_id)\n+    detector_value = _normalize_detector(req.detector)\n+    tracker_value = _normalize_tracker(req.tracker)\n+    try:\n+        JOB_SERVICE.ensure_retinaface_ready(detector_value, req.device, req.det_thresh)\n+    except ValueError as exc:\n+        raise HTTPException(status_code=400, detail=str(exc)) from exc\n+    progress_path = _progress_file_path(req.ep_id)\n+    try:\n+        progress_path.unlink()\n+    except FileNotFoundError:","path":"apps/api/routers/jobs.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:16Z","updated_at":"2025-11-11T20:00:23Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565729","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565729"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565729"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565729/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":455,"original_line":455,"side":"RIGHT","author_association":"NONE","original_position":455,"position":455,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565752","pull_request_review_id":3449784939,"id":2515565752,"node_id":"PRRC_kwDOQRcQs86V8Hy4","diff_hunk":"@@ -0,0 +1,691 @@\n+from __future__ import annotations\n+\n+import csv\n+import json\n+import logging\n+import os\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, List, Sequence\n+\n+from py_screenalytics.artifacts import get_path\n+\n+from apps.api.services import roster as roster_service\n+from apps.api.services.storage import StorageService, artifact_prefixes, episode_context_from_id\n+from apps.shared.storage import s3_write_json, use_s3\n+\n+STORAGE = StorageService()\n+LOGGER = logging.getLogger(__name__)\n+\n+\n+def _manifests_dir(ep_id: str) -> Path:\n+    return get_path(ep_id, \"detections\").parent\n+\n+\n+def _faces_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"faces.jsonl\"\n+\n+\n+def _identities_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"identities.json\"\n+\n+\n+def _tracks_path(ep_id: str) -> Path:\n+    return get_path(ep_id, \"tracks\")\n+\n+\n+def _thumbs_root(ep_id: str) -> Path:\n+    return get_path(ep_id, \"frames_root\") / \"thumbs\"\n+\n+\n+def _thumbnail_url(ep_id: str, rel_path: str | None, s3_key: str | None) -> str | None:\n+    if s3_key:\n+        url = STORAGE.presign_get(str(s3_key))\n+        if url:\n+            return url\n+    if not rel_path:\n+        return None\n+    local = _thumbs_root(ep_id) / rel_path\n+    if local.exists():\n+        return str(local)\n+    return None\n+\n+\n+def _analytics_root(ep_id: str) -> Path:\n+    data_root = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+    return data_root / \"analytics\" / ep_id\n+\n+\n+def _screentime_csv_path(ep_id: str) -> Path:\n+    return _analytics_root(ep_id) / \"screentime.csv\"\n+\n+\n+def _read_json_lines(path: Path) -> List[Dict[str, Any]]:\n+    if not path.exists():\n+        return []\n+    rows: List[Dict[str, Any]] = []\n+    with path.open(\"r\", encoding=\"utf-8\") as handle:\n+        for line in handle:\n+            line = line.strip()\n+            if not line:\n+                continue\n+            try:\n+                obj = json.loads(line)\n+            except json.JSONDecodeError:\n+                continue\n+            if isinstance(obj, dict):\n+                rows.append(obj)\n+    return rows\n+\n+\n+def _write_json_lines(path: Path, rows: List[Dict[str, Any]]) -> Path:\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    with path.open(\"w\", encoding=\"utf-8\") as handle:\n+        for row in rows:\n+            handle.write(json.dumps(row) + \"\\n\")\n+    return path\n+\n+\n+def load_faces(ep_id: str) -> List[Dict[str, Any]]:\n+    return _read_json_lines(_faces_path(ep_id))\n+\n+\n+def write_faces(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    return _write_json_lines(_faces_path(ep_id), rows)\n+\n+\n+def load_tracks(ep_id: str) -> List[Dict[str, Any]]:\n+    return _read_json_lines(_tracks_path(ep_id))\n+\n+\n+def write_tracks(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    return _write_json_lines(_tracks_path(ep_id), rows)\n+\n+\n+def load_identities(ep_id: str) -> Dict[str, Any]:\n+    path = _identities_path(ep_id)\n+    if not path.exists():\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+    try:\n+        return json.loads(path.read_text(encoding=\"utf-8\"))\n+    except json.JSONDecodeError:\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+\n+\n+def write_identities(ep_id: str, payload: Dict[str, Any]) -> Path:\n+    path = _identities_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n+    return path\n+\n+\n+def update_identity_stats(ep_id: str, payload: Dict[str, Any]) -> None:\n+    faces_count = len(load_faces(ep_id))\n+    payload.setdefault(\"stats\", {})\n+    payload[\"stats\"][\"faces\"] = faces_count\n+    payload[\"stats\"][\"clusters\"] = len(payload.get(\"identities\", []))\n+\n+\n+def sync_manifests(ep_id: str, *paths: Path) -> None:\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+    except ValueError:\n+        return\n+    for path in paths:\n+        if path and path.exists():\n+            try:\n+                STORAGE.put_artifact(ctx, \"manifests\", path, path.name)\n+            except Exception:\n+                continue\n+\n+\n+def _identity_rows(payload: Dict[str, Any]) -> List[Dict[str, Any]]:\n+    identities = payload.get(\"identities\")\n+    if isinstance(identities, list):\n+        return identities\n+    if isinstance(payload, list):  # legacy format\n+        return payload  # type: ignore[return-value]\n+    return []\n+\n+\n+def _identity_name_lookup(payload: Dict[str, Any]) -> Dict[str, str]:\n+    mapping: Dict[str, str] = {}\n+    for entry in _identity_rows(payload):\n+        key = entry.get(\"identity_id\") or entry.get(\"id\")\n+        name = entry.get(\"name\")\n+        if not key or not name:\n+            continue\n+        mapping[str(key)] = str(name)\n+    return mapping\n+\n+\n+def _annotate_screentime_csv(ep_id: str, payload: Dict[str, Any]) -> None:\n+    csv_path = _screentime_csv_path(ep_id)\n+    if not csv_path.exists():\n+        return\n+    try:\n+        with csv_path.open(\"r\", encoding=\"utf-8\", newline=\"\") as handle:\n+            reader = csv.DictReader(handle)\n+            rows = list(reader)\n+            fieldnames = reader.fieldnames or []\n+    except (OSError, csv.Error):\n+        return\n+    if not rows or not fieldnames:\n+        return\n+    key_field = \"identity_id\" if \"identity_id\" in fieldnames else None\n+    if key_field is None and \"person_id\" in fieldnames:\n+        key_field = \"person_id\"\n+    if key_field is None:\n+        return\n+    lookup = _identity_name_lookup(payload)\n+    if not lookup:\n+        return\n+    updated = False\n+    for row in rows:\n+        identifier = str(row.get(key_field, \"\") or \"\").strip()\n+        if not identifier:\n+            continue\n+        name = lookup.get(identifier)\n+        if not name or row.get(\"name\") == name:\n+            continue\n+        row[\"name\"] = name\n+        updated = True\n+    if not updated:\n+        return\n+    out_fields = list(fieldnames)\n+    if \"name\" not in out_fields:\n+        out_fields.append(\"name\")\n+    try:\n+        with csv_path.open(\"w\", encoding=\"utf-8\", newline=\"\") as handle:\n+            writer = csv.DictWriter(handle, fieldnames=out_fields)\n+            writer.writeheader()\n+            for row in rows:\n+                if \"name\" not in row:\n+                    row[\"name\"] = \"\"\n+                writer.writerow(row)\n+    except (OSError, csv.Error) as exc:\n+        LOGGER.warning(\"Failed to update screentime.csv for %s: %s\", ep_id, exc)\n+\n+\n+def _next_identity_id(entries: Sequence[Dict[str, Any]]) -> str:\n+    max_idx = 0\n+    for entry in entries:\n+        raw = entry.get(\"identity_id\") or entry.get(\"id\")\n+        if not raw:\n+            continue\n+        digits = \"\".join(ch for ch in str(raw) if ch.isdigit())\n+        try:\n+            idx = int(digits)\n+        except ValueError:\n+            continue\n+        max_idx = max(max_idx, idx)\n+    return f\"id_{max_idx + 1:04d}\"\n+\n+\n+def _track_dir_name(track_id: int) -> str:\n+    return f\"track_{int(track_id):04d}\"\n+\n+\n+def _crop_rel(track_id: int, frame_idx: int) -> str:\n+    return f\"crops/{_track_dir_name(track_id)}/frame_{int(frame_idx):06d}.jpg\"\n+\n+\n+def _thumb_rel(track_id: int, frame_idx: int) -> str:\n+    return f\"{_track_dir_name(track_id)}/thumb_{int(frame_idx):06d}.jpg\"\n+\n+\n+def _faces_by_track(rows: Sequence[Dict[str, Any]]) -> Dict[int, List[Dict[str, Any]]]:\n+    grouped: Dict[int, List[Dict[str, Any]]] = {}\n+    for row in rows:\n+        try:\n+            track_id = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        grouped.setdefault(track_id, []).append(row)\n+    for items in grouped.values():\n+        items.sort(key=lambda r: int(r.get(\"frame_idx\", 0)))\n+    return grouped\n+\n+\n+def _next_track_id(tracks: Sequence[Dict[str, Any]]) -> int:\n+    max_id = 0\n+    for entry in tracks:\n+        try:\n+            max_id = max(max_id, int(entry.get(\"track_id\", 0)))\n+        except (TypeError, ValueError):\n+            continue\n+    return max_id + 1 if max_id >= 0 else 1\n+\n+\n+def _track_lookup(tracks: Sequence[Dict[str, Any]]) -> Dict[int, Dict[str, Any]]:\n+    lookup: Dict[int, Dict[str, Any]] = {}\n+    for entry in tracks:\n+        try:\n+            track_id = int(entry.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        if track_id >= 0:\n+            lookup[track_id] = entry\n+    return lookup\n+\n+\n+def cluster_track_summary(\n+    ep_id: str,\n+    *,\n+    include: Iterable[str] | None = None,\n+    limit_per_cluster: int | None = None,\n+) -> Dict[str, Any]:\n+    include_set = {identity.lower() for identity in include} if include else None\n+    identities_payload = load_identities(ep_id)\n+    tracks = load_tracks(ep_id)\n+    faces = load_faces(ep_id)\n+    track_lookup = _track_lookup(tracks)\n+    face_counts: Dict[int, int] = {}\n+    for row in faces:\n+        try:\n+            tid = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        face_counts[tid] = face_counts.get(tid, 0) + 1\n+\n+    clusters: List[Dict[str, Any]] = []\n+    for identity in identities_payload.get(\"identities\", []):\n+        identity_id = identity.get(\"identity_id\")\n+        if not identity_id:\n+            continue\n+        if include_set and identity_id.lower() not in include_set:\n+            continue\n+        track_ids = identity.get(\"track_ids\", []) or []\n+        tracks_payload: List[Dict[str, Any]] = []\n+        for raw_tid in track_ids:\n+            try:\n+                tid = int(raw_tid)\n+            except (TypeError, ValueError):\n+                continue\n+            track_row = track_lookup.get(tid)\n+            if not track_row:\n+                continue\n+            thumb_url = _thumbnail_url(ep_id, track_row.get(\"thumb_rel_path\"), track_row.get(\"thumb_s3_key\"))\n+            tracks_payload.append(\n+                {\n+                    \"track_id\": tid,\n+                    \"faces\": track_row.get(\"faces_count\") or face_counts.get(tid) or 0,\n+                    \"frames\": track_row.get(\"frame_count\"),\n+                    \"rep_thumb_url\": thumb_url,\n+                }\n+            )\n+        if limit_per_cluster:\n+            limit = max(1, int(limit_per_cluster))\n+            tracks_payload = tracks_payload[:limit]\n+        clusters.append(\n+            {\n+                \"identity_id\": identity_id,\n+                \"name\": identity.get(\"name\"),\n+                \"label\": identity.get(\"label\"),\n+                \"counts\": {\n+                    \"tracks\": len(track_ids),\n+                    \"faces\": identity.get(\"faces\")\n+                    or sum(face_counts.get(int(tid), 0) for tid in track_ids),\n+                },\n+                \"tracks\": tracks_payload,\n+            }\n+        )\n+    return {\"ep_id\": ep_id, \"clusters\": clusters}\n+\n+\n+def _rebuild_track_entry(track_entry: Dict[str, Any], face_rows: Sequence[Dict[str, Any]]) -> Dict[str, Any]:\n+    if not face_rows:\n+        return track_entry\n+    sorted_rows = sorted(face_rows, key=lambda row: int(row.get(\"frame_idx\", 0)))\n+    track_entry[\"frame_count\"] = len(sorted_rows)\n+    track_entry[\"faces_count\"] = len(sorted_rows)\n+    ts_values = [row.get(\"ts\") for row in sorted_rows if isinstance(row.get(\"ts\"), (int, float))]\n+    if ts_values:\n+        track_entry[\"first_ts\"] = float(min(ts_values))\n+        track_entry[\"last_ts\"] = float(max(ts_values))\n+    sample_limit = min(len(sorted_rows), 10)\n+    bboxes = []\n+    for row in sorted_rows[:sample_limit]:\n+        entry = {\n+            \"frame_idx\": row.get(\"frame_idx\"),\n+            \"ts\": row.get(\"ts\"),\n+            \"bbox_xyxy\": row.get(\"bbox_xyxy\"),\n+            \"landmarks\": row.get(\"landmarks\"),\n+        }\n+        bboxes.append(entry)\n+    if bboxes:\n+        track_entry[\"bboxes_sampled\"] = bboxes\n+    thumb_rel = sorted_rows[0].get(\"thumb_rel_path\")\n+    thumb_s3 = sorted_rows[0].get(\"thumb_s3_key\")\n+    if thumb_rel:\n+        track_entry[\"thumb_rel_path\"] = thumb_rel\n+    if thumb_s3:\n+        track_entry[\"thumb_s3_key\"] = thumb_s3\n+    return track_entry\n+\n+\n+def _write_track_index(ep_id: str, track_id: int, face_rows: Sequence[Dict[str, Any]], ctx) -> None:\n+    frames_root = get_path(ep_id, \"frames_root\")\n+    crops_dir = frames_root / \"crops\" / _track_dir_name(track_id)\n+    index_path = crops_dir / \"index.json\"\n+    if not face_rows:\n+        try:\n+            index_path.unlink()\n+        except FileNotFoundError:\n+            pass\n+        return\n+    crops_dir.mkdir(parents=True, exist_ok=True)\n+    entries = [\n+        {\n+            \"key\": f\"{_track_dir_name(track_id)}/frame_{int(row.get('frame_idx', 0)):06d}.jpg\",\n+            \"frame_idx\": int(row.get(\"frame_idx\", 0)),\n+            \"ts\": row.get(\"ts\"),\n+        }\n+        for row in sorted(face_rows, key=lambda r: int(r.get(\"frame_idx\", 0)))\n+    ]\n+    index_path.write_text(json.dumps(entries, indent=2), encoding=\"utf-8\")\n+    try:\n+        STORAGE.put_artifact(ctx, \"crops\", index_path, f\"{_track_dir_name(track_id)}/index.json\")\n+    except Exception:\n+        pass\n+\n+\n+def rename_identity(ep_id: str, identity_id: str, label: str | None) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identity = next((item for item in payload.get(\"identities\", []) if item.get(\"identity_id\") == identity_id), None)\n+    if not identity:\n+        raise ValueError(\"identity_not_found\")\n+    normalized = (label or \"\").strip()\n+    identity[\"label\"] = normalized or None\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return identity\n+\n+\n+def merge_identities(ep_id: str, source_id: str, target_id: str) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    source = next((item for item in identities if item.get(\"identity_id\") == source_id), None)\n+    target = next((item for item in identities if item.get(\"identity_id\") == target_id), None)\n+    if not source or not target:\n+        raise ValueError(\"identity_not_found\")\n+    merged = set(target.get(\"track_ids\", []) or [])\n+    for tid in source.get(\"track_ids\", []) or []:\n+        merged.add(int(tid))\n+    target[\"track_ids\"] = sorted({int(val) for val in merged})\n+    payload[\"identities\"] = [item for item in identities if item.get(\"identity_id\") != source_id]\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return target\n+\n+\n+def move_track(ep_id: str, track_id: int, target_identity_id: str | None) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    source_identity = None\n+    target_identity = None\n+    for identity in identities:\n+        track_ids = identity.get(\"track_ids\", []) or []\n+        if track_id in track_ids:\n+            source_identity = identity\n+        if target_identity_id and identity.get(\"identity_id\") == target_identity_id:\n+            target_identity = identity\n+    if target_identity_id and target_identity is None:\n+        raise ValueError(\"target_not_found\")\n+    if source_identity and track_id in source_identity.get(\"track_ids\", []):\n+        source_identity[\"track_ids\"] = [tid for tid in source_identity[\"track_ids\"] if tid != track_id]\n+    if target_identity is not None:\n+        target_identity.setdefault(\"track_ids\", [])\n+        if track_id not in target_identity[\"track_ids\"]:\n+            target_identity[\"track_ids\"].append(track_id)\n+            target_identity[\"track_ids\"] = sorted(target_identity[\"track_ids\"])\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return {\"identity_id\": target_identity_id, \"track_ids\": target_identity[\"track_ids\"] if target_identity else []}\n+\n+\n+def drop_track(ep_id: str, track_id: int) -> Dict[str, Any]:\n+    tracks = load_tracks(ep_id)\n+    kept_tracks = [row for row in tracks if int(row.get(\"track_id\", -1)) != track_id]\n+    if len(kept_tracks) == len(tracks):\n+        raise ValueError(\"track_not_found\")\n+    tracks_path = write_tracks(ep_id, kept_tracks)\n+    identities = load_identities(ep_id)\n+    for identity in identities.get(\"identities\", []):\n+        identity[\"track_ids\"] = [tid for tid in identity.get(\"track_ids\", []) if tid != track_id]\n+    update_identity_stats(ep_id, identities)\n+    identities_path = write_identities(ep_id, identities)\n+    sync_manifests(ep_id, tracks_path, identities_path)\n+    return {\"track_id\": track_id, \"remaining_tracks\": len(kept_tracks)}\n+\n+\n+def drop_frame(ep_id: str, track_id: int, frame_idx: int, delete_assets: bool = False) -> Dict[str, Any]:\n+    faces = load_faces(ep_id)\n+    removed = [\n+        row\n+        for row in faces\n+        if int(row.get(\"track_id\", -1)) == track_id and int(row.get(\"frame_idx\", -1)) == frame_idx\n+    ]\n+    if not removed:\n+        raise ValueError(\"frame_not_found\")\n+    faces = [row for row in faces if row not in removed]\n+    faces_path = write_faces(ep_id, faces)\n+    thumbs_root = get_path(ep_id, \"frames_root\") / \"thumbs\"\n+    crops_root = get_path(ep_id, \"frames_root\")\n+    if delete_assets:\n+        for row in removed:\n+            thumb_rel = row.get(\"thumb_rel_path\")\n+            if isinstance(thumb_rel, str):\n+                thumb_abs = thumbs_root / thumb_rel\n+                try:\n+                    thumb_abs.unlink()\n+                except FileNotFoundError:\n+                    pass\n+            crop_rel = row.get(\"crop_rel_path\")\n+            if isinstance(crop_rel, str):\n+                crop_abs = crops_root / crop_rel\n+                try:\n+                    crop_abs.unlink()\n+                except FileNotFoundError:\n+                    pass\n+    identities = load_identities(ep_id)\n+    update_identity_stats(ep_id, identities)\n+    identities_path = write_identities(ep_id, identities)\n+    sync_manifests(ep_id, faces_path, identities_path)\n+    return {\"track_id\": track_id, \"frame_idx\": frame_idx, \"removed\": len(removed)}\n+\n+\n+def assign_identity_name(ep_id: str, identity_id: str, name: str, show: str | None = None) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    entries = _identity_rows(payload)\n+    target = next((entry for entry in entries if (entry.get(\"identity_id\") or entry.get(\"id\")) == identity_id), None)\n+    if target is None:\n+        raise ValueError(\"identity_not_found\")\n+    trimmed = (name or \"\").strip()\n+    if not trimmed:\n+        raise ValueError(\"name_required\")\n+    target[\"name\"] = trimmed\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    if use_s3():\n+        try:\n+            s3_write_json(f\"artifacts/manifests/{ep_id}/identities.json\", payload)\n+        except Exception as exc:  # pragma: no cover - best effort sync\n+            LOGGER.warning(\"Failed to mirror identities for %s: %s\", ep_id, exc)\n+    _annotate_screentime_csv(ep_id, payload)\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+        show_slug = show or ctx.show_slug\n+    except ValueError:\n+        show_slug = show or \"\"\n+    if show_slug:\n+        try:\n+            roster_service.add_if_missing(show_slug, trimmed)\n+        except ValueError:\n+            pass\n+    return {\"ep_id\": ep_id, \"identity_id\": identity_id, \"name\": trimmed}\n+\n+\n+def move_frames(\n+    ep_id: str,\n+    from_track_id: int,\n+    face_ids: Sequence[str],\n+    *,\n+    target_identity_id: str | None = None,\n+    new_identity_name: str | None = None,\n+    show_id: str | None = None,\n+) -> Dict[str, Any]:\n+    if not face_ids:\n+        raise ValueError(\"face_ids_required\")\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+    except ValueError as exc:\n+        raise ValueError(\"invalid_ep_id\") from exc\n+\n+    faces = load_faces(ep_id)\n+    face_map = {str(row.get(\"face_id\")): row for row in faces}\n+    selected: List[Dict[str, Any]] = []\n+    for face_id in set(face_ids):\n+        row = face_map.get(face_id)\n+        if not row:\n+            raise ValueError(f\"face_not_found:{face_id}\")\n+        try:\n+            track_id = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            raise ValueError(f\"face_track_invalid:{face_id}\")\n+        if track_id != from_track_id:\n+            raise ValueError(f\"face_not_in_track:{face_id}\")\n+        selected.append(row)\n+\n+    if not selected:\n+        raise ValueError(\"selected_faces_missing\")\n+\n+    identities_payload = load_identities(ep_id)\n+    identities = _identity_rows(identities_payload)\n+    source_identity = next(\n+        (entry for entry in identities if from_track_id in (entry.get(\"track_ids\") or [])),\n+        None,\n+    )\n+\n+    target_identity = None\n+    if target_identity_id:\n+        target_identity = next(\n+            (entry for entry in identities if (entry.get(\"identity_id\") or entry.get(\"id\")) == target_identity_id),\n+            None,\n+        )\n+    trimmed_name = (new_identity_name or \"\").strip()\n+    if target_identity is None and trimmed_name:\n+        target_identity = next(\n+            (\n+                entry\n+                for entry in identities\n+                if isinstance(entry.get(\"name\"), str) and entry[\"name\"].lower() == trimmed_name.lower()\n+            ),\n+            None,\n+        )\n+    if target_identity is None and trimmed_name:\n+        new_identity_id = _next_identity_id(identities)\n+        target_identity = {\"identity_id\": new_identity_id, \"label\": None, \"track_ids\": []}\n+        identities.append(target_identity)\n+    if target_identity is None:\n+        raise ValueError(\"target_identity_not_found\")\n+    target_identity.setdefault(\"track_ids\", [])\n+    if trimmed_name:\n+        target_identity[\"name\"] = trimmed_name\n+        try:\n+            roster_service.add_if_missing(show_id or ctx.show_slug, trimmed_name)\n+        except ValueError:\n+            pass\n+\n+    tracks = load_tracks(ep_id)\n+    source_track = next((row for row in tracks if int(row.get(\"track_id\", -1)) == from_track_id), None)\n+    if source_track is None:\n+        raise ValueError(\"track_not_found\")\n+    new_track_id = _next_track_id(tracks)\n+    prefixes = artifact_prefixes(ctx)\n+    crops_prefix = prefixes.get(\"crops\", \"\")\n+    thumbs_prefix = prefixes.get(\"thumbs_tracks\", \"\")\n+    frames_root = get_path(ep_id, \"frames_root\")\n+    thumbs_root = frames_root / \"thumbs\"\n+\n+    for row in selected:\n+        frame_idx = int(row.get(\"frame_idx\", 0))\n+        old_crop_path = frames_root / (row.get(\"crop_rel_path\") or \"\")\n+        new_crop_rel = _crop_rel(new_track_id, frame_idx)\n+        new_crop_path = frames_root / new_crop_rel\n+        new_crop_path.parent.mkdir(parents=True, exist_ok=True)\n+        if old_crop_path.exists():\n+            try:\n+                old_crop_path.rename(new_crop_path)\n+            except OSError:","path":"apps/api/services/identities.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:16Z","updated_at":"2025-11-11T20:00:23Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565752","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565752"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565752"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565752/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":623,"original_line":623,"side":"RIGHT","author_association":"NONE","original_position":623,"position":623,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565768","pull_request_review_id":3449784939,"id":2515565768,"node_id":"PRRC_kwDOQRcQs86V8HzI","diff_hunk":"@@ -0,0 +1,691 @@\n+from __future__ import annotations\n+\n+import csv\n+import json\n+import logging\n+import os\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, List, Sequence\n+\n+from py_screenalytics.artifacts import get_path\n+\n+from apps.api.services import roster as roster_service\n+from apps.api.services.storage import StorageService, artifact_prefixes, episode_context_from_id\n+from apps.shared.storage import s3_write_json, use_s3\n+\n+STORAGE = StorageService()\n+LOGGER = logging.getLogger(__name__)\n+\n+\n+def _manifests_dir(ep_id: str) -> Path:\n+    return get_path(ep_id, \"detections\").parent\n+\n+\n+def _faces_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"faces.jsonl\"\n+\n+\n+def _identities_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"identities.json\"\n+\n+\n+def _tracks_path(ep_id: str) -> Path:\n+    return get_path(ep_id, \"tracks\")\n+\n+\n+def _thumbs_root(ep_id: str) -> Path:\n+    return get_path(ep_id, \"frames_root\") / \"thumbs\"\n+\n+\n+def _thumbnail_url(ep_id: str, rel_path: str | None, s3_key: str | None) -> str | None:\n+    if s3_key:\n+        url = STORAGE.presign_get(str(s3_key))\n+        if url:\n+            return url\n+    if not rel_path:\n+        return None\n+    local = _thumbs_root(ep_id) / rel_path\n+    if local.exists():\n+        return str(local)\n+    return None\n+\n+\n+def _analytics_root(ep_id: str) -> Path:\n+    data_root = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+    return data_root / \"analytics\" / ep_id\n+\n+\n+def _screentime_csv_path(ep_id: str) -> Path:\n+    return _analytics_root(ep_id) / \"screentime.csv\"\n+\n+\n+def _read_json_lines(path: Path) -> List[Dict[str, Any]]:\n+    if not path.exists():\n+        return []\n+    rows: List[Dict[str, Any]] = []\n+    with path.open(\"r\", encoding=\"utf-8\") as handle:\n+        for line in handle:\n+            line = line.strip()\n+            if not line:\n+                continue\n+            try:\n+                obj = json.loads(line)\n+            except json.JSONDecodeError:\n+                continue\n+            if isinstance(obj, dict):\n+                rows.append(obj)\n+    return rows\n+\n+\n+def _write_json_lines(path: Path, rows: List[Dict[str, Any]]) -> Path:\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    with path.open(\"w\", encoding=\"utf-8\") as handle:\n+        for row in rows:\n+            handle.write(json.dumps(row) + \"\\n\")\n+    return path\n+\n+\n+def load_faces(ep_id: str) -> List[Dict[str, Any]]:\n+    return _read_json_lines(_faces_path(ep_id))\n+\n+\n+def write_faces(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    return _write_json_lines(_faces_path(ep_id), rows)\n+\n+\n+def load_tracks(ep_id: str) -> List[Dict[str, Any]]:\n+    return _read_json_lines(_tracks_path(ep_id))\n+\n+\n+def write_tracks(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    return _write_json_lines(_tracks_path(ep_id), rows)\n+\n+\n+def load_identities(ep_id: str) -> Dict[str, Any]:\n+    path = _identities_path(ep_id)\n+    if not path.exists():\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+    try:\n+        return json.loads(path.read_text(encoding=\"utf-8\"))\n+    except json.JSONDecodeError:\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+\n+\n+def write_identities(ep_id: str, payload: Dict[str, Any]) -> Path:\n+    path = _identities_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n+    return path\n+\n+\n+def update_identity_stats(ep_id: str, payload: Dict[str, Any]) -> None:\n+    faces_count = len(load_faces(ep_id))\n+    payload.setdefault(\"stats\", {})\n+    payload[\"stats\"][\"faces\"] = faces_count\n+    payload[\"stats\"][\"clusters\"] = len(payload.get(\"identities\", []))\n+\n+\n+def sync_manifests(ep_id: str, *paths: Path) -> None:\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+    except ValueError:\n+        return\n+    for path in paths:\n+        if path and path.exists():\n+            try:\n+                STORAGE.put_artifact(ctx, \"manifests\", path, path.name)\n+            except Exception:\n+                continue\n+\n+\n+def _identity_rows(payload: Dict[str, Any]) -> List[Dict[str, Any]]:\n+    identities = payload.get(\"identities\")\n+    if isinstance(identities, list):\n+        return identities\n+    if isinstance(payload, list):  # legacy format\n+        return payload  # type: ignore[return-value]\n+    return []\n+\n+\n+def _identity_name_lookup(payload: Dict[str, Any]) -> Dict[str, str]:\n+    mapping: Dict[str, str] = {}\n+    for entry in _identity_rows(payload):\n+        key = entry.get(\"identity_id\") or entry.get(\"id\")\n+        name = entry.get(\"name\")\n+        if not key or not name:\n+            continue\n+        mapping[str(key)] = str(name)\n+    return mapping\n+\n+\n+def _annotate_screentime_csv(ep_id: str, payload: Dict[str, Any]) -> None:\n+    csv_path = _screentime_csv_path(ep_id)\n+    if not csv_path.exists():\n+        return\n+    try:\n+        with csv_path.open(\"r\", encoding=\"utf-8\", newline=\"\") as handle:\n+            reader = csv.DictReader(handle)\n+            rows = list(reader)\n+            fieldnames = reader.fieldnames or []\n+    except (OSError, csv.Error):\n+        return\n+    if not rows or not fieldnames:\n+        return\n+    key_field = \"identity_id\" if \"identity_id\" in fieldnames else None\n+    if key_field is None and \"person_id\" in fieldnames:\n+        key_field = \"person_id\"\n+    if key_field is None:\n+        return\n+    lookup = _identity_name_lookup(payload)\n+    if not lookup:\n+        return\n+    updated = False\n+    for row in rows:\n+        identifier = str(row.get(key_field, \"\") or \"\").strip()\n+        if not identifier:\n+            continue\n+        name = lookup.get(identifier)\n+        if not name or row.get(\"name\") == name:\n+            continue\n+        row[\"name\"] = name\n+        updated = True\n+    if not updated:\n+        return\n+    out_fields = list(fieldnames)\n+    if \"name\" not in out_fields:\n+        out_fields.append(\"name\")\n+    try:\n+        with csv_path.open(\"w\", encoding=\"utf-8\", newline=\"\") as handle:\n+            writer = csv.DictWriter(handle, fieldnames=out_fields)\n+            writer.writeheader()\n+            for row in rows:\n+                if \"name\" not in row:\n+                    row[\"name\"] = \"\"\n+                writer.writerow(row)\n+    except (OSError, csv.Error) as exc:\n+        LOGGER.warning(\"Failed to update screentime.csv for %s: %s\", ep_id, exc)\n+\n+\n+def _next_identity_id(entries: Sequence[Dict[str, Any]]) -> str:\n+    max_idx = 0\n+    for entry in entries:\n+        raw = entry.get(\"identity_id\") or entry.get(\"id\")\n+        if not raw:\n+            continue\n+        digits = \"\".join(ch for ch in str(raw) if ch.isdigit())\n+        try:\n+            idx = int(digits)\n+        except ValueError:\n+            continue\n+        max_idx = max(max_idx, idx)\n+    return f\"id_{max_idx + 1:04d}\"\n+\n+\n+def _track_dir_name(track_id: int) -> str:\n+    return f\"track_{int(track_id):04d}\"\n+\n+\n+def _crop_rel(track_id: int, frame_idx: int) -> str:\n+    return f\"crops/{_track_dir_name(track_id)}/frame_{int(frame_idx):06d}.jpg\"\n+\n+\n+def _thumb_rel(track_id: int, frame_idx: int) -> str:\n+    return f\"{_track_dir_name(track_id)}/thumb_{int(frame_idx):06d}.jpg\"\n+\n+\n+def _faces_by_track(rows: Sequence[Dict[str, Any]]) -> Dict[int, List[Dict[str, Any]]]:\n+    grouped: Dict[int, List[Dict[str, Any]]] = {}\n+    for row in rows:\n+        try:\n+            track_id = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        grouped.setdefault(track_id, []).append(row)\n+    for items in grouped.values():\n+        items.sort(key=lambda r: int(r.get(\"frame_idx\", 0)))\n+    return grouped\n+\n+\n+def _next_track_id(tracks: Sequence[Dict[str, Any]]) -> int:\n+    max_id = 0\n+    for entry in tracks:\n+        try:\n+            max_id = max(max_id, int(entry.get(\"track_id\", 0)))\n+        except (TypeError, ValueError):\n+            continue\n+    return max_id + 1 if max_id >= 0 else 1\n+\n+\n+def _track_lookup(tracks: Sequence[Dict[str, Any]]) -> Dict[int, Dict[str, Any]]:\n+    lookup: Dict[int, Dict[str, Any]] = {}\n+    for entry in tracks:\n+        try:\n+            track_id = int(entry.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        if track_id >= 0:\n+            lookup[track_id] = entry\n+    return lookup\n+\n+\n+def cluster_track_summary(\n+    ep_id: str,\n+    *,\n+    include: Iterable[str] | None = None,\n+    limit_per_cluster: int | None = None,\n+) -> Dict[str, Any]:\n+    include_set = {identity.lower() for identity in include} if include else None\n+    identities_payload = load_identities(ep_id)\n+    tracks = load_tracks(ep_id)\n+    faces = load_faces(ep_id)\n+    track_lookup = _track_lookup(tracks)\n+    face_counts: Dict[int, int] = {}\n+    for row in faces:\n+        try:\n+            tid = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        face_counts[tid] = face_counts.get(tid, 0) + 1\n+\n+    clusters: List[Dict[str, Any]] = []\n+    for identity in identities_payload.get(\"identities\", []):\n+        identity_id = identity.get(\"identity_id\")\n+        if not identity_id:\n+            continue\n+        if include_set and identity_id.lower() not in include_set:\n+            continue\n+        track_ids = identity.get(\"track_ids\", []) or []\n+        tracks_payload: List[Dict[str, Any]] = []\n+        for raw_tid in track_ids:\n+            try:\n+                tid = int(raw_tid)\n+            except (TypeError, ValueError):\n+                continue\n+            track_row = track_lookup.get(tid)\n+            if not track_row:\n+                continue\n+            thumb_url = _thumbnail_url(ep_id, track_row.get(\"thumb_rel_path\"), track_row.get(\"thumb_s3_key\"))\n+            tracks_payload.append(\n+                {\n+                    \"track_id\": tid,\n+                    \"faces\": track_row.get(\"faces_count\") or face_counts.get(tid) or 0,\n+                    \"frames\": track_row.get(\"frame_count\"),\n+                    \"rep_thumb_url\": thumb_url,\n+                }\n+            )\n+        if limit_per_cluster:\n+            limit = max(1, int(limit_per_cluster))\n+            tracks_payload = tracks_payload[:limit]\n+        clusters.append(\n+            {\n+                \"identity_id\": identity_id,\n+                \"name\": identity.get(\"name\"),\n+                \"label\": identity.get(\"label\"),\n+                \"counts\": {\n+                    \"tracks\": len(track_ids),\n+                    \"faces\": identity.get(\"faces\")\n+                    or sum(face_counts.get(int(tid), 0) for tid in track_ids),\n+                },\n+                \"tracks\": tracks_payload,\n+            }\n+        )\n+    return {\"ep_id\": ep_id, \"clusters\": clusters}\n+\n+\n+def _rebuild_track_entry(track_entry: Dict[str, Any], face_rows: Sequence[Dict[str, Any]]) -> Dict[str, Any]:\n+    if not face_rows:\n+        return track_entry\n+    sorted_rows = sorted(face_rows, key=lambda row: int(row.get(\"frame_idx\", 0)))\n+    track_entry[\"frame_count\"] = len(sorted_rows)\n+    track_entry[\"faces_count\"] = len(sorted_rows)\n+    ts_values = [row.get(\"ts\") for row in sorted_rows if isinstance(row.get(\"ts\"), (int, float))]\n+    if ts_values:\n+        track_entry[\"first_ts\"] = float(min(ts_values))\n+        track_entry[\"last_ts\"] = float(max(ts_values))\n+    sample_limit = min(len(sorted_rows), 10)\n+    bboxes = []\n+    for row in sorted_rows[:sample_limit]:\n+        entry = {\n+            \"frame_idx\": row.get(\"frame_idx\"),\n+            \"ts\": row.get(\"ts\"),\n+            \"bbox_xyxy\": row.get(\"bbox_xyxy\"),\n+            \"landmarks\": row.get(\"landmarks\"),\n+        }\n+        bboxes.append(entry)\n+    if bboxes:\n+        track_entry[\"bboxes_sampled\"] = bboxes\n+    thumb_rel = sorted_rows[0].get(\"thumb_rel_path\")\n+    thumb_s3 = sorted_rows[0].get(\"thumb_s3_key\")\n+    if thumb_rel:\n+        track_entry[\"thumb_rel_path\"] = thumb_rel\n+    if thumb_s3:\n+        track_entry[\"thumb_s3_key\"] = thumb_s3\n+    return track_entry\n+\n+\n+def _write_track_index(ep_id: str, track_id: int, face_rows: Sequence[Dict[str, Any]], ctx) -> None:\n+    frames_root = get_path(ep_id, \"frames_root\")\n+    crops_dir = frames_root / \"crops\" / _track_dir_name(track_id)\n+    index_path = crops_dir / \"index.json\"\n+    if not face_rows:\n+        try:\n+            index_path.unlink()\n+        except FileNotFoundError:\n+            pass\n+        return\n+    crops_dir.mkdir(parents=True, exist_ok=True)\n+    entries = [\n+        {\n+            \"key\": f\"{_track_dir_name(track_id)}/frame_{int(row.get('frame_idx', 0)):06d}.jpg\",\n+            \"frame_idx\": int(row.get(\"frame_idx\", 0)),\n+            \"ts\": row.get(\"ts\"),\n+        }\n+        for row in sorted(face_rows, key=lambda r: int(r.get(\"frame_idx\", 0)))\n+    ]\n+    index_path.write_text(json.dumps(entries, indent=2), encoding=\"utf-8\")\n+    try:\n+        STORAGE.put_artifact(ctx, \"crops\", index_path, f\"{_track_dir_name(track_id)}/index.json\")\n+    except Exception:\n+        pass\n+\n+\n+def rename_identity(ep_id: str, identity_id: str, label: str | None) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identity = next((item for item in payload.get(\"identities\", []) if item.get(\"identity_id\") == identity_id), None)\n+    if not identity:\n+        raise ValueError(\"identity_not_found\")\n+    normalized = (label or \"\").strip()\n+    identity[\"label\"] = normalized or None\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return identity\n+\n+\n+def merge_identities(ep_id: str, source_id: str, target_id: str) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    source = next((item for item in identities if item.get(\"identity_id\") == source_id), None)\n+    target = next((item for item in identities if item.get(\"identity_id\") == target_id), None)\n+    if not source or not target:\n+        raise ValueError(\"identity_not_found\")\n+    merged = set(target.get(\"track_ids\", []) or [])\n+    for tid in source.get(\"track_ids\", []) or []:\n+        merged.add(int(tid))\n+    target[\"track_ids\"] = sorted({int(val) for val in merged})\n+    payload[\"identities\"] = [item for item in identities if item.get(\"identity_id\") != source_id]\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return target\n+\n+\n+def move_track(ep_id: str, track_id: int, target_identity_id: str | None) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    source_identity = None\n+    target_identity = None\n+    for identity in identities:\n+        track_ids = identity.get(\"track_ids\", []) or []\n+        if track_id in track_ids:\n+            source_identity = identity\n+        if target_identity_id and identity.get(\"identity_id\") == target_identity_id:\n+            target_identity = identity\n+    if target_identity_id and target_identity is None:\n+        raise ValueError(\"target_not_found\")\n+    if source_identity and track_id in source_identity.get(\"track_ids\", []):\n+        source_identity[\"track_ids\"] = [tid for tid in source_identity[\"track_ids\"] if tid != track_id]\n+    if target_identity is not None:\n+        target_identity.setdefault(\"track_ids\", [])\n+        if track_id not in target_identity[\"track_ids\"]:\n+            target_identity[\"track_ids\"].append(track_id)\n+            target_identity[\"track_ids\"] = sorted(target_identity[\"track_ids\"])\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return {\"identity_id\": target_identity_id, \"track_ids\": target_identity[\"track_ids\"] if target_identity else []}\n+\n+\n+def drop_track(ep_id: str, track_id: int) -> Dict[str, Any]:\n+    tracks = load_tracks(ep_id)\n+    kept_tracks = [row for row in tracks if int(row.get(\"track_id\", -1)) != track_id]\n+    if len(kept_tracks) == len(tracks):\n+        raise ValueError(\"track_not_found\")\n+    tracks_path = write_tracks(ep_id, kept_tracks)\n+    identities = load_identities(ep_id)\n+    for identity in identities.get(\"identities\", []):\n+        identity[\"track_ids\"] = [tid for tid in identity.get(\"track_ids\", []) if tid != track_id]\n+    update_identity_stats(ep_id, identities)\n+    identities_path = write_identities(ep_id, identities)\n+    sync_manifests(ep_id, tracks_path, identities_path)\n+    return {\"track_id\": track_id, \"remaining_tracks\": len(kept_tracks)}\n+\n+\n+def drop_frame(ep_id: str, track_id: int, frame_idx: int, delete_assets: bool = False) -> Dict[str, Any]:\n+    faces = load_faces(ep_id)\n+    removed = [\n+        row\n+        for row in faces\n+        if int(row.get(\"track_id\", -1)) == track_id and int(row.get(\"frame_idx\", -1)) == frame_idx\n+    ]\n+    if not removed:\n+        raise ValueError(\"frame_not_found\")\n+    faces = [row for row in faces if row not in removed]\n+    faces_path = write_faces(ep_id, faces)\n+    thumbs_root = get_path(ep_id, \"frames_root\") / \"thumbs\"\n+    crops_root = get_path(ep_id, \"frames_root\")\n+    if delete_assets:\n+        for row in removed:\n+            thumb_rel = row.get(\"thumb_rel_path\")\n+            if isinstance(thumb_rel, str):\n+                thumb_abs = thumbs_root / thumb_rel\n+                try:\n+                    thumb_abs.unlink()\n+                except FileNotFoundError:\n+                    pass\n+            crop_rel = row.get(\"crop_rel_path\")\n+            if isinstance(crop_rel, str):\n+                crop_abs = crops_root / crop_rel\n+                try:\n+                    crop_abs.unlink()\n+                except FileNotFoundError:\n+                    pass\n+    identities = load_identities(ep_id)\n+    update_identity_stats(ep_id, identities)\n+    identities_path = write_identities(ep_id, identities)\n+    sync_manifests(ep_id, faces_path, identities_path)\n+    return {\"track_id\": track_id, \"frame_idx\": frame_idx, \"removed\": len(removed)}\n+\n+\n+def assign_identity_name(ep_id: str, identity_id: str, name: str, show: str | None = None) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    entries = _identity_rows(payload)\n+    target = next((entry for entry in entries if (entry.get(\"identity_id\") or entry.get(\"id\")) == identity_id), None)\n+    if target is None:\n+        raise ValueError(\"identity_not_found\")\n+    trimmed = (name or \"\").strip()\n+    if not trimmed:\n+        raise ValueError(\"name_required\")\n+    target[\"name\"] = trimmed\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    if use_s3():\n+        try:\n+            s3_write_json(f\"artifacts/manifests/{ep_id}/identities.json\", payload)\n+        except Exception as exc:  # pragma: no cover - best effort sync\n+            LOGGER.warning(\"Failed to mirror identities for %s: %s\", ep_id, exc)\n+    _annotate_screentime_csv(ep_id, payload)\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+        show_slug = show or ctx.show_slug\n+    except ValueError:\n+        show_slug = show or \"\"\n+    if show_slug:\n+        try:\n+            roster_service.add_if_missing(show_slug, trimmed)\n+        except ValueError:\n+            pass\n+    return {\"ep_id\": ep_id, \"identity_id\": identity_id, \"name\": trimmed}\n+\n+\n+def move_frames(\n+    ep_id: str,\n+    from_track_id: int,\n+    face_ids: Sequence[str],\n+    *,\n+    target_identity_id: str | None = None,\n+    new_identity_name: str | None = None,\n+    show_id: str | None = None,\n+) -> Dict[str, Any]:\n+    if not face_ids:\n+        raise ValueError(\"face_ids_required\")\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+    except ValueError as exc:\n+        raise ValueError(\"invalid_ep_id\") from exc\n+\n+    faces = load_faces(ep_id)\n+    face_map = {str(row.get(\"face_id\")): row for row in faces}\n+    selected: List[Dict[str, Any]] = []\n+    for face_id in set(face_ids):\n+        row = face_map.get(face_id)\n+        if not row:\n+            raise ValueError(f\"face_not_found:{face_id}\")\n+        try:\n+            track_id = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            raise ValueError(f\"face_track_invalid:{face_id}\")\n+        if track_id != from_track_id:\n+            raise ValueError(f\"face_not_in_track:{face_id}\")\n+        selected.append(row)\n+\n+    if not selected:\n+        raise ValueError(\"selected_faces_missing\")\n+\n+    identities_payload = load_identities(ep_id)\n+    identities = _identity_rows(identities_payload)\n+    source_identity = next(\n+        (entry for entry in identities if from_track_id in (entry.get(\"track_ids\") or [])),\n+        None,\n+    )\n+\n+    target_identity = None\n+    if target_identity_id:\n+        target_identity = next(\n+            (entry for entry in identities if (entry.get(\"identity_id\") or entry.get(\"id\")) == target_identity_id),\n+            None,\n+        )\n+    trimmed_name = (new_identity_name or \"\").strip()\n+    if target_identity is None and trimmed_name:\n+        target_identity = next(\n+            (\n+                entry\n+                for entry in identities\n+                if isinstance(entry.get(\"name\"), str) and entry[\"name\"].lower() == trimmed_name.lower()\n+            ),\n+            None,\n+        )\n+    if target_identity is None and trimmed_name:\n+        new_identity_id = _next_identity_id(identities)\n+        target_identity = {\"identity_id\": new_identity_id, \"label\": None, \"track_ids\": []}\n+        identities.append(target_identity)\n+    if target_identity is None:\n+        raise ValueError(\"target_identity_not_found\")\n+    target_identity.setdefault(\"track_ids\", [])\n+    if trimmed_name:\n+        target_identity[\"name\"] = trimmed_name\n+        try:\n+            roster_service.add_if_missing(show_id or ctx.show_slug, trimmed_name)\n+        except ValueError:\n+            pass\n+\n+    tracks = load_tracks(ep_id)\n+    source_track = next((row for row in tracks if int(row.get(\"track_id\", -1)) == from_track_id), None)\n+    if source_track is None:\n+        raise ValueError(\"track_not_found\")\n+    new_track_id = _next_track_id(tracks)\n+    prefixes = artifact_prefixes(ctx)\n+    crops_prefix = prefixes.get(\"crops\", \"\")\n+    thumbs_prefix = prefixes.get(\"thumbs_tracks\", \"\")\n+    frames_root = get_path(ep_id, \"frames_root\")\n+    thumbs_root = frames_root / \"thumbs\"\n+\n+    for row in selected:\n+        frame_idx = int(row.get(\"frame_idx\", 0))\n+        old_crop_path = frames_root / (row.get(\"crop_rel_path\") or \"\")\n+        new_crop_rel = _crop_rel(new_track_id, frame_idx)\n+        new_crop_path = frames_root / new_crop_rel\n+        new_crop_path.parent.mkdir(parents=True, exist_ok=True)\n+        if old_crop_path.exists():\n+            try:\n+                old_crop_path.rename(new_crop_path)\n+            except OSError:\n+                pass\n+        old_thumb_rel = row.get(\"thumb_rel_path\")\n+        old_thumb_path = thumbs_root / old_thumb_rel if old_thumb_rel else None\n+        new_thumb_rel = _thumb_rel(new_track_id, frame_idx)\n+        new_thumb_path = thumbs_root / new_thumb_rel\n+        new_thumb_path.parent.mkdir(parents=True, exist_ok=True)\n+        if old_thumb_path and old_thumb_path.exists():\n+            try:\n+                old_thumb_path.rename(new_thumb_path)\n+            except OSError:","path":"apps/api/services/identities.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:16Z","updated_at":"2025-11-11T20:00:23Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565768","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565768"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565768"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565768/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":633,"original_line":633,"side":"RIGHT","author_association":"NONE","original_position":633,"position":633,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565789","pull_request_review_id":3449784939,"id":2515565789,"node_id":"PRRC_kwDOQRcQs86V8Hzd","diff_hunk":"@@ -0,0 +1,690 @@\n+from __future__ import annotations\n+\n+import asyncio\n+import json\n+import os\n+import subprocess\n+import sys\n+from pathlib import Path\n+from typing import List, Literal\n+\n+from fastapi import APIRouter, HTTPException, Request\n+from fastapi.responses import StreamingResponse\n+from pydantic import BaseModel, Field\n+\n+PROJECT_ROOT = Path(__file__).resolve().parents[3]\n+if str(PROJECT_ROOT) not in sys.path:\n+    sys.path.append(str(PROJECT_ROOT))\n+\n+from py_screenalytics.artifacts import ensure_dirs, get_path\n+\n+from apps.api.services.episodes import EpisodeStore\n+from apps.api.services import jobs as jobs_service\n+from apps.api.services.jobs import JobNotFoundError, JobService\n+from apps.api.services.storage import artifact_prefixes, episode_context_from_id\n+\n+router = APIRouter()\n+JOB_SERVICE = JobService()\n+EPISODE_STORE = EpisodeStore()\n+DETECTOR_CHOICES = {\"retinaface\", \"yolov8face\"}\n+TRACKER_CHOICES = {\"bytetrack\", \"strongsort\"}\n+DEFAULT_DETECTOR_ENV = os.getenv(\"DEFAULT_DETECTOR\", \"retinaface\").lower()\n+DEFAULT_TRACKER_ENV = os.getenv(\"DEFAULT_TRACKER\", \"bytetrack\").lower()\n+SCENE_DETECT_DEFAULT = getattr(jobs_service, \"SCENE_DETECT_DEFAULT\", True)\n+SCENE_THRESHOLD_DEFAULT = getattr(jobs_service, \"SCENE_THRESHOLD_DEFAULT\", 0.30)\n+SCENE_MIN_LEN_DEFAULT = getattr(jobs_service, \"SCENE_MIN_LEN_DEFAULT\", 12)\n+SCENE_WARMUP_DETS_DEFAULT = getattr(jobs_service, \"SCENE_WARMUP_DETS_DEFAULT\", 3)\n+\n+\n+LEGACY_SUFFIX = \"st\" \"ub\"\n+LEGACY_KEYS = (\"use_\" + LEGACY_SUFFIX, LEGACY_SUFFIX)\n+\n+\n+def _has_legacy_marker(payload: dict | None, query_params: dict[str, str]) -> bool:\n+    if any(key in query_params for key in LEGACY_KEYS):\n+        return True\n+    if payload is None:\n+        return False\n+    return any(key in payload for key in LEGACY_KEYS)\n+\n+\n+async def _reject_legacy_payload(request: Request) -> None:\n+    query_markers = dict(request.query_params)\n+    payload_data: dict | None = None\n+    try:\n+        body_bytes = await request.body()\n+    except Exception:\n+        body_bytes = b\"\"\n+    if body_bytes:\n+        try:\n+            parsed = json.loads(body_bytes)\n+        except json.JSONDecodeError:\n+            parsed = None\n+        if isinstance(parsed, dict):\n+            payload_data = parsed\n+    if _has_legacy_marker(payload_data, query_markers):\n+        raise HTTPException(status_code=400, detail=\"Stub mode is not supported.\")\n+\n+\n+class DetectRequest(BaseModel):\n+    ep_id: str = Field(..., description=\"Episode identifier\")\n+    video: str = Field(..., description=\"Source video path or URL\")\n+    stride: int = Field(5, description=\"Frame stride for detection sampling\")\n+    fps: float | None = Field(None, description=\"Optional target FPS for sampling\")\n+    device: Literal[\"auto\", \"cpu\", \"mps\", \"cuda\"] = Field(\"auto\", description=\"Execution device\")\n+\n+\n+class TrackRequest(BaseModel):\n+    ep_id: str = Field(..., description=\"Episode identifier\")\n+\n+\n+class DetectTrackRequest(BaseModel):\n+    ep_id: str = Field(..., description=\"Episode identifier\")\n+    stride: int = Field(5, description=\"Frame stride for detection sampling\")\n+    fps: float | None = Field(None, description=\"Optional target FPS for sampling\")\n+    device: Literal[\"auto\", \"cpu\", \"mps\", \"cuda\"] = Field(\"auto\", description=\"Execution device\")\n+    save_frames: bool = Field(False, description=\"Sample full-frame JPGs to S3/local frames root\")\n+    save_crops: bool = Field(False, description=\"Save per-track crops (requires tracks)\")\n+    jpeg_quality: int = Field(85, ge=1, le=100, description=\"JPEG quality for frame/crop exports\")\n+    detector: str = Field(\n+        DEFAULT_DETECTOR_ENV,\n+        description=\"Face detector backend (retinaface or yolov8face)\",\n+    )\n+    tracker: str = Field(\n+        DEFAULT_TRACKER_ENV,\n+        description=\"Tracker backend (bytetrack or strongsort)\",\n+    )\n+    max_gap: int | None = Field(30, ge=1, description=\"Frame gap before forcing a new track\")\n+    det_thresh: float | None = Field(\n+        0.5,\n+        ge=0.0,\n+        le=1.0,\n+        description=\"RetinaFace detection threshold (0-1)\",\n+    )\n+    scene_detect: bool = Field(\n+        SCENE_DETECT_DEFAULT,\n+        description=\"Enable scene-cut histogram detection prepass\",\n+    )\n+    scene_threshold: float = Field(\n+        SCENE_THRESHOLD_DEFAULT,\n+        ge=0.0,\n+        le=2.0,\n+        description=\"Scene-cut threshold (1 - HSV histogram correlation)\",\n+    )\n+    scene_min_len: int = Field(\n+        SCENE_MIN_LEN_DEFAULT,\n+        ge=1,\n+        description=\"Minimum frames between detected cuts\",\n+    )\n+    scene_warmup_dets: int = Field(\n+        SCENE_WARMUP_DETS_DEFAULT,\n+        ge=0,\n+        description=\"Frames forced to run detection immediately after a cut\",\n+    )\n+\n+class FacesEmbedRequest(BaseModel):\n+    ep_id: str = Field(..., description=\"Episode identifier\")\n+    device: Literal[\"auto\", \"cpu\", \"mps\", \"cuda\"] | None = Field(\n+        None,\n+        description=\"Execution device (defaults to server auto-detect)\",\n+    )\n+    save_frames: bool = Field(False, description=\"Export sampled frames alongside crops\")\n+    save_crops: bool = Field(False, description=\"Export crops to data/frames + S3\")\n+    jpeg_quality: int = Field(85, ge=1, le=100, description=\"JPEG quality for face crops\")\n+    thumb_size: int = Field(256, ge=64, le=512, description=\"Square thumbnail size\")\n+\n+class ClusterRequest(BaseModel):\n+    ep_id: str = Field(..., description=\"Episode identifier\")\n+    device: Literal[\"auto\", \"cpu\", \"mps\", \"cuda\"] | None = Field(\n+        None,\n+        description=\"Execution device (defaults to server auto-detect)\",\n+    )\n+    cluster_thresh: float = Field(0.6, gt=0.0, le=1.0, description=\"Cosine distance threshold for clustering\")\n+    min_cluster_size: int = Field(2, ge=1, description=\"Minimum tracks per identity\")\n+\n+def _artifact_summary(ep_id: str) -> dict:\n+    ensure_dirs(ep_id)\n+    return {\n+        \"video\": str(get_path(ep_id, \"video\")),\n+        \"detections\": str(get_path(ep_id, \"detections\")),\n+        \"tracks\": str(get_path(ep_id, \"tracks\")),\n+        \"frames_root\": str(get_path(ep_id, \"frames_root\")),\n+    }\n+\n+\n+def _validate_episode_ready(ep_id: str) -> Path:\n+    record = EPISODE_STORE.get(ep_id)\n+    if not record:\n+        raise HTTPException(status_code=400, detail=\"Episode not tracked yet; create it via /episodes/upsert_by_id.\")\n+    video_path = get_path(ep_id, \"video\")\n+    if not video_path.exists():\n+        raise HTTPException(status_code=400, detail=\"Episode video not mirrored locally; run Mirror from S3.\")\n+    if not video_path.is_file():\n+        raise HTTPException(status_code=400, detail=\"Episode video path is invalid.\")\n+    try:\n+        with video_path.open(\"rb\"):\n+            pass\n+    except OSError as exc:\n+        raise HTTPException(status_code=400, detail=f\"Episode video unreadable: {exc}\") from exc\n+    return video_path\n+\n+\n+def _s3_prefixes(ep_id: str) -> dict | None:\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+    except ValueError:\n+        return None\n+    return artifact_prefixes(ctx)\n+\n+\n+def _progress_file_path(ep_id: str) -> Path:\n+    manifests_dir = get_path(ep_id, \"detections\").parent\n+    manifests_dir.mkdir(parents=True, exist_ok=True)\n+    return manifests_dir / \"progress.json\"\n+\n+\n+def _normalize_detector(detector: str | None) -> str:\n+    fallback = DEFAULT_DETECTOR_ENV or \"retinaface\"\n+    value = (detector or fallback).strip().lower()\n+    if value not in DETECTOR_CHOICES:\n+        raise HTTPException(status_code=400, detail=f\"Unsupported detector '{detector}'\")\n+    return value\n+\n+\n+def _normalize_tracker(tracker: str | None) -> str:\n+    fallback = DEFAULT_TRACKER_ENV or \"bytetrack\"\n+    value = (tracker or fallback).strip().lower()\n+    if value not in TRACKER_CHOICES:\n+        raise HTTPException(status_code=400, detail=f\"Unsupported tracker '{tracker}'\")\n+    return value\n+\n+\n+def _build_detect_track_command(\n+    req: DetectTrackRequest,\n+    video_path: Path,\n+    progress_path: Path,\n+    detector_value: str,\n+    tracker_value: str,\n+    det_thresh: float | None,\n+    scene_detect: bool,\n+    scene_threshold: float,\n+    scene_min_len: int,\n+    scene_warmup_dets: int,\n+) -> List[str]:\n+    command: List[str] = [\n+        sys.executable,\n+        str(PROJECT_ROOT / \"tools\" / \"episode_run.py\"),\n+        \"--ep-id\",\n+        req.ep_id,\n+        \"--video\",\n+        str(video_path),\n+        \"--stride\",\n+        str(req.stride),\n+        \"--device\",\n+        req.device,\n+        \"--progress-file\",\n+        str(progress_path),\n+    ]\n+    if req.fps is not None and req.fps > 0:\n+        command += [\"--fps\", str(req.fps)]\n+    if req.save_frames:\n+        command.append(\"--save-frames\")\n+    if req.save_crops:\n+        command.append(\"--save-crops\")\n+    if req.jpeg_quality and req.jpeg_quality != 85:\n+        command += [\"--jpeg-quality\", str(req.jpeg_quality)]\n+    command += [\"--detector\", detector_value]\n+    command += [\"--tracker\", tracker_value]\n+    if req.max_gap:\n+        command += [\"--max-gap\", str(req.max_gap)]\n+    if det_thresh is not None:\n+        command += [\"--det-thresh\", str(det_thresh)]\n+    command += [\"--scene-detect\", \"on\" if scene_detect else \"off\"]\n+    command += [\"--scene-threshold\", str(scene_threshold)]\n+    command += [\"--scene-min-len\", str(max(scene_min_len, 1))]\n+    command += [\"--scene-warmup-dets\", str(max(scene_warmup_dets, 0))]\n+    return command\n+\n+\n+def _build_faces_command(req: FacesEmbedRequest, progress_path: Path) -> List[str]:\n+    device_value = req.device or \"auto\"\n+    command: List[str] = [\n+        sys.executable,\n+        str(PROJECT_ROOT / \"tools\" / \"episode_run.py\"),\n+        \"--ep-id\",\n+        req.ep_id,\n+        \"--faces-embed\",\n+        \"--device\",\n+        device_value,\n+        \"--progress-file\",\n+        str(progress_path),\n+    ]\n+    if req.save_frames:\n+        command.append(\"--save-frames\")\n+    if req.save_crops:\n+        command.append(\"--save-crops\")\n+    if req.jpeg_quality and req.jpeg_quality != 85:\n+        command += [\"--jpeg-quality\", str(req.jpeg_quality)]\n+    return command\n+\n+\n+def _build_cluster_command(req: ClusterRequest, progress_path: Path) -> List[str]:\n+    device_value = req.device or \"auto\"\n+    command: List[str] = [\n+        sys.executable,\n+        str(PROJECT_ROOT / \"tools\" / \"episode_run.py\"),\n+        \"--ep-id\",\n+        req.ep_id,\n+        \"--cluster\",\n+        \"--device\",\n+        device_value,\n+        \"--progress-file\",\n+        str(progress_path),\n+    ]\n+    command += [\"--cluster-thresh\", str(req.cluster_thresh)]\n+    command += [\"--min-cluster-size\", str(req.min_cluster_size)]\n+    return command\n+\n+\n+def _format_sse(event_name: str, payload: dict) -> str:\n+    return f\"event: {event_name}\\ndata: {json.dumps(payload)}\\n\\n\"\n+\n+\n+def _parse_progress_line(line: str) -> dict | None:\n+    try:\n+        payload = json.loads(line)\n+    except json.JSONDecodeError:\n+        return None\n+    if not isinstance(payload, dict):\n+        return None\n+    if \"phase\" not in payload:\n+        return None\n+    return payload\n+\n+\n+def _count_lines(path: Path) -> int:\n+    if not path.exists():\n+        return 0\n+    with path.open(\"r\", encoding=\"utf-8\") as handle:\n+        return sum(1 for line in handle if line.strip())\n+\n+\n+def _wants_sse(request: Request) -> bool:\n+    accept = (request.headers.get(\"accept\") or \"\").lower()\n+    return \"text/event-stream\" in accept\n+\n+\n+def _run_job_with_optional_sse(command: List[str], request: Request):\n+    env = os.environ.copy()\n+    if _wants_sse(request):\n+        generator = _stream_progress_command(command, env, request)\n+        return StreamingResponse(generator, media_type=\"text/event-stream\", headers={\"Cache-Control\": \"no-cache\"})\n+    completed = subprocess.run(\n+        command,\n+        cwd=str(PROJECT_ROOT),\n+        capture_output=True,\n+        text=True,\n+        check=False,\n+        env=env,\n+    )\n+    if completed.returncode != 0:\n+        raise HTTPException(\n+            status_code=500,\n+            detail={\n+                \"error\": \"episode_run_failed\",\n+                \"stderr\": completed.stderr.strip(),\n+            },\n+        )\n+    return completed\n+\n+\n+async def _stream_progress_command(command: List[str], env: dict, request: Request):\n+    proc = await asyncio.create_subprocess_exec(\n+        *command,\n+        cwd=str(PROJECT_ROOT),\n+        stdout=asyncio.subprocess.PIPE,\n+        stderr=asyncio.subprocess.PIPE,\n+        env=env,\n+    )\n+    saw_terminal = False\n+    try:\n+        assert proc.stdout is not None\n+        async for raw_line in proc.stdout:\n+            line = raw_line.decode().strip()\n+            if not line:\n+                continue\n+            payload = _parse_progress_line(line)\n+            if payload is None:\n+                continue\n+            phase = str(payload.get(\"phase\", \"\")).lower()\n+            if phase == \"done\":\n+                event_name = \"done\"\n+                saw_terminal = True\n+            elif phase == \"error\":\n+                event_name = \"error\"\n+                saw_terminal = True\n+            else:\n+                event_name = \"progress\"\n+            yield _format_sse(event_name, payload)\n+            if await request.is_disconnected():\n+                proc.terminate()\n+                break\n+        await proc.wait()\n+        if proc.returncode not in (0, None) and not saw_terminal:\n+            stderr_text = \"\"\n+            if proc.stderr:\n+                stderr_bytes = await proc.stderr.read()\n+                stderr_text = stderr_bytes.decode().strip()\n+            payload = {\n+                \"phase\": \"error\",\n+                \"error\": stderr_text or f\"episode_run exited with code {proc.returncode}\",\n+                \"return_code\": proc.returncode,\n+            }\n+            yield _format_sse(\"error\", payload)\n+    finally:\n+        if proc.returncode is None:\n+            proc.kill()\n+\n+\n+@router.post(\"/detect\")\n+async def enqueue_detect(req: DetectRequest, request: Request) -> dict:\n+    await _reject_legacy_payload(request)\n+    artifacts = _artifact_summary(req.ep_id)\n+\n+    command: List[str] = [\n+        \"python\",\n+        \"tools/episode_run.py\",\n+        \"--ep-id\",\n+        req.ep_id,\n+        \"--video\",\n+        req.video,\n+        \"--stride\",\n+        str(req.stride),\n+    ]\n+    if req.fps is not None:\n+        command += [\"--fps\", str(req.fps)]\n+    command += [\"--device\", req.device]\n+\n+    return {\n+        \"job\": \"detect\",\n+        \"ep_id\": req.ep_id,\n+        \"queued\": False,\n+        \"command\": command,\n+        \"artifacts\": artifacts,\n+        \"note\": \"Queue integration pending; run command manually for now.\",\n+    }\n+\n+\n+@router.post(\"/track\")\n+def enqueue_track(req: TrackRequest) -> dict:\n+    artifacts = _artifact_summary(req.ep_id)\n+    command = [\n+        \"python\",\n+        \"FEATURES/tracking/src/bytetrack_runner.py\",\n+        \"--ep-id\",\n+        req.ep_id,\n+        \"--detections\",\n+        artifacts[\"detections\"],\n+        \"--output\",\n+        artifacts[\"tracks\"],\n+    ]\n+    return {\n+        \"job\": \"track\",\n+        \"ep_id\": req.ep_id,\n+        \"queued\": False,\n+        \"command\": command,\n+        \"artifacts\": artifacts,\n+        \"note\": \"Tracking runs locally; detection must be prepared first.\",\n+    }\n+\n+\n+@router.post(\"/detect_track\")\n+async def run_detect_track(req: DetectTrackRequest, request: Request):\n+    await _reject_legacy_payload(request)\n+    artifacts = _artifact_summary(req.ep_id)\n+    video_path = _validate_episode_ready(req.ep_id)\n+    detector_value = _normalize_detector(req.detector)\n+    tracker_value = _normalize_tracker(req.tracker)\n+    try:\n+        JOB_SERVICE.ensure_retinaface_ready(detector_value, req.device, req.det_thresh)\n+    except ValueError as exc:\n+        raise HTTPException(status_code=400, detail=str(exc)) from exc\n+    progress_path = _progress_file_path(req.ep_id)\n+    try:\n+        progress_path.unlink()\n+    except FileNotFoundError:\n+        pass\n+    command = _build_detect_track_command(\n+        req,\n+        video_path,\n+        progress_path,\n+        detector_value,\n+        tracker_value,\n+        req.det_thresh,\n+        req.scene_detect,\n+        req.scene_threshold,\n+        req.scene_min_len,\n+        req.scene_warmup_dets,\n+    )\n+    result = _run_job_with_optional_sse(command, request)\n+    if isinstance(result, StreamingResponse):\n+        return result\n+\n+    detections_count = _count_lines(Path(artifacts[\"detections\"]))\n+    tracks_count = _count_lines(Path(artifacts[\"tracks\"]))\n+\n+    return {\n+        \"job\": \"detect_track\",\n+        \"ep_id\": req.ep_id,\n+        \"command\": command,\n+        \"device\": req.device,\n+        \"detector\": detector_value,\n+        \"tracker\": tracker_value,\n+        \"detections_count\": detections_count,\n+        \"tracks_count\": tracks_count,\n+        \"artifacts\": artifacts,\n+        \"progress_file\": str(progress_path),\n+    }\n+\n+\n+@router.post(\"/faces_embed\")\n+async def run_faces_embed(req: FacesEmbedRequest, request: Request):\n+    await _reject_legacy_payload(request)\n+    track_path = get_path(req.ep_id, \"tracks\")\n+    if not track_path.exists():\n+        raise HTTPException(status_code=400, detail=\"tracks.jsonl not found; run detect/track first\")\n+    progress_path = _progress_file_path(req.ep_id)\n+    command = _build_faces_command(req, progress_path)\n+    result = _run_job_with_optional_sse(command, request)\n+    if isinstance(result, StreamingResponse):\n+        return result\n+\n+    manifests_dir = get_path(req.ep_id, \"detections\").parent\n+    faces_path = manifests_dir / \"faces.jsonl\"\n+    faces_count = _count_lines(faces_path)\n+    frames_dir = get_path(req.ep_id, \"frames_root\") / \"frames\"\n+    return {\n+        \"job\": \"faces_embed\",\n+        \"ep_id\": req.ep_id,\n+        \"faces_count\": faces_count,\n+        \"artifacts\": {\n+            \"faces\": str(faces_path),\n+            \"tracks\": str(track_path),\n+            \"frames\": str(frames_dir) if req.save_frames else None,\n+            \"s3_prefixes\": _s3_prefixes(req.ep_id),\n+        },\n+        \"progress_file\": str(progress_path),\n+    }\n+\n+\n+@router.post(\"/cluster\")\n+async def run_cluster(req: ClusterRequest, request: Request):\n+    await _reject_legacy_payload(request)\n+    manifests_dir = get_path(req.ep_id, \"detections\").parent\n+    faces_path = manifests_dir / \"faces.jsonl\"\n+    if not faces_path.exists():\n+        raise HTTPException(status_code=400, detail=\"faces.jsonl not found; run faces_embed first\")\n+    progress_path = _progress_file_path(req.ep_id)\n+    command = _build_cluster_command(req, progress_path)\n+    result = _run_job_with_optional_sse(command, request)\n+    if isinstance(result, StreamingResponse):\n+        return result\n+\n+    identities_path = manifests_dir / \"identities.json\"\n+    identities_count = 0\n+    faces_count = _count_lines(faces_path)\n+    if identities_path.exists():\n+        try:\n+            identities_doc = json.loads(identities_path.read_text(encoding=\"utf-8\"))\n+            identities_count = len(identities_doc.get(\"identities\", []))\n+            faces_count = int(identities_doc.get(\"stats\", {}).get(\"faces\", faces_count))\n+        except json.JSONDecodeError:","path":"apps/api/routers/jobs.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:16Z","updated_at":"2025-11-11T20:00:23Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565789","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565789"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565789"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565789/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":541,"original_line":541,"side":"RIGHT","author_association":"NONE","original_position":541,"position":541,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565811","pull_request_review_id":3449784939,"id":2515565811,"node_id":"PRRC_kwDOQRcQs86V8Hzz","diff_hunk":"@@ -0,0 +1,459 @@\n+\"\"\"Async job orchestration helpers for detect/track runs.\"\"\"\n+\n+from __future__ import annotations\n+\n+import json\n+import os\n+import signal\n+import subprocess\n+import sys\n+import threading\n+import uuid\n+from datetime import datetime\n+from pathlib import Path\n+from typing import Any, Callable, Dict, Optional\n+\n+PROJECT_ROOT = Path(__file__).resolve().parents[3]\n+if str(PROJECT_ROOT) not in sys.path:\n+    sys.path.append(str(PROJECT_ROOT))\n+\n+from py_screenalytics.artifacts import ensure_dirs, get_path\n+\n+try:  # pragma: no cover - optional ML stack\n+    from tools import episode_run  # type: ignore\n+except ModuleNotFoundError:\n+    episode_run = None  # type: ignore[assignment]\n+DEFAULT_DATA_ROOT = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+DETECTOR_CHOICES = {\"retinaface\", \"yolov8face\"}\n+TRACKER_CHOICES = {\"bytetrack\", \"strongsort\"}\n+DEFAULT_DETECTOR_ENV = os.getenv(\"DEFAULT_DETECTOR\", \"retinaface\").lower()\n+DEFAULT_TRACKER_ENV = os.getenv(\"DEFAULT_TRACKER\", \"bytetrack\").lower()\n+\n+\n+def _env_flag(name: str, default: bool) -> bool:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    value = raw.strip().lower()\n+    if value in {\"1\", \"true\", \"yes\", \"on\"}:\n+        return True\n+    if value in {\"0\", \"false\", \"off\", \"no\"}:\n+        return False\n+    return default\n+\n+\n+def _env_float(name: str, default: float) -> float:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    try:\n+        return float(raw)\n+    except (TypeError, ValueError):\n+        return default\n+\n+\n+def _env_int(name: str, default: int) -> int:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    try:\n+        return int(raw)\n+    except (TypeError, ValueError):\n+        return default\n+\n+\n+SCENE_DETECT_DEFAULT = getattr(episode_run, \"SCENE_DETECT_DEFAULT\", _env_flag(\"SCENE_DETECT\", True))\n+SCENE_THRESHOLD_DEFAULT = getattr(episode_run, \"SCENE_THRESHOLD_DEFAULT\", _env_float(\"SCENE_THRESHOLD\", 0.30))\n+SCENE_MIN_LEN_DEFAULT = getattr(episode_run, \"SCENE_MIN_LEN_DEFAULT\", max(_env_int(\"SCENE_MIN_LEN\", 12), 1))\n+SCENE_WARMUP_DETS_DEFAULT = getattr(\n+    episode_run,\n+    \"SCENE_WARMUP_DETS_DEFAULT\",\n+    max(_env_int(\"SCENE_WARMUP_DETS\", 3), 0),\n+)\n+\n+JobRecord = Dict[str, Any]\n+\n+\n+class JobNotFoundError(FileNotFoundError):\n+    \"\"\"Raised when attempting to operate on a job that is unknown.\"\"\"\n+\n+\n+class JobService:\n+    \"\"\"Minimal filesystem-backed job tracker.\"\"\"\n+\n+    def __init__(self, data_root: Path | str | None = None) -> None:\n+        self.data_root = Path(data_root).expanduser() if data_root else DEFAULT_DATA_ROOT\n+        self.jobs_dir = self.data_root / \"jobs\"\n+        self.jobs_dir.mkdir(parents=True, exist_ok=True)\n+        self._lock = threading.Lock()\n+        self._monitors: Dict[str, threading.Thread] = {}\n+\n+    # ------------------------------------------------------------------\n+    def _now(self) -> str:\n+        return datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n+\n+    def _job_path(self, job_id: str) -> Path:\n+        return self.jobs_dir / f\"{job_id}.json\"\n+\n+    def _read_job(self, job_id: str) -> JobRecord:\n+        path = self._job_path(job_id)\n+        if not path.exists():\n+            raise JobNotFoundError(job_id)\n+        try:\n+            return json.loads(path.read_text(encoding=\"utf-8\"))\n+        except json.JSONDecodeError as exc:  # pragma: no cover - corrupt file\n+            raise JobNotFoundError(job_id) from exc\n+\n+    def _write_job(self, record: JobRecord) -> None:\n+        path = self._job_path(record[\"job_id\"])\n+        tmp_path = path.with_suffix(\".tmp\")\n+        tmp_path.write_text(json.dumps(record, indent=2, sort_keys=True), encoding=\"utf-8\")\n+        tmp_path.replace(path)\n+\n+    def _mutate_job(self, job_id: str, mutator: Callable[[JobRecord], None]) -> JobRecord:\n+        with self._lock:\n+            record = self._read_job(job_id)\n+            mutator(record)\n+            self._write_job(record)\n+            return record\n+\n+    def _progress_path(self, ep_id: str) -> Path:\n+        manifests_dir = get_path(ep_id, \"detections\").parent\n+        return manifests_dir / \"progress.json\"\n+\n+    def _read_progress(self, path: Path) -> Optional[Dict[str, Any]]:\n+        if not path.exists():\n+            return None\n+        try:\n+            return json.loads(path.read_text(encoding=\"utf-8\"))\n+        except json.JSONDecodeError:\n+            return None\n+\n+    def _launch_job(\n+        self,\n+        *,\n+        job_type: str,\n+        ep_id: str,\n+        command: list[str],\n+        progress_path: Path,\n+        requested: Dict[str, Any],\n+    ) -> JobRecord:\n+        ensure_dirs(ep_id)\n+        progress_path.parent.mkdir(parents=True, exist_ok=True)\n+        try:\n+            progress_path.unlink()\n+        except FileNotFoundError:","path":"apps/api/services/jobs.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:17Z","updated_at":"2025-11-11T20:00:23Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565811","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565811"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565811"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565811/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":145,"original_line":145,"side":"RIGHT","author_association":"NONE","original_position":145,"position":145,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565825","pull_request_review_id":3449784939,"id":2515565825,"node_id":"PRRC_kwDOQRcQs86V8H0B","diff_hunk":"@@ -0,0 +1,459 @@\n+\"\"\"Async job orchestration helpers for detect/track runs.\"\"\"\n+\n+from __future__ import annotations\n+\n+import json\n+import os\n+import signal\n+import subprocess\n+import sys\n+import threading\n+import uuid\n+from datetime import datetime\n+from pathlib import Path\n+from typing import Any, Callable, Dict, Optional\n+\n+PROJECT_ROOT = Path(__file__).resolve().parents[3]\n+if str(PROJECT_ROOT) not in sys.path:\n+    sys.path.append(str(PROJECT_ROOT))\n+\n+from py_screenalytics.artifacts import ensure_dirs, get_path\n+\n+try:  # pragma: no cover - optional ML stack\n+    from tools import episode_run  # type: ignore\n+except ModuleNotFoundError:\n+    episode_run = None  # type: ignore[assignment]\n+DEFAULT_DATA_ROOT = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+DETECTOR_CHOICES = {\"retinaface\", \"yolov8face\"}\n+TRACKER_CHOICES = {\"bytetrack\", \"strongsort\"}\n+DEFAULT_DETECTOR_ENV = os.getenv(\"DEFAULT_DETECTOR\", \"retinaface\").lower()\n+DEFAULT_TRACKER_ENV = os.getenv(\"DEFAULT_TRACKER\", \"bytetrack\").lower()\n+\n+\n+def _env_flag(name: str, default: bool) -> bool:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    value = raw.strip().lower()\n+    if value in {\"1\", \"true\", \"yes\", \"on\"}:\n+        return True\n+    if value in {\"0\", \"false\", \"off\", \"no\"}:\n+        return False\n+    return default\n+\n+\n+def _env_float(name: str, default: float) -> float:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    try:\n+        return float(raw)\n+    except (TypeError, ValueError):\n+        return default\n+\n+\n+def _env_int(name: str, default: int) -> int:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    try:\n+        return int(raw)\n+    except (TypeError, ValueError):\n+        return default\n+\n+\n+SCENE_DETECT_DEFAULT = getattr(episode_run, \"SCENE_DETECT_DEFAULT\", _env_flag(\"SCENE_DETECT\", True))\n+SCENE_THRESHOLD_DEFAULT = getattr(episode_run, \"SCENE_THRESHOLD_DEFAULT\", _env_float(\"SCENE_THRESHOLD\", 0.30))\n+SCENE_MIN_LEN_DEFAULT = getattr(episode_run, \"SCENE_MIN_LEN_DEFAULT\", max(_env_int(\"SCENE_MIN_LEN\", 12), 1))\n+SCENE_WARMUP_DETS_DEFAULT = getattr(\n+    episode_run,\n+    \"SCENE_WARMUP_DETS_DEFAULT\",\n+    max(_env_int(\"SCENE_WARMUP_DETS\", 3), 0),\n+)\n+\n+JobRecord = Dict[str, Any]\n+\n+\n+class JobNotFoundError(FileNotFoundError):\n+    \"\"\"Raised when attempting to operate on a job that is unknown.\"\"\"\n+\n+\n+class JobService:\n+    \"\"\"Minimal filesystem-backed job tracker.\"\"\"\n+\n+    def __init__(self, data_root: Path | str | None = None) -> None:\n+        self.data_root = Path(data_root).expanduser() if data_root else DEFAULT_DATA_ROOT\n+        self.jobs_dir = self.data_root / \"jobs\"\n+        self.jobs_dir.mkdir(parents=True, exist_ok=True)\n+        self._lock = threading.Lock()\n+        self._monitors: Dict[str, threading.Thread] = {}\n+\n+    # ------------------------------------------------------------------\n+    def _now(self) -> str:\n+        return datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n+\n+    def _job_path(self, job_id: str) -> Path:\n+        return self.jobs_dir / f\"{job_id}.json\"\n+\n+    def _read_job(self, job_id: str) -> JobRecord:\n+        path = self._job_path(job_id)\n+        if not path.exists():\n+            raise JobNotFoundError(job_id)\n+        try:\n+            return json.loads(path.read_text(encoding=\"utf-8\"))\n+        except json.JSONDecodeError as exc:  # pragma: no cover - corrupt file\n+            raise JobNotFoundError(job_id) from exc\n+\n+    def _write_job(self, record: JobRecord) -> None:\n+        path = self._job_path(record[\"job_id\"])\n+        tmp_path = path.with_suffix(\".tmp\")\n+        tmp_path.write_text(json.dumps(record, indent=2, sort_keys=True), encoding=\"utf-8\")\n+        tmp_path.replace(path)\n+\n+    def _mutate_job(self, job_id: str, mutator: Callable[[JobRecord], None]) -> JobRecord:\n+        with self._lock:\n+            record = self._read_job(job_id)\n+            mutator(record)\n+            self._write_job(record)\n+            return record\n+\n+    def _progress_path(self, ep_id: str) -> Path:\n+        manifests_dir = get_path(ep_id, \"detections\").parent\n+        return manifests_dir / \"progress.json\"\n+\n+    def _read_progress(self, path: Path) -> Optional[Dict[str, Any]]:\n+        if not path.exists():\n+            return None\n+        try:\n+            return json.loads(path.read_text(encoding=\"utf-8\"))\n+        except json.JSONDecodeError:\n+            return None\n+\n+    def _launch_job(\n+        self,\n+        *,\n+        job_type: str,\n+        ep_id: str,\n+        command: list[str],\n+        progress_path: Path,\n+        requested: Dict[str, Any],\n+    ) -> JobRecord:\n+        ensure_dirs(ep_id)\n+        progress_path.parent.mkdir(parents=True, exist_ok=True)\n+        try:\n+            progress_path.unlink()\n+        except FileNotFoundError:\n+            pass\n+\n+        job_id = uuid.uuid4().hex\n+        env = os.environ.copy()\n+        proc = subprocess.Popen(command, cwd=str(PROJECT_ROOT), env=env)  # noqa: S603\n+\n+        record: JobRecord = {\n+            \"job_id\": job_id,\n+            \"job_type\": job_type,\n+            \"ep_id\": ep_id,\n+            \"pid\": proc.pid,\n+            \"state\": \"running\",\n+            \"started_at\": self._now(),\n+            \"ended_at\": None,\n+            \"progress_file\": str(progress_path),\n+            \"command\": command,\n+            \"requested\": requested,\n+            \"summary\": None,\n+            \"error\": None,\n+            \"return_code\": None,\n+            \"data_root\": str(self.data_root),\n+        }\n+        with self._lock:\n+            self._write_job(record)\n+\n+        monitor = threading.Thread(\n+            target=self._monitor_process,\n+            args=(job_id, proc),\n+            name=f\"job-monitor-{job_id}\",\n+            daemon=True,\n+        )\n+        monitor.start()\n+        self._monitors[job_id] = monitor\n+        return record\n+\n+    # ------------------------------------------------------------------\n+    def start_detect_track_job(\n+        self,\n+        *,\n+        ep_id: str,\n+        stride: int,\n+        fps: float | None,\n+        device: str,\n+        video_path: Path,\n+        save_frames: bool,\n+        save_crops: bool,\n+        jpeg_quality: int,\n+        detector: str,\n+        tracker: str,\n+        max_gap: int | None,\n+        det_thresh: float | None,\n+        scene_detect: bool,\n+        scene_threshold: float,\n+        scene_min_len: int,\n+        scene_warmup_dets: int,\n+    ) -> JobRecord:\n+        if not video_path.exists():\n+            raise FileNotFoundError(f\"Episode video not found: {video_path}\")\n+        detector_value = self._normalize_detector(detector)\n+        tracker_value = self._normalize_tracker(tracker)\n+        self.ensure_retinaface_ready(detector_value, device, det_thresh)\n+        progress_path = self._progress_path(ep_id)\n+        scene_min_len = max(int(scene_min_len), 1)\n+        scene_warmup_dets = max(int(scene_warmup_dets), 0)\n+        scene_detect_flag = \"on\" if scene_detect else \"off\"\n+        scene_threshold = max(min(float(scene_threshold), 2.0), 0.0)\n+        command = [\n+            sys.executable,\n+            str(PROJECT_ROOT / \"tools\" / \"episode_run.py\"),\n+            \"--ep-id\",\n+            ep_id,\n+            \"--video\",\n+            str(video_path),\n+            \"--stride\",\n+            str(stride),\n+            \"--device\",\n+            device,\n+            \"--progress-file\",\n+            str(progress_path),\n+        ]\n+        if fps and fps > 0:\n+            command += [\"--fps\", str(fps)]\n+        if save_frames:\n+            command.append(\"--save-frames\")\n+        if save_crops:\n+            command.append(\"--save-crops\")\n+        jpeg_quality = max(1, min(int(jpeg_quality), 100))\n+        if jpeg_quality != 85:\n+            command += [\"--jpeg-quality\", str(jpeg_quality)]\n+        command += [\"--detector\", detector_value]\n+        command += [\"--tracker\", tracker_value]\n+        if max_gap is not None:\n+            command += [\"--max-gap\", str(max_gap)]\n+        if det_thresh is not None:\n+            command += [\"--det-thresh\", str(det_thresh)]\n+        command += [\"--scene-detect\", scene_detect_flag]\n+        command += [\"--scene-threshold\", str(scene_threshold)]\n+        command += [\"--scene-min-len\", str(scene_min_len)]\n+        command += [\"--scene-warmup-dets\", str(scene_warmup_dets)]\n+        requested = {\n+            \"stride\": stride,\n+            \"fps\": fps,\n+            \"device\": device,\n+            \"save_frames\": save_frames,\n+            \"save_crops\": save_crops,\n+            \"jpeg_quality\": jpeg_quality,\n+            \"detector\": detector_value,\n+            \"tracker\": tracker_value,\n+            \"max_gap\": max_gap,\n+            \"det_thresh\": det_thresh,\n+            \"scene_detect\": scene_detect,\n+            \"scene_threshold\": scene_threshold,\n+            \"scene_min_len\": scene_min_len,\n+            \"scene_warmup_dets\": scene_warmup_dets,\n+        }\n+        return self._launch_job(\n+            job_type=\"detect_track\",\n+            ep_id=ep_id,\n+            command=command,\n+            progress_path=progress_path,\n+            requested=requested,\n+        )\n+\n+    def start_faces_embed_job(\n+        self,\n+        *,\n+        ep_id: str,\n+        device: str,\n+        save_frames: bool,\n+        save_crops: bool,\n+        jpeg_quality: int,\n+        thumb_size: int,\n+    ) -> JobRecord:\n+        track_path = get_path(ep_id, \"tracks\")\n+        if not track_path.exists():\n+            raise FileNotFoundError(\"tracks.jsonl not found; run detect/track first\")\n+        progress_path = self._progress_path(ep_id)\n+        command = [\n+            sys.executable,\n+            str(PROJECT_ROOT / \"tools\" / \"episode_run.py\"),\n+            \"--ep-id\",\n+            ep_id,\n+            \"--faces-embed\",\n+            \"--device\",\n+            device,\n+            \"--progress-file\",\n+            str(progress_path),\n+        ]\n+        if save_frames:\n+            command.append(\"--save-frames\")\n+        if save_crops:\n+            command.append(\"--save-crops\")\n+        jpeg_quality = max(1, min(int(jpeg_quality), 100))\n+        if jpeg_quality != 85:\n+            command += [\"--jpeg-quality\", str(jpeg_quality)]\n+        command += [\"--thumb-size\", str(thumb_size)]\n+        requested = {\n+            \"device\": device,\n+            \"save_frames\": save_frames,\n+            \"save_crops\": save_crops,\n+            \"jpeg_quality\": jpeg_quality,\n+            \"thumb_size\": thumb_size,\n+        }\n+        return self._launch_job(\n+            job_type=\"faces_embed\",\n+            ep_id=ep_id,\n+            command=command,\n+            progress_path=progress_path,\n+            requested=requested,\n+        )\n+\n+    def start_cluster_job(\n+        self,\n+        *,\n+        ep_id: str,\n+        device: str,\n+        cluster_thresh: float,\n+        min_cluster_size: int,\n+    ) -> JobRecord:\n+        manifests_dir = get_path(ep_id, \"detections\").parent\n+        faces_path = manifests_dir / \"faces.jsonl\"\n+        if not faces_path.exists():\n+            raise FileNotFoundError(\"faces.jsonl not found; run faces_embed first\")\n+        progress_path = self._progress_path(ep_id)\n+        command = [\n+            sys.executable,\n+            str(PROJECT_ROOT / \"tools\" / \"episode_run.py\"),\n+            \"--ep-id\",\n+            ep_id,\n+            \"--cluster\",\n+            \"--device\",\n+            device,\n+            \"--progress-file\",\n+            str(progress_path),\n+        ]\n+        command += [\"--cluster-thresh\", str(cluster_thresh)]\n+        command += [\"--min-cluster-size\", str(min_cluster_size)]\n+        requested = {\n+            \"device\": device,\n+            \"cluster_thresh\": cluster_thresh,\n+            \"min_cluster_size\": min_cluster_size,\n+        }\n+        return self._launch_job(\n+            job_type=\"cluster\",\n+            ep_id=ep_id,\n+            command=command,\n+            progress_path=progress_path,\n+            requested=requested,\n+        )\n+\n+    def _monitor_process(self, job_id: str, proc: subprocess.Popen) -> None:\n+        error_msg: str | None = None\n+        try:\n+            return_code = proc.wait()\n+        except Exception as exc:  # pragma: no cover - rare failure\n+            return_code = -1\n+            error_msg = str(exc)\n+        state = \"succeeded\" if return_code == 0 and error_msg is None else \"failed\"\n+        self._finalize_job(job_id, state, return_code, error_msg)\n+\n+    def _finalize_job(self, job_id: str, state: str, return_code: int, error_msg: str | None) -> None:\n+        progress_data = None\n+\n+        def _apply(record: JobRecord) -> None:\n+            nonlocal progress_data\n+            if record.get(\"state\") == \"canceled\":\n+                return\n+            record[\"state\"] = state\n+            record[\"ended_at\"] = self._now()\n+            record[\"return_code\"] = return_code\n+            if progress_data is None:\n+                progress_path = Path(record[\"progress_file\"])\n+                progress_data = self._read_progress(progress_path)\n+            if progress_data:\n+                record[\"summary\"] = progress_data\n+            if error_msg:\n+                record[\"error\"] = error_msg\n+            elif state == \"failed\" and not record.get(\"error\"):\n+                record[\"error\"] = f\"episode_run exited with code {return_code}\"\n+\n+        try:\n+            self._mutate_job(job_id, _apply)\n+        except JobNotFoundError:\n+            return\n+\n+    # ------------------------------------------------------------------\n+    def get(self, job_id: str) -> JobRecord:\n+        return self._read_job(job_id)\n+\n+    def get_progress(self, job_id: str) -> Optional[Dict[str, Any]]:\n+        record = self._read_job(job_id)\n+        return self._read_progress(Path(record[\"progress_file\"]))\n+\n+    def cancel(self, job_id: str) -> JobRecord:\n+        def _apply(record: JobRecord) -> None:\n+            if record.get(\"state\") != \"running\":\n+                return\n+            pid = record.get(\"pid\")\n+            if isinstance(pid, int) and pid > 0:\n+                try:\n+                    os.kill(pid, signal.SIGTERM)\n+                except ProcessLookupError:","path":"apps/api/services/jobs.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:17Z","updated_at":"2025-11-11T20:00:23Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565825","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565825"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565825"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565825/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":407,"original_line":407,"side":"RIGHT","author_association":"NONE","original_position":407,"position":407,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565849","pull_request_review_id":3449784939,"id":2515565849,"node_id":"PRRC_kwDOQRcQs86V8H0Z","diff_hunk":"@@ -0,0 +1,459 @@\n+\"\"\"Async job orchestration helpers for detect/track runs.\"\"\"\n+\n+from __future__ import annotations\n+\n+import json\n+import os\n+import signal\n+import subprocess\n+import sys\n+import threading\n+import uuid\n+from datetime import datetime\n+from pathlib import Path\n+from typing import Any, Callable, Dict, Optional\n+\n+PROJECT_ROOT = Path(__file__).resolve().parents[3]\n+if str(PROJECT_ROOT) not in sys.path:\n+    sys.path.append(str(PROJECT_ROOT))\n+\n+from py_screenalytics.artifacts import ensure_dirs, get_path\n+\n+try:  # pragma: no cover - optional ML stack\n+    from tools import episode_run  # type: ignore\n+except ModuleNotFoundError:\n+    episode_run = None  # type: ignore[assignment]\n+DEFAULT_DATA_ROOT = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+DETECTOR_CHOICES = {\"retinaface\", \"yolov8face\"}\n+TRACKER_CHOICES = {\"bytetrack\", \"strongsort\"}\n+DEFAULT_DETECTOR_ENV = os.getenv(\"DEFAULT_DETECTOR\", \"retinaface\").lower()\n+DEFAULT_TRACKER_ENV = os.getenv(\"DEFAULT_TRACKER\", \"bytetrack\").lower()\n+\n+\n+def _env_flag(name: str, default: bool) -> bool:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    value = raw.strip().lower()\n+    if value in {\"1\", \"true\", \"yes\", \"on\"}:\n+        return True\n+    if value in {\"0\", \"false\", \"off\", \"no\"}:\n+        return False\n+    return default\n+\n+\n+def _env_float(name: str, default: float) -> float:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    try:\n+        return float(raw)\n+    except (TypeError, ValueError):\n+        return default\n+\n+\n+def _env_int(name: str, default: int) -> int:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return default\n+    try:\n+        return int(raw)\n+    except (TypeError, ValueError):\n+        return default\n+\n+\n+SCENE_DETECT_DEFAULT = getattr(episode_run, \"SCENE_DETECT_DEFAULT\", _env_flag(\"SCENE_DETECT\", True))\n+SCENE_THRESHOLD_DEFAULT = getattr(episode_run, \"SCENE_THRESHOLD_DEFAULT\", _env_float(\"SCENE_THRESHOLD\", 0.30))\n+SCENE_MIN_LEN_DEFAULT = getattr(episode_run, \"SCENE_MIN_LEN_DEFAULT\", max(_env_int(\"SCENE_MIN_LEN\", 12), 1))\n+SCENE_WARMUP_DETS_DEFAULT = getattr(\n+    episode_run,\n+    \"SCENE_WARMUP_DETS_DEFAULT\",\n+    max(_env_int(\"SCENE_WARMUP_DETS\", 3), 0),\n+)\n+\n+JobRecord = Dict[str, Any]\n+\n+\n+class JobNotFoundError(FileNotFoundError):\n+    \"\"\"Raised when attempting to operate on a job that is unknown.\"\"\"\n+\n+\n+class JobService:\n+    \"\"\"Minimal filesystem-backed job tracker.\"\"\"\n+\n+    def __init__(self, data_root: Path | str | None = None) -> None:\n+        self.data_root = Path(data_root).expanduser() if data_root else DEFAULT_DATA_ROOT\n+        self.jobs_dir = self.data_root / \"jobs\"\n+        self.jobs_dir.mkdir(parents=True, exist_ok=True)\n+        self._lock = threading.Lock()\n+        self._monitors: Dict[str, threading.Thread] = {}\n+\n+    # ------------------------------------------------------------------\n+    def _now(self) -> str:\n+        return datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n+\n+    def _job_path(self, job_id: str) -> Path:\n+        return self.jobs_dir / f\"{job_id}.json\"\n+\n+    def _read_job(self, job_id: str) -> JobRecord:\n+        path = self._job_path(job_id)\n+        if not path.exists():\n+            raise JobNotFoundError(job_id)\n+        try:\n+            return json.loads(path.read_text(encoding=\"utf-8\"))\n+        except json.JSONDecodeError as exc:  # pragma: no cover - corrupt file\n+            raise JobNotFoundError(job_id) from exc\n+\n+    def _write_job(self, record: JobRecord) -> None:\n+        path = self._job_path(record[\"job_id\"])\n+        tmp_path = path.with_suffix(\".tmp\")\n+        tmp_path.write_text(json.dumps(record, indent=2, sort_keys=True), encoding=\"utf-8\")\n+        tmp_path.replace(path)\n+\n+    def _mutate_job(self, job_id: str, mutator: Callable[[JobRecord], None]) -> JobRecord:\n+        with self._lock:\n+            record = self._read_job(job_id)\n+            mutator(record)\n+            self._write_job(record)\n+            return record\n+\n+    def _progress_path(self, ep_id: str) -> Path:\n+        manifests_dir = get_path(ep_id, \"detections\").parent\n+        return manifests_dir / \"progress.json\"\n+\n+    def _read_progress(self, path: Path) -> Optional[Dict[str, Any]]:\n+        if not path.exists():\n+            return None\n+        try:\n+            return json.loads(path.read_text(encoding=\"utf-8\"))\n+        except json.JSONDecodeError:\n+            return None\n+\n+    def _launch_job(\n+        self,\n+        *,\n+        job_type: str,\n+        ep_id: str,\n+        command: list[str],\n+        progress_path: Path,\n+        requested: Dict[str, Any],\n+    ) -> JobRecord:\n+        ensure_dirs(ep_id)\n+        progress_path.parent.mkdir(parents=True, exist_ok=True)\n+        try:\n+            progress_path.unlink()\n+        except FileNotFoundError:\n+            pass\n+\n+        job_id = uuid.uuid4().hex\n+        env = os.environ.copy()\n+        proc = subprocess.Popen(command, cwd=str(PROJECT_ROOT), env=env)  # noqa: S603\n+\n+        record: JobRecord = {\n+            \"job_id\": job_id,\n+            \"job_type\": job_type,\n+            \"ep_id\": ep_id,\n+            \"pid\": proc.pid,\n+            \"state\": \"running\",\n+            \"started_at\": self._now(),\n+            \"ended_at\": None,\n+            \"progress_file\": str(progress_path),\n+            \"command\": command,\n+            \"requested\": requested,\n+            \"summary\": None,\n+            \"error\": None,\n+            \"return_code\": None,\n+            \"data_root\": str(self.data_root),\n+        }\n+        with self._lock:\n+            self._write_job(record)\n+\n+        monitor = threading.Thread(\n+            target=self._monitor_process,\n+            args=(job_id, proc),\n+            name=f\"job-monitor-{job_id}\",\n+            daemon=True,\n+        )\n+        monitor.start()\n+        self._monitors[job_id] = monitor\n+        return record\n+\n+    # ------------------------------------------------------------------\n+    def start_detect_track_job(\n+        self,\n+        *,\n+        ep_id: str,\n+        stride: int,\n+        fps: float | None,\n+        device: str,\n+        video_path: Path,\n+        save_frames: bool,\n+        save_crops: bool,\n+        jpeg_quality: int,\n+        detector: str,\n+        tracker: str,\n+        max_gap: int | None,\n+        det_thresh: float | None,\n+        scene_detect: bool,\n+        scene_threshold: float,\n+        scene_min_len: int,\n+        scene_warmup_dets: int,\n+    ) -> JobRecord:\n+        if not video_path.exists():\n+            raise FileNotFoundError(f\"Episode video not found: {video_path}\")\n+        detector_value = self._normalize_detector(detector)\n+        tracker_value = self._normalize_tracker(tracker)\n+        self.ensure_retinaface_ready(detector_value, device, det_thresh)\n+        progress_path = self._progress_path(ep_id)\n+        scene_min_len = max(int(scene_min_len), 1)\n+        scene_warmup_dets = max(int(scene_warmup_dets), 0)\n+        scene_detect_flag = \"on\" if scene_detect else \"off\"\n+        scene_threshold = max(min(float(scene_threshold), 2.0), 0.0)\n+        command = [\n+            sys.executable,\n+            str(PROJECT_ROOT / \"tools\" / \"episode_run.py\"),\n+            \"--ep-id\",\n+            ep_id,\n+            \"--video\",\n+            str(video_path),\n+            \"--stride\",\n+            str(stride),\n+            \"--device\",\n+            device,\n+            \"--progress-file\",\n+            str(progress_path),\n+        ]\n+        if fps and fps > 0:\n+            command += [\"--fps\", str(fps)]\n+        if save_frames:\n+            command.append(\"--save-frames\")\n+        if save_crops:\n+            command.append(\"--save-crops\")\n+        jpeg_quality = max(1, min(int(jpeg_quality), 100))\n+        if jpeg_quality != 85:\n+            command += [\"--jpeg-quality\", str(jpeg_quality)]\n+        command += [\"--detector\", detector_value]\n+        command += [\"--tracker\", tracker_value]\n+        if max_gap is not None:\n+            command += [\"--max-gap\", str(max_gap)]\n+        if det_thresh is not None:\n+            command += [\"--det-thresh\", str(det_thresh)]\n+        command += [\"--scene-detect\", scene_detect_flag]\n+        command += [\"--scene-threshold\", str(scene_threshold)]\n+        command += [\"--scene-min-len\", str(scene_min_len)]\n+        command += [\"--scene-warmup-dets\", str(scene_warmup_dets)]\n+        requested = {\n+            \"stride\": stride,\n+            \"fps\": fps,\n+            \"device\": device,\n+            \"save_frames\": save_frames,\n+            \"save_crops\": save_crops,\n+            \"jpeg_quality\": jpeg_quality,\n+            \"detector\": detector_value,\n+            \"tracker\": tracker_value,\n+            \"max_gap\": max_gap,\n+            \"det_thresh\": det_thresh,\n+            \"scene_detect\": scene_detect,\n+            \"scene_threshold\": scene_threshold,\n+            \"scene_min_len\": scene_min_len,\n+            \"scene_warmup_dets\": scene_warmup_dets,\n+        }\n+        return self._launch_job(\n+            job_type=\"detect_track\",\n+            ep_id=ep_id,\n+            command=command,\n+            progress_path=progress_path,\n+            requested=requested,\n+        )\n+\n+    def start_faces_embed_job(\n+        self,\n+        *,\n+        ep_id: str,\n+        device: str,\n+        save_frames: bool,\n+        save_crops: bool,\n+        jpeg_quality: int,\n+        thumb_size: int,\n+    ) -> JobRecord:\n+        track_path = get_path(ep_id, \"tracks\")\n+        if not track_path.exists():\n+            raise FileNotFoundError(\"tracks.jsonl not found; run detect/track first\")\n+        progress_path = self._progress_path(ep_id)\n+        command = [\n+            sys.executable,\n+            str(PROJECT_ROOT / \"tools\" / \"episode_run.py\"),\n+            \"--ep-id\",\n+            ep_id,\n+            \"--faces-embed\",\n+            \"--device\",\n+            device,\n+            \"--progress-file\",\n+            str(progress_path),\n+        ]\n+        if save_frames:\n+            command.append(\"--save-frames\")\n+        if save_crops:\n+            command.append(\"--save-crops\")\n+        jpeg_quality = max(1, min(int(jpeg_quality), 100))\n+        if jpeg_quality != 85:\n+            command += [\"--jpeg-quality\", str(jpeg_quality)]\n+        command += [\"--thumb-size\", str(thumb_size)]\n+        requested = {\n+            \"device\": device,\n+            \"save_frames\": save_frames,\n+            \"save_crops\": save_crops,\n+            \"jpeg_quality\": jpeg_quality,\n+            \"thumb_size\": thumb_size,\n+        }\n+        return self._launch_job(\n+            job_type=\"faces_embed\",\n+            ep_id=ep_id,\n+            command=command,\n+            progress_path=progress_path,\n+            requested=requested,\n+        )\n+\n+    def start_cluster_job(\n+        self,\n+        *,\n+        ep_id: str,\n+        device: str,\n+        cluster_thresh: float,\n+        min_cluster_size: int,\n+    ) -> JobRecord:\n+        manifests_dir = get_path(ep_id, \"detections\").parent\n+        faces_path = manifests_dir / \"faces.jsonl\"\n+        if not faces_path.exists():\n+            raise FileNotFoundError(\"faces.jsonl not found; run faces_embed first\")\n+        progress_path = self._progress_path(ep_id)\n+        command = [\n+            sys.executable,\n+            str(PROJECT_ROOT / \"tools\" / \"episode_run.py\"),\n+            \"--ep-id\",\n+            ep_id,\n+            \"--cluster\",\n+            \"--device\",\n+            device,\n+            \"--progress-file\",\n+            str(progress_path),\n+        ]\n+        command += [\"--cluster-thresh\", str(cluster_thresh)]\n+        command += [\"--min-cluster-size\", str(min_cluster_size)]\n+        requested = {\n+            \"device\": device,\n+            \"cluster_thresh\": cluster_thresh,\n+            \"min_cluster_size\": min_cluster_size,\n+        }\n+        return self._launch_job(\n+            job_type=\"cluster\",\n+            ep_id=ep_id,\n+            command=command,\n+            progress_path=progress_path,\n+            requested=requested,\n+        )\n+\n+    def _monitor_process(self, job_id: str, proc: subprocess.Popen) -> None:\n+        error_msg: str | None = None\n+        try:\n+            return_code = proc.wait()\n+        except Exception as exc:  # pragma: no cover - rare failure\n+            return_code = -1\n+            error_msg = str(exc)\n+        state = \"succeeded\" if return_code == 0 and error_msg is None else \"failed\"\n+        self._finalize_job(job_id, state, return_code, error_msg)\n+\n+    def _finalize_job(self, job_id: str, state: str, return_code: int, error_msg: str | None) -> None:\n+        progress_data = None\n+\n+        def _apply(record: JobRecord) -> None:\n+            nonlocal progress_data\n+            if record.get(\"state\") == \"canceled\":\n+                return\n+            record[\"state\"] = state\n+            record[\"ended_at\"] = self._now()\n+            record[\"return_code\"] = return_code\n+            if progress_data is None:\n+                progress_path = Path(record[\"progress_file\"])\n+                progress_data = self._read_progress(progress_path)\n+            if progress_data:\n+                record[\"summary\"] = progress_data\n+            if error_msg:\n+                record[\"error\"] = error_msg\n+            elif state == \"failed\" and not record.get(\"error\"):\n+                record[\"error\"] = f\"episode_run exited with code {return_code}\"\n+\n+        try:\n+            self._mutate_job(job_id, _apply)\n+        except JobNotFoundError:\n+            return\n+\n+    # ------------------------------------------------------------------\n+    def get(self, job_id: str) -> JobRecord:\n+        return self._read_job(job_id)\n+\n+    def get_progress(self, job_id: str) -> Optional[Dict[str, Any]]:\n+        record = self._read_job(job_id)\n+        return self._read_progress(Path(record[\"progress_file\"]))\n+\n+    def cancel(self, job_id: str) -> JobRecord:\n+        def _apply(record: JobRecord) -> None:\n+            if record.get(\"state\") != \"running\":\n+                return\n+            pid = record.get(\"pid\")\n+            if isinstance(pid, int) and pid > 0:\n+                try:\n+                    os.kill(pid, signal.SIGTERM)\n+                except ProcessLookupError:\n+                    pass\n+                except PermissionError:","path":"apps/api/services/jobs.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:17Z","updated_at":"2025-11-11T20:00:23Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565849","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565849"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565849"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565849/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":409,"original_line":409,"side":"RIGHT","author_association":"NONE","original_position":409,"position":409,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565883","pull_request_review_id":3449784939,"id":2515565883,"node_id":"PRRC_kwDOQRcQs86V8H07","diff_hunk":"@@ -0,0 +1,152 @@\n+from __future__ import annotations\n+\n+import json\n+import logging\n+import os\n+from datetime import datetime\n+from pathlib import Path\n+from typing import Any, Dict\n+\n+LOGGER = logging.getLogger(__name__)\n+_S3_PREFIXES: tuple[str, ...] = (\"artifacts/\",)\n+_STORAGE = None\n+\n+\n+def project_root() -> Path:\n+    return Path(__file__).resolve().parents[2]\n+\n+\n+def data_root() -> Path:\n+    raw = os.environ.get(\"SCREENALYTICS_DATA_ROOT\")\n+    if raw:\n+        return Path(raw).expanduser()\n+    return project_root() / \"data\"\n+\n+\n+def use_s3() -> bool:\n+    backend = os.environ.get(\"STORAGE_BACKEND\", \"local\").lower()\n+    return backend in {\"s3\", \"minio\"}\n+\n+\n+def now_iso() -> str:\n+    return datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n+\n+\n+def _ensure_storage():\n+    global _STORAGE\n+    if _STORAGE is None:\n+        from apps.api.services.storage import StorageService\n+\n+        _STORAGE = StorageService()\n+    return _STORAGE\n+\n+\n+def _is_s3_key(target: str | Path) -> tuple[bool, str]:\n+    if not use_s3() or isinstance(target, Path):\n+        return False, \"\"\n+    raw = str(target).strip().lstrip(\"/\").replace(\"\\\\\", \"/\")\n+    if any(raw.startswith(prefix) for prefix in _S3_PREFIXES):\n+        return True, raw\n+    return False, \"\"\n+\n+\n+def _resolve_local_path(target: str | Path) -> Path:\n+    if isinstance(target, Path):\n+        return target\n+    raw = str(target).strip()\n+    if not raw:\n+        raise ValueError(\"Path must be non-empty\")\n+    candidate = Path(raw)\n+    if candidate.is_absolute():\n+        return candidate\n+    normalized = raw.replace(\"\\\\\", \"/\")\n+    if normalized.startswith(\"data/\"):\n+        remainder = normalized.split(\"/\", 1)[1] if \"/\" in normalized else \"\"\n+        return data_root() / remainder\n+    return project_root() / normalized\n+\n+\n+def exists(target: str | Path) -> bool:\n+    is_s3, key = _is_s3_key(target)\n+    if is_s3:\n+        storage = _ensure_storage()\n+        try:\n+            return storage.object_exists(key)\n+        except Exception as exc:  # pragma: no cover - best-effort existence check\n+            LOGGER.debug(\"Failed to check S3 object %s: %s\", key, exc)\n+            return False\n+    path = _resolve_local_path(target)\n+    return path.exists()\n+\n+\n+def read_json(target: str | Path) -> Dict[str, Any]:\n+    is_s3, key = _is_s3_key(target)\n+    if is_s3:\n+        return _s3_read_json(key)\n+    path = _resolve_local_path(target)\n+    if not path.exists():\n+        raise FileNotFoundError(path)\n+    try:\n+        return json.loads(path.read_text(encoding=\"utf-8\"))\n+    except json.JSONDecodeError as exc:\n+        raise ValueError(f\"Invalid JSON at {path}\") from exc\n+\n+\n+def write_json(target: str | Path, payload: Dict[str, Any]) -> str | Path:\n+    is_s3, key = _is_s3_key(target)\n+    if is_s3:\n+        s3_write_json(key, payload)\n+        return key\n+    path = _resolve_local_path(target)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    tmp_path = path.with_suffix(\".tmp\")\n+    tmp_path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n+    tmp_path.replace(path)\n+    return path\n+\n+\n+def _s3_read_json(key: str) -> Dict[str, Any]:\n+    storage = _ensure_storage()\n+    client = getattr(storage, \"_client\", None)\n+    if client is None:\n+        raise FileNotFoundError(f\"S3 backend not configured for key {key}\")\n+    try:\n+        response = client.get_object(Bucket=storage.bucket, Key=key)  # type: ignore[assignment]\n+    except Exception as exc:  # pragma: no cover - network errors\n+        raise FileNotFoundError(f\"S3 object {key} not found\") from exc\n+    body = response.get(\"Body\")\n+    if body is None:\n+        raise FileNotFoundError(f\"S3 object {key} missing body\")\n+    try:\n+        data = body.read()\n+    finally:  # pragma: no cover - boto handles closing\n+        try:\n+            body.close()\n+        except Exception:","path":"apps/shared/storage.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-11-11T20:00:17Z","updated_at":"2025-11-11T20:00:23Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565883","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565883"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565883"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565883/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":125,"original_line":125,"side":"RIGHT","author_association":"NONE","original_position":125,"position":125,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565903","pull_request_review_id":3449784939,"id":2515565903,"node_id":"PRRC_kwDOQRcQs86V8H1P","diff_hunk":"@@ -0,0 +1,691 @@\n+from __future__ import annotations\n+\n+import csv\n+import json\n+import logging\n+import os\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, List, Sequence\n+\n+from py_screenalytics.artifacts import get_path\n+\n+from apps.api.services import roster as roster_service\n+from apps.api.services.storage import StorageService, artifact_prefixes, episode_context_from_id\n+from apps.shared.storage import s3_write_json, use_s3\n+\n+STORAGE = StorageService()\n+LOGGER = logging.getLogger(__name__)\n+\n+\n+def _manifests_dir(ep_id: str) -> Path:\n+    return get_path(ep_id, \"detections\").parent\n+\n+\n+def _faces_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"faces.jsonl\"\n+\n+\n+def _identities_path(ep_id: str) -> Path:\n+    return _manifests_dir(ep_id) / \"identities.json\"\n+\n+\n+def _tracks_path(ep_id: str) -> Path:\n+    return get_path(ep_id, \"tracks\")\n+\n+\n+def _thumbs_root(ep_id: str) -> Path:\n+    return get_path(ep_id, \"frames_root\") / \"thumbs\"\n+\n+\n+def _thumbnail_url(ep_id: str, rel_path: str | None, s3_key: str | None) -> str | None:\n+    if s3_key:\n+        url = STORAGE.presign_get(str(s3_key))\n+        if url:\n+            return url\n+    if not rel_path:\n+        return None\n+    local = _thumbs_root(ep_id) / rel_path\n+    if local.exists():\n+        return str(local)\n+    return None\n+\n+\n+def _analytics_root(ep_id: str) -> Path:\n+    data_root = Path(os.environ.get(\"SCREENALYTICS_DATA_ROOT\", \"data\")).expanduser()\n+    return data_root / \"analytics\" / ep_id\n+\n+\n+def _screentime_csv_path(ep_id: str) -> Path:\n+    return _analytics_root(ep_id) / \"screentime.csv\"\n+\n+\n+def _read_json_lines(path: Path) -> List[Dict[str, Any]]:\n+    if not path.exists():\n+        return []\n+    rows: List[Dict[str, Any]] = []\n+    with path.open(\"r\", encoding=\"utf-8\") as handle:\n+        for line in handle:\n+            line = line.strip()\n+            if not line:\n+                continue\n+            try:\n+                obj = json.loads(line)\n+            except json.JSONDecodeError:\n+                continue\n+            if isinstance(obj, dict):\n+                rows.append(obj)\n+    return rows\n+\n+\n+def _write_json_lines(path: Path, rows: List[Dict[str, Any]]) -> Path:\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    with path.open(\"w\", encoding=\"utf-8\") as handle:\n+        for row in rows:\n+            handle.write(json.dumps(row) + \"\\n\")\n+    return path\n+\n+\n+def load_faces(ep_id: str) -> List[Dict[str, Any]]:\n+    return _read_json_lines(_faces_path(ep_id))\n+\n+\n+def write_faces(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    return _write_json_lines(_faces_path(ep_id), rows)\n+\n+\n+def load_tracks(ep_id: str) -> List[Dict[str, Any]]:\n+    return _read_json_lines(_tracks_path(ep_id))\n+\n+\n+def write_tracks(ep_id: str, rows: List[Dict[str, Any]]) -> Path:\n+    return _write_json_lines(_tracks_path(ep_id), rows)\n+\n+\n+def load_identities(ep_id: str) -> Dict[str, Any]:\n+    path = _identities_path(ep_id)\n+    if not path.exists():\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+    try:\n+        return json.loads(path.read_text(encoding=\"utf-8\"))\n+    except json.JSONDecodeError:\n+        return {\"ep_id\": ep_id, \"identities\": [], \"stats\": {}}\n+\n+\n+def write_identities(ep_id: str, payload: Dict[str, Any]) -> Path:\n+    path = _identities_path(ep_id)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+    path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n+    return path\n+\n+\n+def update_identity_stats(ep_id: str, payload: Dict[str, Any]) -> None:\n+    faces_count = len(load_faces(ep_id))\n+    payload.setdefault(\"stats\", {})\n+    payload[\"stats\"][\"faces\"] = faces_count\n+    payload[\"stats\"][\"clusters\"] = len(payload.get(\"identities\", []))\n+\n+\n+def sync_manifests(ep_id: str, *paths: Path) -> None:\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+    except ValueError:\n+        return\n+    for path in paths:\n+        if path and path.exists():\n+            try:\n+                STORAGE.put_artifact(ctx, \"manifests\", path, path.name)\n+            except Exception:\n+                continue\n+\n+\n+def _identity_rows(payload: Dict[str, Any]) -> List[Dict[str, Any]]:\n+    identities = payload.get(\"identities\")\n+    if isinstance(identities, list):\n+        return identities\n+    if isinstance(payload, list):  # legacy format\n+        return payload  # type: ignore[return-value]\n+    return []\n+\n+\n+def _identity_name_lookup(payload: Dict[str, Any]) -> Dict[str, str]:\n+    mapping: Dict[str, str] = {}\n+    for entry in _identity_rows(payload):\n+        key = entry.get(\"identity_id\") or entry.get(\"id\")\n+        name = entry.get(\"name\")\n+        if not key or not name:\n+            continue\n+        mapping[str(key)] = str(name)\n+    return mapping\n+\n+\n+def _annotate_screentime_csv(ep_id: str, payload: Dict[str, Any]) -> None:\n+    csv_path = _screentime_csv_path(ep_id)\n+    if not csv_path.exists():\n+        return\n+    try:\n+        with csv_path.open(\"r\", encoding=\"utf-8\", newline=\"\") as handle:\n+            reader = csv.DictReader(handle)\n+            rows = list(reader)\n+            fieldnames = reader.fieldnames or []\n+    except (OSError, csv.Error):\n+        return\n+    if not rows or not fieldnames:\n+        return\n+    key_field = \"identity_id\" if \"identity_id\" in fieldnames else None\n+    if key_field is None and \"person_id\" in fieldnames:\n+        key_field = \"person_id\"\n+    if key_field is None:\n+        return\n+    lookup = _identity_name_lookup(payload)\n+    if not lookup:\n+        return\n+    updated = False\n+    for row in rows:\n+        identifier = str(row.get(key_field, \"\") or \"\").strip()\n+        if not identifier:\n+            continue\n+        name = lookup.get(identifier)\n+        if not name or row.get(\"name\") == name:\n+            continue\n+        row[\"name\"] = name\n+        updated = True\n+    if not updated:\n+        return\n+    out_fields = list(fieldnames)\n+    if \"name\" not in out_fields:\n+        out_fields.append(\"name\")\n+    try:\n+        with csv_path.open(\"w\", encoding=\"utf-8\", newline=\"\") as handle:\n+            writer = csv.DictWriter(handle, fieldnames=out_fields)\n+            writer.writeheader()\n+            for row in rows:\n+                if \"name\" not in row:\n+                    row[\"name\"] = \"\"\n+                writer.writerow(row)\n+    except (OSError, csv.Error) as exc:\n+        LOGGER.warning(\"Failed to update screentime.csv for %s: %s\", ep_id, exc)\n+\n+\n+def _next_identity_id(entries: Sequence[Dict[str, Any]]) -> str:\n+    max_idx = 0\n+    for entry in entries:\n+        raw = entry.get(\"identity_id\") or entry.get(\"id\")\n+        if not raw:\n+            continue\n+        digits = \"\".join(ch for ch in str(raw) if ch.isdigit())\n+        try:\n+            idx = int(digits)\n+        except ValueError:\n+            continue\n+        max_idx = max(max_idx, idx)\n+    return f\"id_{max_idx + 1:04d}\"\n+\n+\n+def _track_dir_name(track_id: int) -> str:\n+    return f\"track_{int(track_id):04d}\"\n+\n+\n+def _crop_rel(track_id: int, frame_idx: int) -> str:\n+    return f\"crops/{_track_dir_name(track_id)}/frame_{int(frame_idx):06d}.jpg\"\n+\n+\n+def _thumb_rel(track_id: int, frame_idx: int) -> str:\n+    return f\"{_track_dir_name(track_id)}/thumb_{int(frame_idx):06d}.jpg\"\n+\n+\n+def _faces_by_track(rows: Sequence[Dict[str, Any]]) -> Dict[int, List[Dict[str, Any]]]:\n+    grouped: Dict[int, List[Dict[str, Any]]] = {}\n+    for row in rows:\n+        try:\n+            track_id = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        grouped.setdefault(track_id, []).append(row)\n+    for items in grouped.values():\n+        items.sort(key=lambda r: int(r.get(\"frame_idx\", 0)))\n+    return grouped\n+\n+\n+def _next_track_id(tracks: Sequence[Dict[str, Any]]) -> int:\n+    max_id = 0\n+    for entry in tracks:\n+        try:\n+            max_id = max(max_id, int(entry.get(\"track_id\", 0)))\n+        except (TypeError, ValueError):\n+            continue\n+    return max_id + 1 if max_id >= 0 else 1\n+\n+\n+def _track_lookup(tracks: Sequence[Dict[str, Any]]) -> Dict[int, Dict[str, Any]]:\n+    lookup: Dict[int, Dict[str, Any]] = {}\n+    for entry in tracks:\n+        try:\n+            track_id = int(entry.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        if track_id >= 0:\n+            lookup[track_id] = entry\n+    return lookup\n+\n+\n+def cluster_track_summary(\n+    ep_id: str,\n+    *,\n+    include: Iterable[str] | None = None,\n+    limit_per_cluster: int | None = None,\n+) -> Dict[str, Any]:\n+    include_set = {identity.lower() for identity in include} if include else None\n+    identities_payload = load_identities(ep_id)\n+    tracks = load_tracks(ep_id)\n+    faces = load_faces(ep_id)\n+    track_lookup = _track_lookup(tracks)\n+    face_counts: Dict[int, int] = {}\n+    for row in faces:\n+        try:\n+            tid = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            continue\n+        face_counts[tid] = face_counts.get(tid, 0) + 1\n+\n+    clusters: List[Dict[str, Any]] = []\n+    for identity in identities_payload.get(\"identities\", []):\n+        identity_id = identity.get(\"identity_id\")\n+        if not identity_id:\n+            continue\n+        if include_set and identity_id.lower() not in include_set:\n+            continue\n+        track_ids = identity.get(\"track_ids\", []) or []\n+        tracks_payload: List[Dict[str, Any]] = []\n+        for raw_tid in track_ids:\n+            try:\n+                tid = int(raw_tid)\n+            except (TypeError, ValueError):\n+                continue\n+            track_row = track_lookup.get(tid)\n+            if not track_row:\n+                continue\n+            thumb_url = _thumbnail_url(ep_id, track_row.get(\"thumb_rel_path\"), track_row.get(\"thumb_s3_key\"))\n+            tracks_payload.append(\n+                {\n+                    \"track_id\": tid,\n+                    \"faces\": track_row.get(\"faces_count\") or face_counts.get(tid) or 0,\n+                    \"frames\": track_row.get(\"frame_count\"),\n+                    \"rep_thumb_url\": thumb_url,\n+                }\n+            )\n+        if limit_per_cluster:\n+            limit = max(1, int(limit_per_cluster))\n+            tracks_payload = tracks_payload[:limit]\n+        clusters.append(\n+            {\n+                \"identity_id\": identity_id,\n+                \"name\": identity.get(\"name\"),\n+                \"label\": identity.get(\"label\"),\n+                \"counts\": {\n+                    \"tracks\": len(track_ids),\n+                    \"faces\": identity.get(\"faces\")\n+                    or sum(face_counts.get(int(tid), 0) for tid in track_ids),\n+                },\n+                \"tracks\": tracks_payload,\n+            }\n+        )\n+    return {\"ep_id\": ep_id, \"clusters\": clusters}\n+\n+\n+def _rebuild_track_entry(track_entry: Dict[str, Any], face_rows: Sequence[Dict[str, Any]]) -> Dict[str, Any]:\n+    if not face_rows:\n+        return track_entry\n+    sorted_rows = sorted(face_rows, key=lambda row: int(row.get(\"frame_idx\", 0)))\n+    track_entry[\"frame_count\"] = len(sorted_rows)\n+    track_entry[\"faces_count\"] = len(sorted_rows)\n+    ts_values = [row.get(\"ts\") for row in sorted_rows if isinstance(row.get(\"ts\"), (int, float))]\n+    if ts_values:\n+        track_entry[\"first_ts\"] = float(min(ts_values))\n+        track_entry[\"last_ts\"] = float(max(ts_values))\n+    sample_limit = min(len(sorted_rows), 10)\n+    bboxes = []\n+    for row in sorted_rows[:sample_limit]:\n+        entry = {\n+            \"frame_idx\": row.get(\"frame_idx\"),\n+            \"ts\": row.get(\"ts\"),\n+            \"bbox_xyxy\": row.get(\"bbox_xyxy\"),\n+            \"landmarks\": row.get(\"landmarks\"),\n+        }\n+        bboxes.append(entry)\n+    if bboxes:\n+        track_entry[\"bboxes_sampled\"] = bboxes\n+    thumb_rel = sorted_rows[0].get(\"thumb_rel_path\")\n+    thumb_s3 = sorted_rows[0].get(\"thumb_s3_key\")\n+    if thumb_rel:\n+        track_entry[\"thumb_rel_path\"] = thumb_rel\n+    if thumb_s3:\n+        track_entry[\"thumb_s3_key\"] = thumb_s3\n+    return track_entry\n+\n+\n+def _write_track_index(ep_id: str, track_id: int, face_rows: Sequence[Dict[str, Any]], ctx) -> None:\n+    frames_root = get_path(ep_id, \"frames_root\")\n+    crops_dir = frames_root / \"crops\" / _track_dir_name(track_id)\n+    index_path = crops_dir / \"index.json\"\n+    if not face_rows:\n+        try:\n+            index_path.unlink()\n+        except FileNotFoundError:\n+            pass\n+        return\n+    crops_dir.mkdir(parents=True, exist_ok=True)\n+    entries = [\n+        {\n+            \"key\": f\"{_track_dir_name(track_id)}/frame_{int(row.get('frame_idx', 0)):06d}.jpg\",\n+            \"frame_idx\": int(row.get(\"frame_idx\", 0)),\n+            \"ts\": row.get(\"ts\"),\n+        }\n+        for row in sorted(face_rows, key=lambda r: int(r.get(\"frame_idx\", 0)))\n+    ]\n+    index_path.write_text(json.dumps(entries, indent=2), encoding=\"utf-8\")\n+    try:\n+        STORAGE.put_artifact(ctx, \"crops\", index_path, f\"{_track_dir_name(track_id)}/index.json\")\n+    except Exception:\n+        pass\n+\n+\n+def rename_identity(ep_id: str, identity_id: str, label: str | None) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identity = next((item for item in payload.get(\"identities\", []) if item.get(\"identity_id\") == identity_id), None)\n+    if not identity:\n+        raise ValueError(\"identity_not_found\")\n+    normalized = (label or \"\").strip()\n+    identity[\"label\"] = normalized or None\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return identity\n+\n+\n+def merge_identities(ep_id: str, source_id: str, target_id: str) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    source = next((item for item in identities if item.get(\"identity_id\") == source_id), None)\n+    target = next((item for item in identities if item.get(\"identity_id\") == target_id), None)\n+    if not source or not target:\n+        raise ValueError(\"identity_not_found\")\n+    merged = set(target.get(\"track_ids\", []) or [])\n+    for tid in source.get(\"track_ids\", []) or []:\n+        merged.add(int(tid))\n+    target[\"track_ids\"] = sorted({int(val) for val in merged})\n+    payload[\"identities\"] = [item for item in identities if item.get(\"identity_id\") != source_id]\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return target\n+\n+\n+def move_track(ep_id: str, track_id: int, target_identity_id: str | None) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    identities = payload.get(\"identities\", [])\n+    source_identity = None\n+    target_identity = None\n+    for identity in identities:\n+        track_ids = identity.get(\"track_ids\", []) or []\n+        if track_id in track_ids:\n+            source_identity = identity\n+        if target_identity_id and identity.get(\"identity_id\") == target_identity_id:\n+            target_identity = identity\n+    if target_identity_id and target_identity is None:\n+        raise ValueError(\"target_not_found\")\n+    if source_identity and track_id in source_identity.get(\"track_ids\", []):\n+        source_identity[\"track_ids\"] = [tid for tid in source_identity[\"track_ids\"] if tid != track_id]\n+    if target_identity is not None:\n+        target_identity.setdefault(\"track_ids\", [])\n+        if track_id not in target_identity[\"track_ids\"]:\n+            target_identity[\"track_ids\"].append(track_id)\n+            target_identity[\"track_ids\"] = sorted(target_identity[\"track_ids\"])\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    return {\"identity_id\": target_identity_id, \"track_ids\": target_identity[\"track_ids\"] if target_identity else []}\n+\n+\n+def drop_track(ep_id: str, track_id: int) -> Dict[str, Any]:\n+    tracks = load_tracks(ep_id)\n+    kept_tracks = [row for row in tracks if int(row.get(\"track_id\", -1)) != track_id]\n+    if len(kept_tracks) == len(tracks):\n+        raise ValueError(\"track_not_found\")\n+    tracks_path = write_tracks(ep_id, kept_tracks)\n+    identities = load_identities(ep_id)\n+    for identity in identities.get(\"identities\", []):\n+        identity[\"track_ids\"] = [tid for tid in identity.get(\"track_ids\", []) if tid != track_id]\n+    update_identity_stats(ep_id, identities)\n+    identities_path = write_identities(ep_id, identities)\n+    sync_manifests(ep_id, tracks_path, identities_path)\n+    return {\"track_id\": track_id, \"remaining_tracks\": len(kept_tracks)}\n+\n+\n+def drop_frame(ep_id: str, track_id: int, frame_idx: int, delete_assets: bool = False) -> Dict[str, Any]:\n+    faces = load_faces(ep_id)\n+    removed = [\n+        row\n+        for row in faces\n+        if int(row.get(\"track_id\", -1)) == track_id and int(row.get(\"frame_idx\", -1)) == frame_idx\n+    ]\n+    if not removed:\n+        raise ValueError(\"frame_not_found\")\n+    faces = [row for row in faces if row not in removed]\n+    faces_path = write_faces(ep_id, faces)\n+    thumbs_root = get_path(ep_id, \"frames_root\") / \"thumbs\"\n+    crops_root = get_path(ep_id, \"frames_root\")\n+    if delete_assets:\n+        for row in removed:\n+            thumb_rel = row.get(\"thumb_rel_path\")\n+            if isinstance(thumb_rel, str):\n+                thumb_abs = thumbs_root / thumb_rel\n+                try:\n+                    thumb_abs.unlink()\n+                except FileNotFoundError:\n+                    pass\n+            crop_rel = row.get(\"crop_rel_path\")\n+            if isinstance(crop_rel, str):\n+                crop_abs = crops_root / crop_rel\n+                try:\n+                    crop_abs.unlink()\n+                except FileNotFoundError:\n+                    pass\n+    identities = load_identities(ep_id)\n+    update_identity_stats(ep_id, identities)\n+    identities_path = write_identities(ep_id, identities)\n+    sync_manifests(ep_id, faces_path, identities_path)\n+    return {\"track_id\": track_id, \"frame_idx\": frame_idx, \"removed\": len(removed)}\n+\n+\n+def assign_identity_name(ep_id: str, identity_id: str, name: str, show: str | None = None) -> Dict[str, Any]:\n+    payload = load_identities(ep_id)\n+    entries = _identity_rows(payload)\n+    target = next((entry for entry in entries if (entry.get(\"identity_id\") or entry.get(\"id\")) == identity_id), None)\n+    if target is None:\n+        raise ValueError(\"identity_not_found\")\n+    trimmed = (name or \"\").strip()\n+    if not trimmed:\n+        raise ValueError(\"name_required\")\n+    target[\"name\"] = trimmed\n+    update_identity_stats(ep_id, payload)\n+    identities_path = write_identities(ep_id, payload)\n+    sync_manifests(ep_id, identities_path)\n+    if use_s3():\n+        try:\n+            s3_write_json(f\"artifacts/manifests/{ep_id}/identities.json\", payload)\n+        except Exception as exc:  # pragma: no cover - best effort sync\n+            LOGGER.warning(\"Failed to mirror identities for %s: %s\", ep_id, exc)\n+    _annotate_screentime_csv(ep_id, payload)\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+        show_slug = show or ctx.show_slug\n+    except ValueError:\n+        show_slug = show or \"\"\n+    if show_slug:\n+        try:\n+            roster_service.add_if_missing(show_slug, trimmed)\n+        except ValueError:\n+            pass\n+    return {\"ep_id\": ep_id, \"identity_id\": identity_id, \"name\": trimmed}\n+\n+\n+def move_frames(\n+    ep_id: str,\n+    from_track_id: int,\n+    face_ids: Sequence[str],\n+    *,\n+    target_identity_id: str | None = None,\n+    new_identity_name: str | None = None,\n+    show_id: str | None = None,\n+) -> Dict[str, Any]:\n+    if not face_ids:\n+        raise ValueError(\"face_ids_required\")\n+    try:\n+        ctx = episode_context_from_id(ep_id)\n+    except ValueError as exc:\n+        raise ValueError(\"invalid_ep_id\") from exc\n+\n+    faces = load_faces(ep_id)\n+    face_map = {str(row.get(\"face_id\")): row for row in faces}\n+    selected: List[Dict[str, Any]] = []\n+    for face_id in set(face_ids):\n+        row = face_map.get(face_id)\n+        if not row:\n+            raise ValueError(f\"face_not_found:{face_id}\")\n+        try:\n+            track_id = int(row.get(\"track_id\", -1))\n+        except (TypeError, ValueError):\n+            raise ValueError(f\"face_track_invalid:{face_id}\")\n+        if track_id != from_track_id:\n+            raise ValueError(f\"face_not_in_track:{face_id}\")\n+        selected.append(row)\n+\n+    if not selected:\n+        raise ValueError(\"selected_faces_missing\")\n+\n+    identities_payload = load_identities(ep_id)\n+    identities = _identity_rows(identities_payload)\n+    source_identity = next(\n+        (entry for entry in identities if from_track_id in (entry.get(\"track_ids\") or [])),\n+        None,\n+    )\n+\n+    target_identity = None\n+    if target_identity_id:\n+        target_identity = next(\n+            (entry for entry in identities if (entry.get(\"identity_id\") or entry.get(\"id\")) == target_identity_id),\n+            None,\n+        )\n+    trimmed_name = (new_identity_name or \"\").strip()\n+    if target_identity is None and trimmed_name:\n+        target_identity = next(\n+            (\n+                entry\n+                for entry in identities\n+                if isinstance(entry.get(\"name\"), str) and entry[\"name\"].lower() == trimmed_name.lower()\n+            ),\n+            None,\n+        )\n+    if target_identity is None and trimmed_name:\n+        new_identity_id = _next_identity_id(identities)\n+        target_identity = {\"identity_id\": new_identity_id, \"label\": None, \"track_ids\": []}\n+        identities.append(target_identity)\n+    if target_identity is None:\n+        raise ValueError(\"target_identity_not_found\")\n+    target_identity.setdefault(\"track_ids\", [])\n+    if trimmed_name:\n+        target_identity[\"name\"] = trimmed_name\n+        try:\n+            roster_service.add_if_missing(show_id or ctx.show_slug, trimmed_name)\n+        except ValueError:\n+            pass\n+\n+    tracks = load_tracks(ep_id)\n+    source_track = next((row for row in tracks if int(row.get(\"track_id\", -1)) == from_track_id), None)\n+    if source_track is None:\n+        raise ValueError(\"track_not_found\")\n+    new_track_id = _next_track_id(tracks)\n+    prefixes = artifact_prefixes(ctx)\n+    crops_prefix = prefixes.get(\"crops\", \"\")\n+    thumbs_prefix = prefixes.get(\"thumbs_tracks\", \"\")\n+    frames_root = get_path(ep_id, \"frames_root\")\n+    thumbs_root = frames_root / \"thumbs\"\n+\n+    for row in selected:\n+        frame_idx = int(row.get(\"frame_idx\", 0))\n+        old_crop_path = frames_root / (row.get(\"crop_rel_path\") or \"\")\n+        new_crop_rel = _crop_rel(new_track_id, frame_idx)\n+        new_crop_path = frames_root / new_crop_rel\n+        new_crop_path.parent.mkdir(parents=True, exist_ok=True)\n+        if old_crop_path.exists():\n+            try:\n+                old_crop_path.rename(new_crop_path)\n+            except OSError:\n+                pass\n+        old_thumb_rel = row.get(\"thumb_rel_path\")\n+        old_thumb_path = thumbs_root / old_thumb_rel if old_thumb_rel else None\n+        new_thumb_rel = _thumb_rel(new_track_id, frame_idx)\n+        new_thumb_path = thumbs_root / new_thumb_rel\n+        new_thumb_path.parent.mkdir(parents=True, exist_ok=True)\n+        if old_thumb_path and old_thumb_path.exists():\n+            try:\n+                old_thumb_path.rename(new_thumb_path)\n+            except OSError:\n+                pass\n+        row[\"track_id\"] = new_track_id\n+        row[\"crop_rel_path\"] = new_crop_rel\n+        if crops_prefix:\n+            row[\"crop_s3_key\"] = f\"{crops_prefix}track_{new_track_id:04d}/frame_{frame_idx:06d}.jpg\"\n+        row[\"thumb_rel_path\"] = new_thumb_rel\n+        if thumbs_prefix:\n+            row[\"thumb_s3_key\"] = f\"{thumbs_prefix}track_{new_track_id:04d}/thumb_{frame_idx:06d}.jpg\"\n+\n+    faces_path = write_faces(ep_id, faces)\n+    faces_by_track = _faces_by_track(faces)\n+\n+    new_track_entry = dict(source_track)\n+    new_track_entry[\"track_id\"] = new_track_id\n+    new_track_entry = _rebuild_track_entry(new_track_entry, faces_by_track.get(new_track_id, []))\n+    tracks.append(new_track_entry)\n+\n+    remaining_faces = faces_by_track.get(from_track_id, [])\n+    if remaining_faces:\n+        _rebuild_track_entry(source_track, remaining_faces)\n+    else:\n+        tracks = [row for row in tracks if int(row.get(\"track_id\", -1)) != from_track_id]\n+        if source_identity:\n+            source_identity[\"track_ids\"] = [tid for tid in source_identity.get(\"track_ids\", []) if tid != from_track_id]\n+\n+    _write_track_index(ep_id, from_track_id, remaining_faces, ctx)\n+    _write_track_index(ep_id, new_track_id, faces_by_track.get(new_track_id, []), ctx)\n+\n+    target_identity[\"track_ids\"] = sorted({int(tid) for tid in target_identity.get(\"track_ids\", [])} | {new_track_id})\n+    identities_payload[\"identities\"] = identities\n+    update_identity_stats(ep_id, identities_payload)\n+\n+    identities_path = write_identities(ep_id, identities_payload)\n+    tracks_path = write_tracks(ep_id, tracks)\n+    _annotate_screentime_csv(ep_id, identities_payload)\n+    sync_manifests(ep_id, faces_path, tracks_path, identities_path)\n+\n+    affected_ids = {target_identity.get(\"identity_id\")}\n+    if source_identity and source_identity.get(\"identity_id\"):\n+        affected_ids.add(source_identity[\"identity_id\"])\n+    summary = cluster_track_summary(ep_id, include=affected_ids)\n+\n+    return {\n+        \"ep_id\": ep_id,\n+        \"moved_faces\": len(selected),\n+        \"new_track_id\": new_track_id,\n+        \"target_identity_id\": target_identity.get(\"identity_id\"),\n+        \"target_name\": target_identity.get(\"name\"),\n+        \"clusters\": summary[\"clusters\"],\n+    }\n+    return {\n+        \"ep_id\": ep_id,\n+        \"source_track_id\": source_track_id,\n+        \"new_track_id\": new_track_id,\n+        \"moved_frames\": len(selected),\n+        \"target_identity_id\": target_identity.get(\"identity_id\"),\n+        \"target_name\": target_identity.get(\"name\"),\n+    }","path":"apps/api/services/identities.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"This statement is unreachable.","created_at":"2025-11-11T20:00:18Z","updated_at":"2025-11-11T20:00:23Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565903","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565903"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565903"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565903/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":684,"original_start_line":684,"start_side":"RIGHT","line":691,"original_line":691,"side":"RIGHT","author_association":"NONE","original_position":691,"position":691,"subject_type":"line"},{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565923","pull_request_review_id":3449784939,"id":2515565923,"node_id":"PRRC_kwDOQRcQs86V8H1j","diff_hunk":"@@ -0,0 +1,899 @@\n+\"\"\"Object storage helpers for presigned uploads and artifact sync.\"\"\"\n+\n+from __future__ import annotations\n+\n+import json\n+import logging\n+import os\n+import re\n+import shutil\n+from dataclasses import dataclass\n+from mimetypes import guess_type\n+from pathlib import Path\n+from typing import Any, Dict, Iterable, List, Optional\n+\n+from py_screenalytics.artifacts import get_path\n+\n+DEFAULT_BUCKET = \"screenalytics\"\n+DEFAULT_REGION = \"us-east-1\"\n+DEFAULT_EXPIRY = 900  # 15 minutes\n+LOCAL_UPLOAD_BASE = \"http://localhost/_local-storage\"\n+ARTIFACT_ROOT = \"artifacts\"\n+_V2_KEY_RE = re.compile(\n+    r\"raw/videos/(?P<show>[^/]+)/s(?P<season>\\d{2})/e(?P<episode>\\d{2})/episode\\.mp4\"\n+)\n+_V1_KEY_RE = re.compile(r\"raw/videos/(?P<ep_id>[^/]+)/episode\\.mp4\")\n+_EP_ID_REGEX = re.compile(r\"^(?P<show>.+)-s(?P<season>\\d{2})e(?P<episode>\\d{2})$\", re.IGNORECASE)\n+_FRAME_NAME_RE = re.compile(r\"frame_(\\d{6})\\.jpg$\", re.IGNORECASE)\n+_CURSOR_SEP = \"|\"\n+LOGGER = logging.getLogger(__name__)\n+\n+\n+def _frame_idx_from_key(key: str) -> int | None:\n+    match = _FRAME_NAME_RE.search(key)\n+    if not match:\n+        return None\n+    try:\n+        return int(match.group(1))\n+    except ValueError:\n+        return None\n+\n+\n+def _split_cursor(raw: str | None) -> tuple[str | None, int]:\n+    if not raw:\n+        return None, 0\n+    value = raw.strip()\n+    if not value:\n+        return None, 0\n+    if _CURSOR_SEP in value:\n+        key, _, remainder = value.partition(_CURSOR_SEP)\n+        try:\n+            cycle = int(remainder)\n+        except ValueError:\n+            cycle = 0\n+        return key or None, max(cycle, 0)\n+    return value, 0\n+\n+\n+def _encode_cursor(key: str, cycle: int) -> str:\n+    cycle = max(int(cycle), 0)\n+    return f\"{key}{_CURSOR_SEP}{cycle}\" if cycle else key\n+\n+\n+def _normalize_cursor_key(track_prefix: str, raw_key: str | None) -> str | None:\n+    if not raw_key:\n+        return None\n+    candidate = raw_key.strip()\n+    if not candidate:\n+        return None\n+    candidate = candidate.replace(\"\\\\\", \"/\")\n+    marker = track_prefix\n+    idx = candidate.find(marker)\n+    if idx >= 0:\n+        candidate = candidate[idx:]\n+    if candidate.startswith(marker):\n+        return candidate\n+    leaf = candidate.split(\"/\")[-1]\n+    if not leaf:\n+        return None\n+    return f\"{marker}{leaf}\"\n+\n+\n+def _boto3():\n+    try:\n+        import boto3  # type: ignore\n+        return boto3\n+    except ImportError as exc:  # pragma: no cover - only triggered in misconfig\n+        raise RuntimeError(\n+            \"boto3 is required when STORAGE_BACKEND is 's3' or 'minio'\"\n+        ) from exc\n+\n+\n+@dataclass(frozen=True)\n+class EpisodeContext:\n+    ep_id: str\n+    show_slug: str\n+    season_number: int\n+    episode_number: int\n+\n+\n+@dataclass\n+class PresignedUpload:\n+    ep_id: str\n+    bucket: str\n+    object_key: str\n+    upload_url: str | None\n+    expires_in: int | None\n+    headers: Dict[str, str]\n+    method: str\n+    path: str | None = None\n+    backend: str = \"s3\"\n+\n+\n+def episode_context_from_id(ep_id: str) -> EpisodeContext:\n+    match = _EP_ID_REGEX.match(ep_id)\n+    if not match:\n+        raise ValueError(f\"Unable to parse episode id '{ep_id}' into show/season/episode\")\n+    show = match.group(\"show\")\n+    season = int(match.group(\"season\"))\n+    episode = int(match.group(\"episode\"))\n+    return EpisodeContext(ep_id=ep_id, show_slug=show, season_number=season, episode_number=episode)\n+\n+\n+def artifact_prefixes(ep_ctx: EpisodeContext) -> Dict[str, str]:\n+    \"\"\"Return v2 S3 prefixes for frames/crops/manifests under single bucket.\"\"\"\n+\n+    show = ep_ctx.show_slug\n+    season = ep_ctx.season_number\n+    episode = ep_ctx.episode_number\n+    return {\n+        \"frames\": f\"{ARTIFACT_ROOT}/frames/{show}/s{season:02d}/e{episode:02d}/frames/\",\n+        \"crops\": f\"{ARTIFACT_ROOT}/crops/{show}/s{season:02d}/e{episode:02d}/tracks/\",\n+        \"manifests\": f\"{ARTIFACT_ROOT}/manifests/{show}/s{season:02d}/e{episode:02d}/\",\n+        \"thumbs_tracks\": f\"{ARTIFACT_ROOT}/thumbs/{show}/s{season:02d}/e{episode:02d}/tracks/\",\n+        \"thumbs_identities\": f\"{ARTIFACT_ROOT}/thumbs/{show}/s{season:02d}/e{episode:02d}/identities/\",\n+    }\n+\n+\n+def parse_v2_episode_key(key: str) -> Dict[str, object] | None:\n+    \"\"\"Parse a v2 episode key (raw/videos/{show}/s{ss}/e{ee}/episode.mp4).\"\"\"\n+\n+    match = _V2_KEY_RE.search(key)\n+    if not match:\n+        return None\n+    show = match.group(\"show\")\n+    season = int(match.group(\"season\"))\n+    episode = int(match.group(\"episode\"))\n+    ep_id = f\"{show.lower()}-s{season:02d}e{episode:02d}\"\n+    return {\n+        \"ep_id\": ep_id,\n+        \"show\": show,\n+        \"show_slug\": show,\n+        \"season\": season,\n+        \"episode\": episode,\n+        \"key_version\": \"v2\",\n+    }\n+\n+\n+class StorageService:\n+    \"\"\"Lightweight S3/MinIO helper that only presigns uploads.\"\"\"\n+\n+    def __init__(self) -> None:\n+        self.backend = os.environ.get(\"STORAGE_BACKEND\", \"s3\").lower()\n+        self.region = os.environ.get(\"AWS_DEFAULT_REGION\", DEFAULT_REGION)\n+        self.prefix = os.environ.get(\"AWS_S3_PREFIX\", \"raw/\")\n+        if self.prefix and not self.prefix.endswith(\"/\"):\n+            self.prefix += \"/\"\n+        auto_create = os.environ.get(\"S3_AUTO_CREATE\", \"0\")\n+        self.auto_create = auto_create.lower() in {\"1\", \"true\", \"yes\"}\n+        self.bucket = DEFAULT_BUCKET\n+        self._client = None\n+        self._client_error_cls = None\n+        self.write_enabled = True\n+\n+        if self.backend == \"s3\":\n+            boto3_mod = _boto3()\n+            from botocore.exceptions import ClientError  # type: ignore\n+\n+            client_kwargs: Dict[str, object] = {\"region_name\": self.region}\n+            custom_endpoint = os.environ.get(\"SCREENALYTICS_OBJECT_STORE_ENDPOINT\")\n+            if custom_endpoint:\n+                client_kwargs[\"endpoint_url\"] = custom_endpoint\n+            self._client = boto3_mod.client(\"s3\", **client_kwargs)\n+            configured_bucket = os.environ.get(\"SCREENALYTICS_S3_BUCKET\") or os.environ.get(\"AWS_S3_BUCKET\")\n+            self.bucket = configured_bucket or DEFAULT_BUCKET\n+            self._client_error_cls = ClientError\n+            self._ensure_s3_bucket(ClientError)\n+        elif self.backend == \"minio\":\n+            boto3_mod = _boto3()\n+            from botocore.client import Config  # type: ignore\n+            from botocore.exceptions import ClientError  # type: ignore\n+\n+            endpoint = os.environ.get(\"SCREENALYTICS_OBJECT_STORE_ENDPOINT\", \"http://localhost:9000\")\n+            access_key = os.environ.get(\"SCREENALYTICS_OBJECT_STORE_ACCESS_KEY\", \"minio\")\n+            secret_key = os.environ.get(\"SCREENALYTICS_OBJECT_STORE_SECRET_KEY\", \"miniosecret\")\n+            signature_version = os.environ.get(\"SCREENALYTICS_OBJECT_STORE_SIGNATURE\", \"s3v4\")\n+            minio_region = os.environ.get(\"SCREENALYTICS_OBJECT_STORE_REGION\", DEFAULT_REGION)\n+            self.bucket = os.environ.get(\"SCREENALYTICS_OBJECT_STORE_BUCKET\", DEFAULT_BUCKET)\n+            self._client = boto3_mod.client(\n+                \"s3\",\n+                endpoint_url=endpoint,\n+                region_name=minio_region,\n+                aws_access_key_id=access_key,\n+                aws_secret_access_key=secret_key,\n+                config=Config(signature_version=signature_version),\n+            )\n+            self._client_error_cls = ClientError\n+        elif self.backend == \"local\":\n+            self.bucket = \"local\"\n+        else:\n+            raise ValueError(f\"Unsupported STORAGE_BACKEND '{self.backend}'\")\n+\n+        flag = os.environ.get(\"S3_WRITE\")\n+        default_enabled = self.backend in {\"s3\", \"minio\"} and self._client is not None\n+        if flag is not None:\n+            default_enabled = flag.lower() in {\"1\", \"true\", \"yes\"}\n+        self.write_enabled = default_enabled and self.backend in {\"s3\", \"minio\"} and self._client is not None\n+\n+    def s3_enabled(self) -> bool:\n+        return self.backend in {\"s3\", \"minio\"} and self._client is not None\n+\n+    def presign_episode_video(\n+        self,\n+        ep_id: str,\n+        *,\n+        object_key: str,\n+        content_type: str = \"video/mp4\",\n+        expires_in: int = DEFAULT_EXPIRY,\n+    ) -> PresignedUpload:\n+        headers = {\"Content-Type\": content_type}\n+\n+        if self.backend == \"local\":\n+            upload_url = f\"{LOCAL_UPLOAD_BASE}/{object_key}\"\n+            method = \"FILE\"\n+            path = object_key\n+        else:\n+            assert self._client is not None  # for mypy\n+            params = {\"Bucket\": self.bucket, \"Key\": object_key, \"ContentType\": content_type}\n+            upload_url = self._client.generate_presigned_url(\n+                \"put_object\",\n+                Params=params,\n+                ExpiresIn=expires_in,\n+                HttpMethod=\"PUT\",\n+            )\n+            method = \"PUT\"\n+            path = None\n+\n+        return PresignedUpload(\n+            ep_id=ep_id,\n+            bucket=self.bucket,\n+            object_key=object_key,\n+            upload_url=upload_url,\n+            expires_in=expires_in,\n+            headers=headers,\n+            method=method,\n+            path=path,\n+            backend=self.backend,\n+        )\n+\n+    def presign_get(self, key: str, expires_in: int = 3600) -> str | None:\n+        if self.backend not in {\"s3\", \"minio\"} or self._client is None:\n+            return None\n+        params = {\"Bucket\": self.bucket, \"Key\": key}\n+        return self._client.generate_presigned_url(  # type: ignore[union-attr]\n+            \"get_object\",\n+            Params=params,\n+            ExpiresIn=expires_in,\n+        )\n+\n+    def ensure_local_mirror(\n+        self,\n+        ep_id: str,\n+        *,\n+        show_ref: str | None = None,\n+        season_number: int | None = None,\n+        episode_number: int | None = None,\n+    ) -> Dict[str, Optional[object]]:\n+        local_path = get_path(ep_id, \"video\")\n+        local_path.parent.mkdir(parents=True, exist_ok=True)\n+        info: Dict[str, Optional[object]] = {\n+            \"local_video_path\": str(local_path),\n+            \"bytes\": local_path.stat().st_size if local_path.exists() else None,\n+            \"etag\": None,\n+            \"used_key_version\": None,\n+        }\n+        if self.backend == \"local\":\n+            return info\n+        if self.backend not in {\"s3\", \"minio\"} or self._client is None or self._client_error_cls is None:\n+            return info\n+        keys_to_try: List[tuple[str, str]] = []\n+        if show_ref is not None and season_number is not None and episode_number is not None:\n+            keys_to_try.append((\"v2\", self.video_object_key_v2(show_ref, season_number, episode_number)))\n+        keys_to_try.append((\"v1\", self.video_object_key_v1(ep_id)))\n+\n+        for version, key in keys_to_try:\n+            try:\n+                head = self._client.head_object(Bucket=self.bucket, Key=key)\n+                etag = head.get(\"ETag\")\n+                info[\"etag\"] = etag.strip('\"') if isinstance(etag, str) else etag\n+                info[\"bytes\"] = head.get(\"ContentLength\")\n+                if not local_path.exists():\n+                    self._client.download_file(self.bucket, key, str(local_path))\n+                    info[\"bytes\"] = local_path.stat().st_size\n+                info[\"used_key_version\"] = version\n+                return info\n+            except self._client_error_cls as exc:  # type: ignore[misc]\n+                error_code = exc.response.get(\"Error\", {}).get(\"Code\") if hasattr(exc, \"response\") else None\n+                if error_code in {\"404\", \"NoSuchKey\", \"NotFound\"}:\n+                    continue\n+                raise\n+        raise RuntimeError(\"Episode video not found in S3 (checked v2 then v1)\")\n+        return info","path":"apps/api/services/storage.py","commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","original_commit_id":"810f04573d8d2cf2138490bbb4892e1acb9440c6","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"This statement is unreachable.","created_at":"2025-11-11T20:00:18Z","updated_at":"2025-11-11T20:00:23Z","html_url":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565923","pull_request_url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4","_links":{"self":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565923"},"html":{"href":"https://github.com/therealityreport/screenalytics/pull/4#discussion_r2515565923"},"pull_request":{"href":"https://api.github.com/repos/therealityreport/screenalytics/pulls/4"}},"reactions":{"url":"https://api.github.com/repos/therealityreport/screenalytics/pulls/comments/2515565923/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":311,"original_line":311,"side":"RIGHT","author_association":"NONE","original_position":311,"position":311,"subject_type":"line"}]
