name: ML Integration Tests

on:
  push:
    branches: [main, nov-18]
  pull_request:
    branches: [main]
  workflow_dispatch:

env:
  RUN_ML_TESTS: "1"

jobs:
  ml-tests:
    name: ML Tests (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest

    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.10", "3.11", "3.12"]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg libsm6 libxext6 libxrender-dev libgomp1 \
            libavformat-dev libavcodec-dev libavutil-dev libswscale-dev libswresample-dev

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-ml.txt
          pip install pytest pytest-timeout requests

      - name: Run ML integration tests
        env:
          RUN_ML_TESTS: "1"
          SCREENALYTICS_DATA_ROOT: ${{ github.workspace }}/test_data
        run: |
          mkdir -p test_data

          echo "::group::Detect/Track Metrics Tests"
          pytest tests/ml/test_detect_track_metrics.py -v --tb=short
          echo "::endgroup::"

          echo "::group::Faces Embed Metrics Tests"
          pytest tests/ml/test_faces_embed_metrics.py -v --tb=short
          echo "::endgroup::"

          echo "::group::Cluster Metrics Tests"
          pytest tests/ml/test_cluster_metrics.py -v --tb=short
          echo "::endgroup::"

          echo "::group::Episode Cleanup Tests"
          pytest tests/ml/test_episode_cleanup_metrics.py -v --tb=short
          echo "::endgroup::"

      - name: Run API smoke tests
        env:
          RUN_ML_TESTS: "1"
          SCREANALYTICS_DATA_ROOT: ${{ github.workspace }}/test_data
        run: |
          echo "::group::API Smoke Tests"
          pytest tests/api/test_jobs_smoke.py -v --tb=short
          echo "::endgroup::"

      - name: Run config/profile unit tests
        run: |
          echo "::group::Profile Config Resolution Tests"
          pytest tests/unit/test_profiles_config_resolution.py -v --tb=short
          echo "::endgroup::"

      - name: Validate metric thresholds
        if: always()
        run: |
          echo "=== Metric Threshold Validation ==="
          echo "Checking for metric threshold violations in test outputs..."

          # Parse ACCEPTANCE_MATRIX.md for expected thresholds
          echo "Expected thresholds from ACCEPTANCE_MATRIX.md:"
          echo "  tracks_per_minute: ≤ 50"
          echo "  short_track_fraction: ≤ 0.30"
          echo "  id_switch_rate: ≤ 0.10"
          echo "  singleton_fraction: ≤ 0.50"
          echo "  largest_cluster_fraction: ≤ 0.60"
          echo ""

          # If any test failed, pytest will have already failed the job
          # This step provides additional context about metric violations
          if [ -f test_data/manifests/*/track_metrics.json ]; then
            echo "✓ Test artifacts generated successfully"
          else
            echo "⚠ No metric artifacts found (tests may have been skipped)"
          fi

      - name: Upload test artifacts
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: test-artifacts-py${{ matrix.python-version }}
          path: |
            test_data/
            pytest.log
          retention-days: 7
